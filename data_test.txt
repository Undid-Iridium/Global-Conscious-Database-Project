https://www.google.com/appserve/mkt/p/ALvTkbYWLBLO4UuPhevTMdBq8wB7XjFBt8hnsgx2Q6FP3CNSWTc2moz3Yv6zjGTbpRXjw2CK85J1DHTuBNdPJ18-rytzyyz3et9H1pqUC1_BE-LUDVuv3iQyHWaEwwE-GWqYapZStIugQfIMxmaCBULCzYvZV1J3fHV1gtpoaEfoO6zIN1a8FJ9aYbcCosC0F1MFG8s3SjIv0x6njhTw9n-sl6qGQJdubn_6QsydcIKcyKyO5vZ96_CQkRzORAZlzIov
           1        Desktop notifications are on   | Turn off       Get breaking news alerts from The Washington Post   Turn on desktop notifications?   Yes  Not now                            Sections          Home            Democracy Dies in Darkness             Try 1 month for $1       Username      Sign In       Account and Profile  Newsletters & Alerts  Gift Subscriptions  Contact Us  Help Desk              Subscribe        Account and Profile  Newsletters & Alerts  Gift Subscriptions  Contact Us  Help Desk               Accessibility for screenreader                    Share on Google Plus    Share on Facebook   Share on Twitter   Share on Google Plus   Share via Email   Share on LinkedIn   Share on Pinterest   Share on Tumblr        Resize Text  Print Article   Comments                          Morning Mix    Women ‘are the n-word of the world,’ Bette Midler tweeted. She apologized hours later.                     Bette Midler was condemned on Twitter on Thursday after she quoted the title of a 1972 song written by John Lennon and Yoko Ono. (Mike Coppola/Getty Images for Michael Kors)        By  Timothy Bella          Timothy Bella  Deputy editor for Morning Mix   Email  Bio   Follow         October 5   As protests in Washington unfolded on Thursday in response to the sexual assault allegations against Supreme Court nominee Brett M. Kavanaugh and his upcoming procedural vote in the Senate, actress Bette Midler took to Twitter to make her feelings known. The end result was the “Hocus Pocus” actress tweeting an offensive phrase from a John Lennon and Yoko Ono song and being on the wrong end of a lot of criticism.  “‘Women, are the n-word of the world,'” the actress tweeted on Thursday. “Raped, beaten, enslaved, married off, worked like dumb animals; denied education and inheritance; enduring the pain and danger of childbirth and life IN SILENCE for THOUSANDS of years[.]”  She concluded: “They are the most disrespected creatures on earth.”     An image of a now-deleted tweet from Bette Midler. (Twitter)   Midler, 72, was paraphrasing the title of a 1972 song written by Lennon and Ono. Much like the response Lennon and Ono received at the time of the song’s release, Midler was met with outrage. On Twitter, people condemned the actress and called on her to delete the tweet, which would receive 14,000 likes and more than 8,000 replies, the majority of which were critical, according to USA Today .  Users made it clear in their responses that it was not right for Midler to use the word, or make any such comparison. One of the most notable rebukes came from Jemele Hill, a reporter for the Atlantic who recently left ESPN: “Full stop.”    Full stop. https://t.co/KcQUASkzVu — Jemele Hill (@jemelehill) October 5, 2018       Bette Midler out here thinking she's a white woman of color. — Genie Lauren (@MoreAndAgain) October 5, 2018       If only it was so simple. If only all women were treated the same. Can you, perhaps, listen when SOME WOMEN are saying they took offense to this? Ain't I a woman? Do you know that quote? You know what? Never mind. *jumps out window, lands on Women's March* — Angela Nissel (@AngelaNissel) October 5, 2018       You: "It's not about race." Also you: *brings up the n word* — diane 👩🏾‍💻 (@dianelyssa) October 5, 2018     Midler responded a couple of hours later. But instead of apologizing, she doubled down on her previous comments, emphasizing that her message was not about race but gender.  “I gather I have offended many by my last tweet,” she wrote. “‘Women are the . . . etc’ is a quote from Yoko Ono from 1972, which I never forgot. It rang true then, and it rings true today, whether you like it or not. This is not about race, this is about the status of women; THEIR HISTORY.”     An image of a now-deleted tweet from Bette Midler. (Twitter)   Midler would later delete both tweets. She would also apologize hours after the initial tweet.  “The too brief investigation of allegations against Kavanaugh infuriated me,” she stated. “Angrily I tweeted w/o thinking my choice of words would be enraging to black women who doubly suffer, both by being women and by being black. I am an ally and stand with you; always have. And I apologize.”    The too brief investigation of allegations against Kavanaugh infuriated me. Angrily I tweeted w/o thinking my choice of words would be enraging to black women who doubly suffer, both by being women and by being black. I am an ally and stand with you; always have. And I apologize. — Bette Midler (@BetteMidler) October 5, 2018     Released in April 1972 , Lennon and Ono hailed the song as a pro-feminist anthem, a stinging attack on the patriarchy. Even though the National Organization for Women awarded the song with a “Positive Image of Women” citation, almost every radio station in the country chose not to play it, the Los Angeles Times reported. The response baffled the Beatle.  “As the song says, ‘Woman is the slave of the slaves,'” Lennon told the Los Angeles Times in 1972. “I agree that a lot of people, black and white, are slaves in the world, but each of them has his own slave and that’s usually the wife.”  Lennon and Ono would give TV and newspaper interviews to offer their perspective of the song’s meaning. One of those interviews was on “ The Dick Cavett Show .”  “I had to find out about myself and my attitudes toward women,” Lennon said to Cavett.  In an interview published to YouTube in 2012, Cavett recalled how Lennon and Ono had wanted to perform the song on his ABC show, something that did not go over well with network executives.  Cavett said that a compromise was later made to allow the show to keep the song on the episode if Cavett could offer a statement ahead of time, warning of the performance’s controversial content. Cavett was surprised at the response he received for the episode.  “A lot of complaints did come in, but all of them were about the mealy mouthed message you made Cavett say before the song and none about the song itself,” he said in the interview.  The response over Midler’s sentiments continued into late Thursday, as some people were upset this Twitter incident sprang up before they could enjoy their annual viewing of “Hocus Pocus,” her 1993 Halloween movie.  “@BetteMidler had to go get herself canceled in October, of all months??” HuffPost contributor Gennette Cordova wrote . “I haven’t even gotten to watch Hocus Pocus yet, this Fall.”  More from Morning Mix:  Trevor Noah gets serious: Trump is convincing men they are the ‘true victims’ of #MeToo movement  A 2-year-old shredded $1,060 of his family’s cash. His mom cried — until she laughed.              Comment s              Timothy Bella  Timothy Bella is the deputy editor of Morning Mix. His work has appeared in outlets such as Esquire, the Atlantic, New York magazine and the Undefeated. Follow                 The story must be told.  Your subscription supports journalism that matters.    Try 1 month for $1                                      Sign up for email updates from the "Confronting the Caliphate" series.  You have signed up for the "Confronting the Caliphate" series.             ✕  Thank you for signing up  You'll receive e-mail when new stories are published in this series.       Most Read National       1   ‘Crying Nazi’ vows ‘destruction’ of the left after Charlottesville killer’s murder conviction       2   Another Jehovah’s Witness center has been destroyed in 9-month rash of arson attacks       3   A blood clot formed in the exact shape of a man’s lung passage — then he coughed it up       4   ‘Make better choices’: Endangered Hawaiian monk seals keep getting eels stuck up their noses and scientists want them to stop       5   Ammon Bundy spoke kindly about the migrant caravan. The backlash has him reevaluating his supporters.            Opinion Senate Republicans are responsible for the most unethical and incompetent administration ever            Opinion The utterly lawless ‘Individual-1’                      washingtonpost.com  © 1996-2018 The Washington Post    Help and Contact Us  Policies and Standards  Terms of Service  Privacy Policy  Print Products Terms of Sale  Digital Products Terms of Sale  Submissions and Discussion Policy  RSS Terms of Service  Ad Choices                     Read content from allstate           Content from Allstate  This content is paid for by an advertiser and published by WP BrandStudio. The Washington Post newsroom was not involved in the creation of this content. Learn more about WP BrandStudio.    We went to the source. Here’s what matters to millennials.  A state-by-state look at where Generation Y stands on the big issues.                         
https://www.google.com/docs/about/
      Google Editors       Docs     Sheets     Slides     Forms      For Work    Help    Go to Google Docs             Looking for Google Drive?  Visit drive.google.com to see all of your files.   Learn more | Dismiss        Create
          amazing Kim documents  Create    documents  With Google Docs, you can write, edit, and collaborate wherever you are. For free. Download Google Docs   Go to Google Docs                        SAN FRANCISCO  VACATION    Jono      Salit    Day 1 / Golden Gate Bridge  The Golden Gate Bridge is a suspension bridge spanning the Golden Gate
                      strait, the 1 mile wide, 3 mile long channel between San Francisco Bay and
                      the Pacific Ocean.    The structure links the U.S. city of San Francisco, on the northern tip of
                      the San Francisco Peninsula, to Marin County, bridging both U.S. Route 101
                      and California State Route 1 across the strait.        Lindsay         More than letters and words  Google Docs brings your documents to life with smart editing and styling tools to
              help you easily format text and paragraphs. Choose from hundreds of fonts, add links,
              images, and drawings. All for free.       Get a head start with templates  Choose from a wide variety of resumes, reports, and other pre-made documents — all
            designed to make your work that much better, and your life that much easier.     View all
          templates        Get to your documents anywhere, anytime  Access, create, and edit your documents wherever you go — from your phone, tablet, or
              computer — even when there's no connection.   Download the app     Get Docs for Android    Get Docs for iPhone and iPad    Download Google Docs                          Share    Invite people:   Enter names, email...        SAN FRANCISCO  May      You have to check out           When do you think you can have the edits done? Tue, 1:06 PM        Maybe by two? Tue, 1:06 PM              Do more,
                  together  With Google Docs, everyone can
                  work together in the same document at the same time.    Share with
                  anyone  Click share and let anyone –
                  friends, classmates, coworkers, family – view, make suggestions that you can
                  accept or reject, or edit your document directly.    Edit in
                  real-time  When someone is editing your
                  document, you can see their cursor as they make changes or highlight
                  text.    Chat &
                  comment  Chat with others directly inside
                  any document or add a comment with “+” their email address and they’ll get a
                  notification.           Never hit “save” again  All your changes are automatically saved as you type. You can even use revision
              history to see old versions of the same document, sorted by date and who made the
              change.                  Works with Word    Open, and edit, and save Microsoft Word files with the Chrome extension or app.     Convert Word files to Google Docs and vice versa.     Don't worry about file formats again.              Golden gate images     san francisco  restaturants in sf  fisherman's wharf      gmail  google  google maps  google translate  google drive  google docs      google  google maps  google translate  google drive  google docs  google earth      golden state warriors  golden corral  golf  golfnow  gold price  gold cup      golden state warriors  golden corral  gold price  gold cup  gold cup 2015  golden state      golden state warriors  golden corral  golden 1  golden ratio  golden state  golden retriever      golden gate bridge  golden girls  golden gate park  golden gate bridge toll  golden gate fields  golden gate transit      golden gate insurance  golden gate indian  golden gate international exposition  golden gate interview questions  golden gate inn  golden gate in 48      golden gate images  golden gate imdb  golden gate implementation step  golden gate import export  golden gate imports  golden gate implementation      golden gate images  golden gate images free  golden gate bridge images free  golden gate park images  golden gate bridge images download  golden gate fields images                  All of Search, right in Docs.  Explore and get inspired by images, quotes, and text from Google searches, without
              leaving the Android app.   Get Docs for Android          Do more with add-ons  Take your Docs experience even further with add-ons. Try Lucidchart Diagrams to create and insert flow charts and
              diagrams. See what else you
            can add on           Get started now  Docs is ready to go when you are. Simply create a document through your browser or
              download the app for your mobile device.   Get Google
              Docs    Go to Docs    Get Docs for Android    Get Docs for iPhone and iPad    Download Google Docs         Stay in the know  Never miss out on the latest updates and handy tips for getting the most out of Google
          Docs.      Sign up    Please enter a valid email address  Thanks for signing up. You can unsubscribe at any time at the bottom of any email you
              receive from Google Docs.      terms of use    privacy policy           Share:      Follow:          About Google Docs    Google Docs    Google Sheets    Google Slides    Google Forms      Downloads & more    Docs for Android    Sheets for Android    Slides for Android    Docs for
                  iPhone & iPad    Sheets
                  for iPhone & iPad    Slides
                  for iPhone & iPad    Office editing for Docs, Sheets, and Slides    Add-ons for
                  Docs    Add-ons for
                  Sheets    Add-ons for Forms      Support    Docs Help Center    Google Docs
                  Forum      Related Solutions    Google Drive    Google Cloud Platform    G Suite    Google Apps for Education    Google Apps for Government    Google Business Solutions           Change language:   Afrikaans  Azərbaycanca  Bahasa Indonesia  Bahasa Melayu  Català  Čeština  Dansk  Deutsch  Eesti  English  English (United Kingdom)  Español  Español (Latinoamérica)  Euskara  Filipino  Français  Français (Canada)  Galego  Hrvatski  Isizulu  Íslenska  Italiano  Kiswahili  Latviešu  Lietuvių  Magyar  Nederlands  Norsk  Polski  Português (Brasil)  Português (Portugal)  Română  Slovenčina  Slovenščina  Suomi  Svenska  Tiếng Việt  Türkçe  Ελληνικά  Български  Монгол  Русский  Српски  Українська  ქართული  Հայերեն  ‫עברית‬  ‫اردو‬  ‫العربية‬  ‫فارسی‬  አማርኛ  नेपाली  मराठी  हिन्दी  বাংলা  ગુજરાતી  தமிழ்  తెలుగు  ಕನ್ನಡ  മലയാളം  සිංහල  ไทย  ລາວ  ខ្មែរ  한국어  中文 (香港)  中文（简体中文）  中文（繁體中文）  日本語      Google    Privacy    Terms              
http://www.google.com/think/research-studies/word-of-mouth-and-the-internet.html
         Skip to Content        Open Main Navigation           Close Main Navigation       Think with Google         Consumer Insights    Marketing Resources      Data & Measurement  Experience & Design  Monetization  Omnichannel  Organizational Culture  Programmatic Advertising    Tools  Tutorials       Advertising Channels      Emerging Technology  Mobile  Search  Video                   Think with Google               Consumer Insights    Marketing Resources      Data & Measurement  Experience & Design  Monetization  Omnichannel  Organizational Culture  Programmatic Advertising    Tools  Tutorials       Advertising Channels      Emerging Technology  Mobile  Search  Video            Word of Mouth Infographic     Share     Share       Share this article   Close          Email       Copy     Link Copied    Linkedin       Twitter       Facebook            Link Copied  Word of Mouth Infographic  August 2011       Subscribe        Word of Mouth Infographic     August 2011   Consumer Insights        Email       Copy     Link Copied    Linkedin       Twitter       Facebook           Share     Share       Share this article   Close          Email       Copy     Link Copied    Linkedin       Twitter       Facebook            Link Copied  Word of Mouth Infographic  August 2011            We wondered what effect the internet and internet-enabled devices like smartphones and tablets have on good old fashioned word-of-mouth. What you'll see in this infographic, among other things, is that the internet is the number one resource people turn to after having a conversation about a brand.     Word of Mouth (WOM)     Download PDF                  Recommended for you           When the Path to Purchase Becomes the Path to Purpose    Article  June 2014            The Golden Age of Marketing Research    Perspective  August 2012            New research shows how to connect with U.S. Hispanics online    Article  June 2015            Get the latest data, insights, and inspiration from Google.  Subscribe         When the Path to Purchase Becomes the Path to Purpose          When the Path to Purchase Becomes the Path to Purpose    Article  June 2014            The Golden Age of Marketing Research    Perspective  August 2012            New research shows how to connect with U.S. Hispanics online    Article  June 2015             Google       About Google  Products  Advertising  Business  Privacy & Terms  About Think with Google            Follow us on LinkedIn       Follow us on Twitter       Follow us on Facebook       Follow us on Youtube       Subscribe to our RSS feed               Get our Newsletter  Stay on top of the latest and greatest   Subscribe       
https://www.google.com/appserve/mkt/p/ALvTkbaPt8HUYC44YFm1lWX-EC-EAYEurAtk-1phrSEqS2VBuFV7QiGC3Zx0QKxhnKgZSHMrKOa2OBRN9j1ZML1Ki_b5bxpdV9HNTigW_s-xvVajPNudT3gaCcFlOWx9MGOR4Ha-lmouDJcwjgjdMGu4-Ssdu6os0T085E6A-E4
   Breaking News 45 Congress Supreme Court 2018 Election Results Search » The definition of a nationalist By Doug Criss , CNN Updated 4:02 PM ET, Tue October 23, 2018 Chat with us in Facebook Messenger. Find out what's happening in the world as it unfolds. JUST WATCHED Trump declares himself a nationalist Replay More Videos ... MUST WATCH Trump declares himself a nationalist  00:57 (CNN) During a rally Monday night in Texas, President Donald Trump used a word he had never before uttered publicly to describe himself: nationalist. After telling the crowd that a globalist is someone who "wants the globe to do well" at the expense of the country, Trump made it clear he's not one of those. "You know what I am, I'm a nationalist. Use that word," he roared, as the crowd erupted in cheers of "USA! USA!" So just what the heck does the word nationalist mean, and is having a self-declared nationalist in the White House good or bad for the country? What does the word nationalist mean? Read More The dictionary's definition of the word seems innocent enough: a person devoted to nationalism (the devotion and loyalty to one's own country). But make no mistake about it. Nationalist is a loaded and controversial term. It dates back to the 17th century and rose into prominence during the uprisings in the 18th century that produced the American and French revolutions. But the word attained more of a negative connotation during the 20th century as it become associated with the nationalism movements in Europe that helped lead to World War I and World War II. Today the word is often associated with the far-right, racist ideologies of white nationalists. Nationalism has been on the rise worldwide, as evidenced by the success of nationalist political parties in Europe. And it had a double bang in 2016, as it powered Brexit in the UK and the election of Trump as US President. Steve Bannon, Trump's former top strategist, said Trump used an "economic nationalist agenda" to win. JUST WATCHED 2016: Nationalist wave spreads throughout world Replay More Videos ... MUST WATCH 2016: Nationalist wave spreads throughout world  03:53 It's also a word that means different things to different people. "There are different definitions depending on whose nationalism you're talking about," Paul D. Miller, a senior fellow with the Atlantic Council's Scowcroft Center for Strategy and Security , told CNN. "Scholars generally differentiate between civic and ethnic/sectarian nationalism, that is, between rooting American identity in the ideals of the American experiment versus rooting it in some aspect of our culture, heritage, history, language or ethnicity. Civic nationalism is the same as what I would call patriotism, and it is essential to a healthy democracy. The second kind of nationalism -- sectarian nationalism -- is pernicious and dangerous." But Raheem Kassam, a former senior adviser to Brexit leader Nigel Farage, rejects this second, more negative definition of nationalist. "Nationalism is not inherently ugly. It is in fact inherently beautiful," said Kassam, who is currently a fellow at the Middle East Forum . "Nationalism is a philosophy based around either the nation state, what we know colloquially as 'countries,' or around another identity factor, which could be religion, ethnicity, geography or even interests," he told CNN. "In the case of President Trump, he is no doubt using the word to outline his belief in a nation of people unified by beliefs, interests and a common history. This is typically what nationalism has meant since the earliest references to it in human history, though there have no doubt been periods where nationalism, just like socialism or other philosophies, has been used to divide rather than unite, which is ironically the antithesis of its purpose." What do nationalists believe in? Nationalists' primary belief is that people in similar societies benefit when they are united by shared values and a common belief system. "Uniting people -- whether under flags, banners, anthems, or constitutions -- is conducive to a more robust civic society and stronger communities," Kassam said. But Miller dismisses that as an "incoherent" ideology. "No one has ever been able to agree on what defines the nation. It is impractical because there is no feasible way to make governments overlap exactly with all the supposed nations in the world today," Miller said. Nationalists are also populists and consider themselves sticking up for the common, working man against the elites and so-called globalists. There are voters in both US political parties receptive to that kind of messaging, and that's why the fiery populist rhetoric of Bernie Sanders and Trump during the 2016 campaign ended up appealing to overlapping groups of voters. Nationalists are also extremely protectionist, preferring to look inward when it comes to matters of foreign affairs and trade. Trump's political positions have shifted all of his life, but the one constant has been his distrust of international trade agreements and his belief that they're ultimately bad for the United States. JUST WATCHED Trump: America first and only America first Replay More Videos ... MUST WATCH Trump: America first and only America first  01:19 "We must protect our borders from the ravages of other countries making our products, stealing our companies and destroying our jobs," Trump said during his inaugural speech in January 2017 . "Protection will lead to great prosperity and strength." This type of thinking is typical of nationalists. Does the term automatically mean 'white nationalist'? The term white nationalism originated as a euphemism for white supremacy, the belief that white people are superior to all other races and should therefore dominate society, according to Oren Segal, director of the Anti-Defamation League's Center on Extremism. People who hold these beliefs sometimes go by other names, including alt-right, identitarians and race realists. However, these are simply a rebranding -- "a new name for this old hatred," Segal said. White supremacists and their ilk see diversity as a threat, Segal said. A popular white supremacist slogan is "Diversity is a code word for white genocide." Kassam doesn't buy the idea that this is what white nationalism is, stressing that the term doesn't mean what people thinks it means. "White nationalism could mean one of two things," he said. "It can mean protectionism along ethnic lines, or supremacy on ethnic grounds, but neither of these appears to fit with the tradition of economic or civic nationalism embraced by President Trump." JUST WATCHED Max Boot: GOP has become white nationalist party Replay More Videos ... MUST WATCH Max Boot: GOP has become white nationalist party  01:27 The words nationalist or white nationalist may sound innocuous to some whites, said Miller of the Atlantic Council's Scowcroft Center, but they don't realize how racially insensitive the terms may be to nonwhite Americans because they have a poor understanding of American history and culture. "That is why patriotism can be and is universal across racial, ethnic, and religious lines, but American nationalism is almost entirely limited to white Americans," Miller said. "They are not necessarily championing white identity, but the lines between American, Protestant and white are too often uncomfortably blurred. George Hawley, a political scientist at the University of Alabama, said a sense of white victimhood is key to the movement. "There is a sense that whites are under siege and being deliberately dispossessed by hostile elites who wish to usher in a new multicultural order," Hawley said. "They dislike the culturally foreign immigrants who enter the United States and work for low wages, and they dislike the political and economic elites that invite them in. They are also hostile to the media and academia, which they contend push an anti-white message." Is it good or bad that a self-declared nationalist sits in the White House? Kassam said it was good because it would ensure that the ideology of globalism would be "kept in check." Miller had a one-word answer: bad. CNN's Jeremy Diamond and Ray Sanchez contributed to this report. From Our Partners Juvetress Thinning Hair? Pour This On Your Head And Watch What Happens  Gundry MD This Toxic Vegetable Is The #1 Danger In Your Diet  LendingTree How to pay off your house ASAP (It's so simple)  Military Savings If you're current or former military, you'd better read this  Using Your Home's Equity Need cash? How to access your home's equity  Content by CompareCards Attention: these cards have 0% interest until 2020  7 cards that generate serious amounts of cash back  9 cards charging 0% interest until 2020  The easy way to get up to a $500 cash sign-up bonus  Transfer your debt and pay no interest for 18 months  45 Congress Supreme Court 2018 Election Results © 2018 Cable News Network. Turner Broadcasting System, Inc. All Rights Reserved. CNN Sans ™ & © 2016 Cable News Network. Terms of service | Privacy guidelines | AdChoices
https://www.google.com/appserve/mkt/p/AFOm0uFs6KjwlReB8MH9i6m16tMNnvH0YF-V1vnIk-ARpUrXTbPvIxin6zSKLwHjCBGxEbGT6pFBLjevaA84s8JNFrvzz_DvYBEPDpR16s5hK976WJ2g0Gk5hsTISrW0hWusM7KJWH4YKAsS9NPwpy18EGjGfUxcGQjeVR3DZ1BEY4U1hZdocnXZKhE
                                                                                                                                                               Thanks for contacting us. We've received your submission.  Back to Reading     News  Metro  Page Six  Sports  Business  Opinion  Entertainment  Fashion  Living  Media  Tech  Real Estate  Sub Menu 1   Video  Photos  Covers  Columnists  Horoscopes  Sports Odds  Email Newsletters  Home Delivery    Sub Menu 2   Page Six TV      Sign in               Sections       Search      Search        Tips     Sign Up           New York Post                                     Share this: Facebook Twitter   Flipboard    WhatsApp Email Copy       Sports    Share this: Facebook Twitter   Flipboard    WhatsApp Email Copy     Carson Wentz has one word to describe watching Eagles’ run   By Paul Schwartz        View author archive      email the author      follow on twitter      Get author RSS feed           Name (required)     Email (required)     Comment (required)               February 4, 2018 | 2:34am    Modal Trigger      Injured Eagles quarterback Carson Wentz holds up the NFC Championship trophy after watching from the sideline as his squad earned a trip to the Super Bowl.  Getty Images        More On:  Super Bowl LII   Bill Belichick can't get away from the Malcolm Butler drama   Tom Brady reminds us that his life is still great in defeat   Eagles shed light on 'fake' Super Bowl walkthrough for Patriots   Wild Eagles player roasts columnist for Doug Pederson criticism      MINNEAPOLIS — Leave it to Carson Wentz to select the most appropriate word to describe what he is feeling as the Eagles take on the Patriots in Super Bowl LII.  “This is a little bittersweet,’’ Wentz said.  How could it not be? Wentz lifted the Eagles on his shoulders — the quarterback was probably the front-runner for NFL MVP — but crumbled when he tore his ACL on Dec. 10, ending his season and ushering in the Nick Foles backup plan. Foles got the Eagles here, and Wentz will watch Sunday from the sideline at U.S. Bank Stadium.  “It’s definitely different, just knowing the extent of the injury and how long it’s going to take to get back,” Wentz said. “But our team’s been rolling, and it’s been so cool to see everybody elevate their level of play week in and week out to get this far, and overcome all of the injuries that we’ve had. Three of our five captains are on injured reserve, but here we are. We’re in the Super Bowl.”   Eagles coach Doug Pederson invited former Packers quarterback Brett Favre to talk to his team the day before the Super Bowl. Pederson was Favre’s backup quarterback in Green Bay some 20 years ago.  “He and I stay in touch,’’ Pederson said. “We are great friends. We were teammates together for eight years. I figured since he was going to be in town to ask him [if he would talk to the team]. He’s going to stop by the hotel and visit with the guys on Saturday morning.’’  Foles sounded fired up by Favre’s visit, saying, “Any time we have the opportunity to listen to someone like him speak, I’m excited to listen.’’   Strange but true: The Patriots have not scored a point in the first quarter of their seven Super Bowl appearances with Tom Brady as their quarterback.  Brady has a problem with that statistical, historical analysis.  “There is a little caveat to that, in my opinion,” Brady said. “In 2007, it was our first drive of the game and it just happened to be the first play of the second quarter.’’  Indeed, in that game against the Giants, the Patriots scored their first points on a Laurence Maroney 1-yard touchdown run on the first play of the second quarter, competing a 12-play drive.  “I’d love to score 21 points in the first quarter if we can,” Brady said, “but obviously this [Eagles] defense is going to make it really tough for us.”      Jim Schwartz, the Eagles’ defensive coordinator, said he had an “aha’’ moment when trying to stop Brady.  “Gosh, it’s been for such a long period of time,’’ Schwartz said. “I know one play that’s really affected my coaching since, I think it was 2002 [when] I was with the Titans. We were playing them on a Monday night game down in Tennessee, we had a pass rush on, they were in empty [backfield formation] and he stepped up under center and ran a quarterback sneak from like 8 yards out for a touchdown, because he saw that opportunity.”  Brady’s run was New England’s only score in that game, a 24-7 Tennessee win.  “That’s affected my coaching ever since and I’m always aware now of when that opportunity exists, sometimes the greatest lessons are learned when you paid the heaviest price in something,” Schwartz said. “You don’t think of Tom Brady for his mobility, but he certainly made us pay on that one.’’  Share this: Facebook Twitter   Flipboard    WhatsApp Email Copy     Filed under  brett favre  ,  carson wentz  ,  Super Bowl LII  ,  tom brady      Share this article:  Share this: Facebook Twitter   Flipboard    WhatsApp Email Copy    Read Next    Patriots' major Super Bowl edge won't be on offense or def...        Read Next    Patriots' major Super Bowl edge won't be on offense or def...              Share Selection           Columnists    Mark Cannizzaro        Here's some free advice for Jets players not named Jamal Adams    Steve Serby        The Pat Shurmur era is finally starting to take shape for Giants    see all columnists             Sports Odds   Get the latest odds on all the top sports.    See Odds          Most Popular Today    1         Google employee, 22, found dead inside NYC headquarters     Ex-Miss Kentucky busted for allegedly sexting teen student     Jay-Z's scholarship barely gives some students enough to cover books                More Stories    page six     'Mighty Ducks' star Shaun Weiss arrested for shoplifting          nypost     Off-duty firefighter dies in Brooklyn road-rage attack            New York Post       Facebook  Twitter  Instagram  LinkedIn  Messenger  Email     Email Newsletters  Mobile Apps  Contact Us  Tips       Facebook  Twitter  Instagram  LinkedIn  Messenger  Email      Sections & Features   News  Tech  Metro  Real Estate  Page Six  Video  Sports  Photos  Business  Alexa  Opinion  Covers  Entertainment  Horoscopes  Fashion  Sports Odds  Living  Classifieds  Media    Newsletters & More   Email Newsletters  RSS Feeds  NYPost Store  Post Headliners  Home Delivery   Subscribe  Manage Subscription  Delivery Help      Help/Support   Customer Service  App FAQ & Help  Contact Us   Tips  Newsroom  Letters to the Editor  Reprints  Careers      Apps   iPhone App  iPad App  Android Phone  Android Tablet  Advertise   Media Kit  Contact           Post Digital Network                             © 2018 NYP Holdings, Inc. All Rights Reserved | Terms of Use | Privacy  | Your Ad Choices | Sitemap                       News Corp. is a network of leading companies in the world of diversified media, news, and information services.    News Corp   HarperCollins  Marketwatch  realtor.com  Dow Jones  The Sun  Storyful  Wall Street Journal  The Times  Mansion Global  New York Post  The Australian  Checkout51  News America Marketing  News.com.au  Unruly                           Send to Email Address   Your Name   Your Email Address       Cancel  Post was not sent - check your email addresses!  Email check failed, please try again  Sorry, your blog cannot share posts by email.         click to copy                                                             
https://www.google.com/newsstand/s/posts/CAIiEFqBnPBHz1aZ9R8oMiC4NiYqFQgEKg0IACoGCAowrqkBMKBFMLKAAg/In+South+Africa+Nuclear+Energy+Is+Becoming+A+Dirty+Word
            Billionaires      All Billionaires     World's Billionaires   Forbes 400   America's Richest Self-Made Women   China's Richest   India's Richest   Indonesia's Richest   Korea's Richest   Thailand's Richest   Japan's Richest   Australia's Richest   Taiwan's Richest   Singapore's Richest   Philippines' Richest   Hong Kong's Richest   Malaysia's Richest      Innovation      All Innovation     AI & Big Data   Cloud 100 2018   Consumer Tech   Cybersecurity   Enterprise & Cloud   Games   Green Tech   Healthcare   Japan BrandVoice   KPMG BrandVoice   NVIDIA BrandVoice   Oracle BrandVoice   SAP BrandVoice   Science   ServiceNow BrandVoice   Sharing Economy   Smartsheet BrandVoice   Social Media   Venture Capital   CIO Network      Leadership      All Leadership     Careers   CMO Network   Deloitte Brandvoice   Education   Entrepreneurs   ForbesWomen   Franchises   Leadership Strategy   Small Business   Under 30   Workday BrandVoice      Money      All Money     Banking & Insurance   Braintree BrandVoice   Capital One BrandVoice   Crypto & Blockchain   ETFs & Mutual Funds   Fintech   Hedge Funds & Private Equity   Impact Partners BrandVoice   Investing   Markets   Personal Finance   Retirement   Taxes   Wealth Management      Consumer      All Consumer     Food & Drink   Hollywood & Entertainment   Media   Real Estate   Retail   SportsMoney      Industry      All Industry     Aerospace & Defense   Energy   Manufacturing   Mitsubishi Heavy Industries BrandVoice   Policy   Transportation      Lifestyle      All Lifestyle     Arts   ForbesLife   Vices   Boats & Planes   Watches & Jewelry   Cars & Bikes   Cole Haan BrandVoice   Travel   Forbes Travel Guide   Dining & Drinking       Featured      30 Under 30 2019   1850 Brand Coffee BrandVoice: Bold Moves   Centers Of Gravity   CEOs Rising   Citizens Access BrandVoice: Money Myths: Debunked   Forbes 400   Forbes Editors' Picks   Forbes Insights: AI   Forbes Insights: IoT Connecting Tomorrow   Forbes Video   Impact Investing   IWC BrandVoice: Engineering Success   OFX BrandVoice: Global Movers   Pivotal Moments: The Making Of A Small Giant   Scale Up   Side Hustle Handbook   The Portfolio   Top Wealth Advisors   Views From The Under 30 Summit       BrandVoice      Braintree   Capital One   Cole Haan   Deloitte   Impact Partners   Japan   Mitsubishi Heavy Industries   NVIDIA   Oracle   SAP   ServiceNow   Smartsheet   Workday      Lists                      Dec 13, 2016, 08:37am In South Africa, Nuclear Energy Is Becoming A Dirty Word             Nishtha Chugh    Contributor       Opinions expressed by Forbes Contributors are their own.                        Tweet This     The sharp divisions over a nuclear-powered future are now beginning to hurt South Africa's nascent renewables industry.         Share to facebook      Share to twitter      Share to linkedin             Solar panels at George airport in South Africa, the continent's first solar-powered airport. The plant produces up to 750 kilowatts a day and powers 41 percent of the airport, with the aim to convert to 100 percent function by the end of the year. (Photo by GIANLUIGI GUERCIA/AFP/Getty Images)     The nuclear reactors are still a plan on paper. But already the noxious debate over their future has made nuclear energy a dirty word in South Africa. To build or not to build – the stalemate over the proposed nuclear reactors to power the continent's most advanced economy shows no sign of being resolved.  The sharp divisions over a nuclear-powered future are now beginning to hurt South Africa's nascent renewables industry.  State power utility Eskom is dragging its feet on honoring government-brokered deals with private renewables companies. Its refusal to purchase 250 megawatts of power from wind and solar projects has left its Irish and Saudi Arabian suppliers fuming and in limbo. More than scuppering the deals, Eskom’s actions, critically, threaten to undermine the gains made by the country’s green energy program, which many have come to hail as the shining beacon of a renewables-based future . On the Fieldstone Africa Renewable Index or FARI, South Africa’s ranking has plummeted off the charts entirely, prompting concerns amongst investors over green energy’s future in the country. Its decline is ironic given the rainbow nation had topped the continent-wide list just four months ago.  With a cluster of over 100 solar and wind projects South Africa is still currently home to the world’s fastest growing renewables program, generating 2.2 gigawatts of energy. According to FARI the country’s program has delivered enormous economic value for South Africa, attracting R196.4 billion ($14.4billion) in investments and created 20,000 jobs.    The program was started in 2010 in response to incessant power cuts South Africans endured day after day. They blamed the cuts for not only blighting their private and social life but also disrupting health, education and small scale industries.  Touching another small yet unique milestone the country’s first independent landfill gas-to-power project began operating in capital Johannesburg last week. The largest project of its kind in South Africa the plant at Robinson Deep landfill is the first of the five planned sites and has the capacity to generate 3 megawatts of clean energy – enough to power more than 5000 homes.   But Eskom’s snub just when the green energy industry is beginning to thrive in South Africa is not without reason. The state utility remains adamant that green energy from renewables is unreliable during peak times. Defending its position to not sign off 20-year Power Purchase Agreements (PPAs) with the contracted companies the agency claims it’s protecting the consumers from higher tariffs because current arrangements make it ‘ very expensive ’ to buy power from renewable producers.  Eskom’s stance, however, is openly questionable, particularly in view of its staunch support to the nuclear power plant proposals. While green energy suppliers have accused the agency of abusing its power, the environmental groups and civic rights organization have challenged its rigidity and claims over the matter. Another reason for dismally low faith in Eskom’s capability and judgement on future energy policy is its failure to finish the construction and commission of Medupi and Kusile coal-powered power plants. Both projects have missed their deadlines and are woefully overbudget now.  The ire directed at Eskom and department of energy has at its root legitimate concerns over the way energy future debate in South Africa is being shaped. Despite overwhelming evidence in support and expansion of existing renewables infrastructure the government machinery, going right up to President Jacob Zuma himself, has been leaning heavily towards constructing nuclear power plants to meet future demands. The plant, according to the official Integrated Resource Plan, will cost R100 billion ($7.3 billion) a year every year from 2040 when they are proposed to be built.  The proponents of renewable energy believe it is not only wholly unnecessary but will also spell doom for the South African economy.  Drop in the cost of producing energy from solar and wind has been dramatic over the past few years all across the globe. South Africa’s own premier research agency Council for Scientific and Industrial Research (CSIR) has backed the case for an energy mix where 75% of the supply comes from renewable sources. Currently, only 2% of the energy is supplied by solar and wind projects.  Zuma administration’s reluctance to consider the case in favor of renewables combined with Eskom’s inflexibility with private energy producers is already shaking investor confidence in doing business with South Africa.  Earlier in August SMA Solar Technology folded its business in Cape Town to move to Germany and China citing ‘ lack of commitment ’ towards the renewable energy program. The current standoff between Eskom and green power companies is now threatening to risk billions of dollars of investment already pumped into the country.  This could be the beginning of the end of world’s most promising green energy initiative. The Zuma government needs to rethink its strategy if it still wants a clean and cost-efficient energy future for the country.           Gallery: The World's Biggest Public Energy Companies 2016  26 images   View gallery                   I am a UK-based development journalist with a keen interest in the future of energy in the developing world, the home to 83% of the global population. In particular, I focus on the energy policies of the emerging market economies and their impact on climate change. I have re...   MORE           Print    Site Feedback    Tips    Corrections    Reprints & Permissions    Terms    Privacy     ©2018 Forbes Media LLC. All Rights Reserved.   AdChoices                       
https://www.google.com/patents/US7917350
      US7917350B2 - Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building 
        - Google Patents  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building   Download PDF  Info   Publication number  US7917350B2    US7917350B2  US12126980  US12698008A  US7917350B2  US 7917350 B2  US7917350 B2  US 7917350B2  US 12126980  US12126980  US 12126980  US 12698008 A  US12698008 A  US 12698008A  US 7917350 B2  US7917350 B2  US 7917350B2  Authority  US  Grant status  Grant   Patent type     Prior art keywords  word  probability  corpus  character  characters  Prior art date  2004-07-14  Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)   Active , expires 2026-02-23    Application number  US12126980  Other versions    US20080228463A1 ( en  )   Inventor  Shinsuke Mori  Daisuke Takuma  Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)  International Business Machines Corp  Original Assignee  International Business Machines Corp  Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)  2004-07-14 Filing date 2008-05-26 Publication date 2011-03-29 Grant date 2011-03-29 Links    USPTO     USPTO Assignment     Espacenet     Global Dossier     Discuss    Images                                     Classifications      G — PHYSICS    G06 — COMPUTING; CALCULATING; COUNTING    G06F — ELECTRIC DIGITAL DATA PROCESSING    G06F17/00 — Digital computing or data processing equipment or methods, specially adapted for specific functions    G06F17/20 — Handling natural language data    G06F17/28 — Processing or translating of natural language    G06F17/2863 — Processing of non-latin text          G — PHYSICS    G06 — COMPUTING; CALCULATING; COUNTING    G06F — ELECTRIC DIGITAL DATA PROCESSING    G06F17/00 — Digital computing or data processing equipment or methods, specially adapted for specific functions    G06F17/20 — Handling natural language data    G06F17/27 — Automatic analysis, e.g. parsing    G06F17/2705 — Parsing    G06F17/2715 — Statistical methods        Abstract   Calculates a word n-gram probability with high accuracy in a situation where a first corpus), which is a relatively small corpus containing manually segmented word information, and a second corpus, which is a relatively large corpus, are given as a training corpus that is storage containing vast quantities of sample sentences. Vocabulary including contextual information is expanded from words occurring in first corpus of relatively small size to words occurring in second corpus of relatively large size by using a word n-gram probability estimated from an unknown word model and the raw corpus. The first corpus (word-segmented) is used for calculating n-grams and the probability that the word boundary between two adjacent characters will be the boundary of two words (segmentation probability). The second corpus (word-unsegmented), in which probabilistic word boundaries are assigned based on information in the first corpus (word-segmented), is used for calculating a word n-grams.     Description   CROSS-REFERENCE TO RELATED APPLICATIONS  This is a continuation of U.S. application Ser. No. 11/180,153, entitled “Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building”, filed Jul. 13, 2005, which claims priority from Japanese application number JP2004-207864, filed Jul. 14, 2004, all of which are incorporated herein by reference.  FIELD OF THE INVENTION  The present invention relates to recognition technology in natural language processing, and improving the accuracy of recognition in natural language processing by using a corpus, in particular by effectively using a corpus to which segmentation is not applied.  BACKGROUND ART  Along with the progress of recognition technology for natural language, various techniques, including kana-kanji conversion, spelling checking (character error correction), OCR, and speech recognition techniques, have achieved a practical-level predication capability. At present, most of the methods for implementing these techniques with high accuracy are based on probabilistic language models and/or statistical language models. Probabilistic language models are based on the frequency of occurrence of words or characters and require a collection of a huge number of texts (corpus) in an application field.  The following documents are considered:     [Non-patent Document 1] “Natural Language Processing: Fundamentals and applications”, edited by Hozumi Tanaka, 1999, Institute of Electronics, Information and Communication Engineers  [Non-patent Document 2] W. J. Teahan, and John G. Cleary, 1996, “The entropy of English using ppm-based models”, In DCC.  [Non-patent Document 3] Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Chales J. Stone, 1984, Classification and Regression Trees, Chapman & Hall, Inc.  [Non-patent Document 4] Masaaki Nagata, “A Self-Organizing Japanese Word Segmenter using Heuristic Word Identification and Re-estimation”, 1997     In most speech recognition systems, the most probable character string is selected from among a number of candidates by referring to a probabilistic language model as well as an acoustic model. In spell checking (character error correction), unnatural character strings and their correction candidates are listed based on the likelihood of a probabilistic language model.  Because a practical model treats a word as a unit, it is required that a corpus be provided with information about word boundaries. In order to determine word boundaries, an operation such as segmentation or tagging is performed.  Automatic word segmentation methods have been already known. However, the existing automatic word segmentation systems provide low accuracies in fields such as the medical field, where many technical terms are used. To manually correct the results of automatic word segmentation, the operator needs to have knowledge of technical terms in the application field, and typically, a minimum of tens of thousands sentences are required in order to achieve recognition sufficiently accurate enough for practical use.  In training using a corpus in an application field, it is generally difficult to obtain a huge corpus segmented and tagged manually for the application field, taking much time and cost and thus making it difficult to develop a system in a short period.  Although information segmented into words in a field (for example in the medical field) may works in processing the language in that field, there is no assurance that the information will work also in another application field (for example in the economic field, which is completely different from the medical field). In other words, a correct corpus segmented and tagged in a field may be definitely correct in that field, but may not necessarily correct in other fields because the segmented and/or tagged corpus has been fixed by segmentation and/or tagging.  In this regard, there are many techniques in the background art that are pursuing efficiency and accuracy in word segmentation in Asian languages. However, all of these techniques are aiming to predetermine word boundaries in word segmentation fixedly.  Taking Japanese out of the Asian languages as an example, word information required for analyzing Japanese text relates to the structure of word spelling, which is the information regarding the character configuration (representation form) and pronunciation of entry words, including “spelling information”, “pronunciation information”, and “morphological information”. These items of information may provide important clues mainly in extracting candidate words from Japanese text in morphological analysis.  Although there is no clear definition of the term “word”, attention is directed to two elements of the “word” herein, “spelling” and “pronunciation” and two words are regarded as the same words if and only if they have the same spelling (characters) and pronunciation. Isomorphic words just having the same spelling (characters) or homophonic words just having the same pronunciation are regarded as different words. The spelling of a word is involved in identifying a morphological characteristic and the pronunciation is involved in identifying a phonemic characteristic.  Hence, the Japanese words composed of two Chinese characters (reporter), (train), (return to the office), and  (charity) all have the same pronunciation (kisha) but different spellings (characters), whereby they are different words. The “word” is symbolized in the computer, in which the correspondence between the symbol as the spelling (character) and the symbol as its meaning is registered. Japanese is one kind of agglutinative language, and has an extremely high word formation power, whereby care must be taken in registering words in the computer as “dictionary”. The pronunciation is given in a string of input symbols (e.g., katakana in Japanese, Roman character representation of katakana) in the computer.  A word is registered in the computer by a method of registering all the possible spellings (characters), or collecting and registering the spellings having high use frequency, a method of registering only typical spellings, and searching for a word in combination with its pronunciation, or a method of providing various sorts of character conversion table apart from the dictionary and investigating the correspondence with headwords, or a combination of these methods.  A plain example for correcting the result of automatic word segmentation is given below. For example, for the pronunciation of (ha-ki-mo-no), there are two corresponding spellings. One is the word (footwear) and the other is a sequence of two words (postpositional particle) and (kimono). These two spellings are associated with the pronunciation “ha-ki-mo-no”. If there is an occurrence of this pronunciation and the spelling resulting from word segmentation is considered to be improper, the spelling is corrected by re-segmenting. Unlike English, Japanese language does not have a space between words (write with a space between words), therefore an expert must determine word boundaries from the context around an sample sentence, based on the knowledge of technical terms.  As an example indicates that the word (footwear) is assigned to the pronunciation (ha-ki-mo-no), it will be found that the word needs to be correctly recognized using the knowledge of vocabulary. Therefore, there is a demand for a method for increasing the accuracy, making effective use of the corpus without segmentation.  For all processes in natural language processing, conversion of character strings or speech data into a string of morphemes is a prerequisite. However, in Asian languages such as Japanese, it is difficult to morphologically analyze even written text because, unlike English text, text in such languages is written without a space between words. Therefore, as part of the accuracy problem described above, there is the need for accurately obtaining candidate morpheme strings (x) when input data (y) such as a hiragana character string, a katakana character string, or speech data is given.  In a statistical approach, this can be formulated as the maximization problem of P(x|y) and Bayes' theorem can be used to decompose it into two models of maximizing, P(y|x) and P(x), as shown in the right-hand side of the equation  P ( x|Y )= P ( y|x )/ P ( x )/ P ( y )  where P(y) is a constant as y is given. The model of P(x) is independent of the type of input data (whether it is a symbol string, character string, or speech data), hence called a “language model”. One of the most commonly used probabilistic language models is a word n-gram model.  <Conventional Art Relating to the Use of Unsegmented Corpus>  As conventional art there are methods in which the result of segmentation of an unsegmented corpus based on training with a segmented corpus is used:     (a) Counting n-grams with weight by using candidate segmentations,  (b) Using only 1-best of the candidates resulting from automatic segmentation, and  (c) Using n-best of the candidates resulting from automatic segmentation.     However, methods (a) and (c) require high computational costs for bi-gram and higher and are unrealistic. Advantages of the present invention over method (b) will be described later with respect to experiments.  SUMMARY OF THE INVENTION  In light of the foregoing, a general aspect of the present invention can be summarized as follows. The invention provides that a word n-gram probability is calculated with high accuracy in a situation where:     (a) a first corpus (word-segmented), which is a relatively small corpus containing manually segmented word information, and  (b) a second corpus (word-unsegmented), which is a relatively large corpus containing raw information are given as a training corpus that is storage containing vast quantities of sample sentences.     Vocabulary including contextual information is expanded from words occurring in the first corpus (word-segmented) of relatively small size to words occurring in the second corpus (word-unsegmented) of relatively large size by using a word n-gram probability estimated from an unknown word model and the raw corpus.  <Usage of Word Segmented Corpus>  A first corpus (word-segmented) is used for calculating n-grams and the probability that the boundary between two adjacent characters will be the boundary of two words (segmentation probability). A second corpus (word-unsegmented), in which probabilistic word boundaries are assigned based on information in the first corpus (word-segmented), is used for calculating a word n-gram.  <Calculation of Probabilistic Word Boundaries>  In the second corpus (word-unsegmented), the segmentation probability calculated from the first corpus (word-segmented) is assigned between characters.  <Character-Wise Unknown-Word Model>  The correspondences between each character and its pronunciations are modeled. Thereby, a kana-kanji conversion model for an unknown word is proposed.  Advantages of the invention include that with a word boundary probability estimating device, a probabilistic language model building device, a kana-kanji converting device, and a method therefor according to the present invention as described above, existing vocabulary/linguistic models concerning the first corpus (word-segmented) are combined with vocabulary/linguistic models built by probabilistically segmenting the second corpus (word-unsegmented), which is a raw corpus, whereby the accuracy of recognition in natural language processing can be improved. Because the capability of a probabilistic language model can be improved simply by collecting sample sentences in a field of interest, application of the present invention to fields for which language recognition technique corpuses not provided can be supported.   BRIEF DESCRIPTION OF THE DRAWINGS  For a more complete understanding of the present invention and the advantages thereof, reference is now made to the following description taken in conjunction with the accompanying drawings, in which:   FIG. 1 illustrates a configuration of a kana-kanji converting device according to the present invention;   FIG. 2 shows a configuration of a kana-kanji converting device and a kana-kanji conversion program that implement a kana-kanji conversion method according to the present invention;   FIG. 3 shows details of the kana-kanji converting device shown in FIG. 2 ;   FIG. 4 shows a probabilistic language model building device;   FIG. 5 is a flowchart of a process for calculating the expected frequency of a character string as a word;   FIG. 6 shows the probability that a word boundary will exist between characters when a character of a particular type is followed by the character of different or the same type;   FIG. 7 is a flowchart of a process for calculating word n-gram probability;   FIG. 8 is a flowchart of an operation of the kana-kanji converting device (language decoding section) shown in FIG. 2 in which the weights of the first corpus (word-segmented) and the second corpus (word-unsegmented) are adjusted with λ;   FIG. 9 shows details of the corpuses used in the experiments to demonstrate advantages of introducing the proposed method in writing applicable documents;   FIG. 10 shows the results of the experiments on the corpuses shown in FIG. 9 ;   FIG. 11 shows details of the corpuses for calculating the precision and the recall as well for evaluating the performance of word segmentation; and   FIG. 12 shows models used in FIG. 11 and the results.   DESCRIPTION OF SYMBOLS        1 . . . Kana-kanji converting device   10 . . . CPU   12 . . . Input device   14 . . . Display device   16 . . . Storage device   18 . . . Recording medium   2 . . . Kana-kanji conversion program   22 . . . Language decoding section   30 . . . Base form pool   300 . . . Vocabulary dictionary   302 . . . Character dictionary   32 . . . Language model   320 . . . First corpus (word-segmented)   322 . . . Second corpus (word-unsegmented)     DETAILED DESCRIPTION OF THE INVENTION  The present invention provides that a word n-gram probability is calculated with high accuracy in a situation where:     (a) a first corpus (word-segmented), which is a relatively small corpus containing manually segmented word information, and  (b) a second corpus (word-unsegmented), which is a relatively large corpus containing raw information are given as a training corpus that is storage containing vast quantities of sample sentences.     Vocabulary including contextual information is expanded from words occurring in the first corpus (word-segmented) of relatively small size to words occurring in the second corpus (word-unsegmented) of relatively large size by using a word n-gram probability estimated from an unknown word model and the raw corpus.  <Usage of Word Segmented Corpus>  The first corpus (word-segmented) is used for calculating n-grams and the probability that the boundary between two adjacent characters will be the boundary of two words (segmentation probability). The second corpus (word-unsegmented), in which probabilistic word boundaries are assigned based on information in the first corpus (word-segmented), is used for calculating a word n-gram.  <Calculation of Probabilistic Word Boundaries>  In the second corpus (word-unsegmented), the segmentation probability calculated from the first corpus (word-segmented) is assigned between characters.  <Character-Wise Unknown-Word Model>  The correspondences between each character and its pronunciations are modeled. Thereby, a kana-kanji conversion model for an unknown word is proposed.  Thus, with a word boundary probability estimating device, a probabilistic language model building device, a kana-kanji converting device, and a method therefor according to the present invention as described above, existing vocabulary/linguistic models concerning the first corpus (word-segmented) are combined with vocabulary/linguistic models built by probabilistically segmenting the second corpus (word-unsegmented), which is a raw corpus, whereby the accuracy of recognition in natural language processing can be improved. Because the capability of a probabilistic language model can be improved simply by collecting sample sentences in a field of interest, application of the present invention to fields for which language recognition technique corpuses not provided can be supported.  Furthermore, even for words that do not appear in the first corpus (word-segmented), candidates can be listed by referring to character-wise frequency information. Moreover, if the word n-gram probability of the second corpus (word-unsegmented) is used, which is a raw corpus, contextual information about unknown words can be used as well.  The following is an example of an advantageous embodiment:  (Operation of Kana-Kanji Converting Device)  Operation of a kana-kanji converting device 1 (in FIGS. 1 and 2 ) to which the present invention can be applied will be described below.  (Kana-Kanji Converting Device 1 )  A kana-kanji converting device 1 to which the present invention can be applied will be described below. FIG. 1 illustrates an exemplary configuration of the kana-kanji converting device 1 to which the present invention can be applied. As shown in FIG. 1 , the kana-kanji converting device 1 according to the present invention includes a CPU 10 including a microprocessor, memory, their peripheral circuits, and the like (not shown), input device 12 such as a mouse and a keyboard, a display device 14 such as a CRT display, and a storage device 16 such as an HDD drive, a DVD drive, and a CD drive.  That is, the kana-kanji converting device 1 has a typical hardware configuration, executes a kana-kanji conversion program 2 (which will be described later with reference to FIG. 2 ), which is supplied in the form of a program recorded on a recording medium 18 , such as a DVD, CD-ROM, or CD-RAM, converts input symbols strings which are input from the keyboard 120 in the input devices 12 and converted into a digital format to create text data, and records the text data on a recording medium 18 placed in the storage device 16 or display the text data on the display device 14 . The kana-kanji converting device can be construed as a smaller unit than the kana-kanji converting device, such as a word boundary probability estimating device, and a probabilistic language model building device (see FIG. 4 ). The same applies to cases where the kana-kanji device is construed as being in the categories of method and program.  (Kana-Kanji Conversion Program 2 )   FIG. 2 shows a configuration of a kana-kanji conversion program 2 that implement a kana-kanji conversion method to which the present invention can be applied. As shown in FIG. 2 , the kana-kanji conversion program 2 includes a language model 32 and a base form pool 30 described above. The language model 32 includes a first corpus (word-segmented) 320 and a second corpus (word-unsegmented) 322 . The kana-kanji conversion program 2 may be stored in the program storage device 16 or may be loaded into a storage device such as a memory (for example, a random access memory) in the CPU 10 (which may be allocated as an array in the program) when being executed.  (Base Form Pool 30 )  Stored in the base form pool 30 is a vocabulary dictionary 300 in which the pronunciations of words occurring in the first corpus (word-segmented) are stored correspondingly to the first corpus (word-segmented) 320 of the language model 32 . Also, all characters constituting the words and their pronunciations are stored in a character dictionary 302 . The character dictionary 302 is sometimes referred to as the single-kanji dictionary.  It is novel to consider combinations of all characters and their pronunciations and associate and store the pronunciations with their occurrence probabilities in the character dictionary. In particular, without this provision, it is impossible to conceive of referring to the occurrence probabilities when applying kana-kanji conversion to a pronunciation.   FIG. 3 shows the details of the base form pool 30 . For example, the pronunciation /takahashi/ is stored for the word (Takahashi, a family name) in the vocabulary dictionary 300 , the pronunciation /kore/ is stored for the word (this), and the pronunciation /kiyo/ is stored for the word (contribution). These are stored as already segmented words.  If this vocabulary dictionary is provided correspondingly to the first corpus (word-segmented), statistics on the probabilities of occurrence (the likelihood that each word will appear) can be readily taken like this: the probability that the word will appear in the first corpus (word-segmented) is 0.010, the probably that will appear is 0.0300, and the probability that will appear is 0.020. While the probability of occurrence is shown as being stored in the first corpus (word-segmented) 320 in FIG. 3 , the storage location is not limited to the example shown in FIG. 3 , provided that it is associated with the first corpus (word-segmented).  Stored in the character dictionary 302 are combinations of all pronunciations of characters. For example, for the spelling of the character , combinations of all of its pronunciations, including /taka/ and /kou/, are stored; and for the spelling of the character , combinations of all of its pronunciations, including /hashi/ and /kyou/, are stored.  Also stored in the character dictionary 302 is a table containing the probabilities of occurrence of pronunciations associated with characters. A 0.7 probability of /taka/occurring and a 0.3 probability of /kou/ occurring are contained in association with the spelling of the character , a 0.7 probability of /hashi/ occurring and a 0.3 probability of /kyou/ occurring are contained in association with the spelling of the character , and a 0.7 probability of /kore/ occurring and 0.3 probability of /ze/ occurring are contained in association with the spelling of the character , and 0.7 probability of /kiyo/ occurring and a 0.3 probability of /sei/ occurring are contained in association with the spelling of the character .  These probabilities of occurrence may not necessarily be included in the character dictionary 302 . They may be stored in a location separate from the character dictionary 302 , provided that correspondences between all the pronunciations and characters are described. In this way, a character-wise unknown-word model is built. Building of the character-wise unknown-word model allows listing of candidates by referring to the occurrence probabilities (frequency information) This is summed in FIG. 4 and will be described later.  (First Corpus (Word-Segmented) 320 )  Details of the first corpus are shown in FIG. 3 . Stored in the first corpus (word-segmented) 320 are character strings made up of a number of characters.  (Second Corpus (Word-Unsegmented) 322 )  Details of the second corpus are shown in FIG. 3 . A word boundary probability estimating device ( FIG. 4 ) calculates the probability of a word boundary existing between characters in the first corpus (word-segmented), refers to the probability, applies it to between characters in the second corpus (word-unsegmented), and estimates the probability of a word boundary existing between them.  Stored in the second corpus (word-unsegmented) 322 are character strings made up of a number of characters. The second corpus (word-unsegmented), which segmentation has not been applied to, is also called the “raw corpus”. Because segmentation requires manual correction and therefore is troublesome as described above, it is preferable that the large second corpus (word-unsegmented) of large size can be used effectively.  It should be noted that, while a model including the first corpus (word-segmented) and second corpus (word-unsegmented) is called a language model 32 herein, models, including those stored in the base form pool 30 , may be referred to as “languages models” in some places herein. The term “language model” as used herein refers to a storage device in which these items of information.  (Language Decoding Section 22 )  A language decoding section 22 decodes an input symbol string into the word string (W′ in the expression 2 below) that provides the highest probability calculated using the base form pool 30 and the language model 32 to the display device 14 or the storage device 16 , and outputs it as text data to display or store it on the display device 14 or the storage device 16 .  According to the present invention, the first corpus (word-segmented) and the second corpus (word-unsegmented) can be linearly interpolated (deleted interpolation) according to the following expression 1. This process will be described later with reference to FIG. 8 .  Pr ( w 1 |w 2, w 3)=λ P 1( w 1 |w 2, w 3)+(1−λ) P 2( w 1 |w 2, w 3)  (1)  In the expression, N=3, 0≦λ≦1, P 1 denotes the first corpus (word-segmented), and P 2 denotes the second corpus (word-unsegmented).  In the following expression 2, P(Y|W) is given by the base form model 30 and P(W) is given by the language model 32 . P(W) can be obtained by calculating a weighted average of the first corpus (word-segmented) 320 and the second corpus (word-unsegmented) 322 according to the value of λ by using the expression 1.  W ′=argmax P ( W|Y )=argmax P ( Y|W ) P ( W )  (2) where y represents the input symbol string (y 1 , y 2 , . . . , yk), W represents the word string (w 1 , w 2 , . . . , w 1 ), and W′ represents the word string (w′ 1 , w′ 2 , . . . , w′ 1 ). (Calculation of Word Boundary Probability)  A method for calculating the probability that a word boundary will exist in the first corpus (word-segmented) 320 will be described below.  In the first corpus (word-segmented) 320 , the character string  (Learning linguistics), is stored as sample sentence. The character string is stored as seven characters, , , , , , , and “∘”. These seven characters are classified into six character types: kanji, symbol, numeric, hiragana, katakana, and alphabetic (characters that cannot be classified into symbol, numeric, nor any of the other character type). The characters in the sequence in the sample sentence can be classified as, “kanji”, “kanji,” “kanji”, “hiragana”, “kanji”, “hiragana”, and “symbol”, respectively.   FIG. 6 shows the probabilities that word boundaries will exist between characters when a character of a particular type is followed by the character of a different or the same type, calculated from the relations in the sequence of the character types in the first corpus (word-segmented) 320 . The probability can be readily calculated if information indicating whether a word boundary already exists between characters can be obtained from the first corpus.  However, even if such information cannot obtained from the first corpus, the next calculation can be performed by setting a probability of 0.50000000 is set for the entire corpus as preliminary information concerning whether or not word boundaries will exist. It should be noted that, although this action may result in a lower accuracy, the technical idea of the present invention can be widely applied to such a case.  Furthermore, even if words are not segmented and therefore information as to whether or not word boundaries already exist is not available, a probability of 1.00000000 for sentences the endpoints of which are known.  In FIG. 6 , it is indicated that the probability that a word boundary will exist between to kanjis is 0.24955045 if one kanji follows the other kanji. Also, it is indicated that the probability of a word boundary will exist between a kanji and a hiragana that follows the kanji is 0.67322202, the probability of a word boundary will exist between a hiragana and a kanji that follows the hiragana is 0.97213218, and the probability of a word boundary exist between a hiragana and a symbol that follow the hiragana is 0.99999955.  Probabilities closer to 1 are higher (likely) and probabilities closer to 0 are lower (unlikely). If word boundaries are already determined (word boundaries already exist), the two values 0 and 1 would be enough for discriminating whether or not text is segmented into words. It should be noted that intermediate values between 0 and 1 are used for probabilistically showing the degrees of segmentations. Of course, any other method that shows the degree of probability may be used.  (Estimation of the Probability of a Word Boundary Existing)  In the second corpus (word-unsegmented) 322 , probabilistic segmentation can be estimated by referring to the probability of a word boundary existing that is calculated from the first corpus (word-segmented) 320 .  In one of the simplest examples, it is estimated that the probability of word boundaries obtained in the first corpus (word-segmented) 320 will directly apply to the second corpus (word-unsegmented). In this case, the values obtained in the first corpus (word-segmented) can be used directly as the probabilities of word boundaries existing, though many other reference methods may be used. It should be noted that the term “reference” or “refer” as used herein has broad meaning in terms of usage.  The probabilities of word boundaries existing that are calculated in the first corpus (word-segmented) are assigned between words in the second corpus (word-unsegmented). For example, if the sample sentence occurs in the second corpus (word-unsegmented), the following boundaries are applied to between two characters along with the probabilities shown in the square brackets.  [1] [0.24955045] [0.24955045] [0.67322202] [0.971213218] [0.67332202] [1] ∘ [1]  This is based on the relations in the sequence of character types shown in FIG. 6 .  That is, if the sample sentence (Reading a declarative sentence.) occurs in the second corpus (word-unsegmented), the following boundaries along with the same probabilities shown in the square brackets that have been used above are applied to between two characters.  [1] [0.24055045] [0.24955045] [0.67332202] [0.97213218] [0.67322202] [0.99999955] ∘ [1]  (Calculation of Word N-Gram Probability)  An n-gram model is a language model for examining how often n character strings or combinations of words will occur in a character string.  If the word segmentation probability (the probability of segmentation between the i-th character and the i+1-th character is represented by Pi) is calculated, the uni-gram of a word w can be calculated as:       fr  ⁡   (  w  )    =    ∑   i  ∈   0  1     ⁢      ⁢     P  i   ⁡   [    ∏   j  =  1    k  -  1    ⁢      ⁢   (   1  -   P   i  +  j     )    ]    ⁢   P   i  +  k             O  1   =   {    i  |   x   i  +  1    i  +  k     =  w   }       The frequency of the uni-gram in this example can be calculated as follows.  This means that the probability of the word n-gram is calculated. The uni-gram (N=1) probability can be calculated by using the relation between the preceding and succeeding characters of the character constituting the word uni-gram, in the position of its occurrence.  For example, the uni-gram of the word can be calculated as:  :1×(1−0.24955045)×(1−0.24955045)  Furthermore, the uni-gram of the word having the longer character string can be calculated as:  :1×(1−0.24955045)×(1−0.24955045)×(1−0.67332202)  The uni-gram of the word can be calculated as:  :0.6733202×(1−0.97213218)×0.6733202  However, the expression yields an extremely low value. Accordingly, it can be estimated that the probability of the word occurring is extremely low, that is, a kanji is unlikely to occur subsequently to a hiragana. This can be understood empirically.  A typical word n-gram probability can be calculated by expanding the above expression. For example bi-gram probability can be calculated as follows:       fr  ⁡   (   w  ⁢      ⁢  1  ⁢  W  ⁢      ⁢  2   )    =    ∑   i  ∈  02    ⁢      ⁢   (     P  i   ⁡   [    ∏   j  =  1    k  -  1    ⁢      ⁢   (   1  -   P   i  +  j     )    ]    ⁢   P   i  +  k    ×   [    ∏   j  =  1    1  -  1    ⁢      ⁢   (   1  -   P   i  +  K  +  j     )    ]   ⁢   P   i  +  k  +  1     )              ⁢    O  2   =   {    i  |   x   i  +  1    i  +  k     =    w  ⁢      ⁢   1  ⋀   x   i  +  K  +  1    i  +  k  +  1      =   w  ⁢      ⁢  2     }        A method for efficiently calculating the expected frequency of the character string x 1 x 2 . . . x k will be described below with reference to FIG. 5 .   FIG. 5 shows a flow chart of a process of calculating the expected frequency of the character string x 1 x 2 . . . x k in the second corpus (word-unsegmented) 322 . In S 200 , the probability P int that a word boundary does not exist in the character string of interest can be calculated.  Here, the probability that a word boundary will exist in a character string of interest, (consisting of four characters) will be calculated. This word is a proper noun and contains characters of different types. The word boundary probabilities will be represented on a character-type basis.  The character string of interest consists of four characters of three character types: katakana, katakana, kanji, and alphabetic.  The probability P int that a word boundary does not exist in the character string of interest can be calculated as: (1−0.05458673) (1−0.90384580) (1−0.99999955).  At step S 210 , the position in the second corpus (word-unsegmented) 322 at which the character string of interest occurs is sought.  For example, suppose that the character string of interest is found as follows:   (where in fact there are other character strings preceding and succeeding the character string of interest, which are represented by the suspension points to show the omission; the same applies to the following description).  A hiragana, precedes the character string of interest and a hiragana succeeds the character string of interest. Accordingly, P sum is calculated as: (1−0.99999955) (1−0.99999955).  At step S 230 , the next occurrence position of the character string of interest is sought. If at step S 240 the character string of interest (a vicinity pattern) is found as   then the process returns to S 220 . The symbol precedes the character string of interest and the symbol succeeds the character string of interest.  Accordingly, P sum is calculated as (1−0.99999955) (1−0.99999955) and is added to the P sum calculated above.  Such addition is repeated until the character string of interest is no longer found in the second corpus (word-unsegmented) 322 at S 240 , then P int ×P sum is calculated finally at S 250 . In this way, P int and P sum are calculated separately, thereby efficiently calculating the frequency of occurrence of the character string. The calculation process shown in FIG. 5 is called twice in the flow shown in FIG. 7 as a subroutine.  Other methods for calculating the segmentation probability may be used such as methods using a decision tree or PPM. By using these methods, broader range of character strings can be referred to. The technical idea of the present invention is not limited to these. Any other methods can be used that can describe the existence of a word boundary between characters.  Using the word segmentation probabilities, the second corpus (word-unsegmented) 322 can be treated as a corpus in which characters are segmented at a character boundary (between xi and xi+1) with a probability Pi.  All occurrences of the spelling of a word w in a raw corpus is given by  O  1  ={i|x  i+1  i+k  =w}   Then, the probabilistic frequency fr of a word w in a raw corpus can be defined as the sum of probabilistic frequencies as follow:       fr  ⁡   (  w  )    =    ∑   i  ∈   0  1     ⁢      ⁢     P  i   ⁡   [    ∏   j  =  1    k  -  1    ⁢      ⁢   (   1  -   P   i  +  j     )    ]    ⁢   P   i  +  k          This shows that fr is the expected frequency of w in the raw corpus.  Therefore, the word uni-gram probability can be represented as:        P  r   ⁡   (  w  )    =     f  r   ⁡   (  w  )    /    f  r   ⁡   (  ·  )          where         f  r   ⁡   (  ·  )    =   1  +    ∑   i  =  1    nr  -  1    ⁢      ⁢   P  i          FIG. 7 shows a method for calculating the word n-gram probability P(W n |W 1 , W 2 , . . . , W n−1 ). At steps S 400 and S 430 , the subroutine shown in FIG. 5 is called. In the process, f 2 /f 1 is calculated. If f 1 is 0, the f 2 /f 1 is indeterminate and therefore 0 is returned at S 420 . On the other hand, if f 1 is not 0, the expected frequency f 2 of W 1 , W 2 , . . . , W n is calculated at S 430 and f 2 /f 1 is returned at S 440 .  (Kana-Kanji Conversion Using the Second Corpus (Word-Unsegmented))  The language decoding section 22 refers to both of the vocabulary dictionary 300 and the character dictionary 302 in the base form pool 300 . At step S 100 , the language decoding section 22 receives an input symbols string from the keyboard. At step S 102 , the language decoding section 22 lists possible input symbol strings and their probabilities. As summarized in FIG. 4 , the fact that the probabilistic language model has been built by using the second corpus (word-unsegmented) contributes to the listing of new probabilities for selecting conversion candidates. At step S 120 , λ is set. At step S 122 , the language decoding section 22 refers to the first corpus (word-segmented) 320 and the second corpus (word-unsegmented) 322 by assigning weights to them according to the expression 1 given earlier, where λ≠1. At step S 124 , the language decoding section 22 sequentially outputs the word string with the highest occurrence probability as text data representing the result of the kana-kanji conversion. It should be noted that the word string can be correctly provided even though it has a probability as low as 0.001. This means that the input symbol string /takahashikorekiyo/ can be correctly converted into the proper noun even though the input symbol string represents an unknown word.   FIG. 4 summarizes the relations. The words and / occur with probabilities of 0.010, 0.030, and 0.020, respectively, in the first corpus (word-segmented) only. However, the multiplication of these probabilities produces a value of 0.0006, which is the probably of these words being candidates. Because the word string has a higher probability of 0.001, it is evident that the symbol string was correctly converted.  This is because it can be estimated from the character-wise unknown word model that the character sting occurs in the second corpus (word-unsegmented) and that the input symbol string /takahashikorekiyo/ corresponds to with a constant probability of 0.001. If the word string were not correctly provide even with a probability as low as 0.001, it would be incorrectly converted to , which is a string of known words with high occurrence frequencies. This is because a sample (bi-gram) in which /kore/” is followed by /kiyo/” does not occur.  First Experiment  Advantages of introducing the proposed method in writing applicable documents will be demonstrated below. FIG. 9 shows details of the corpuses used in the experiment.   FIG. 10 shows the results of comparative experiments on eight models using the corpuses. Model C represents the proposed method. The testing was conducted using the investigation 323 sentences. Both in the testing and training, documents similarly relating with each other are not include in both of the corpuses.  Comparing with model A, model A′, and Model B, it is evident that the accuracy was increased by the introduction of the first corpus (word-segmented). Also, comparing model B with model B′, model C with model C′, and model D with model D′, it can be seen that that the advantage of the models that allow bi-grams or higher order to be calculate.  Furthermore, comparing model C with model D, it can be seen that the proposed method efficiently uses the unsegmented corpus in the application field.  Second Experiment  An experiment was conducted to also calculate the precision and the recall for evaluating the performance of word segmentation. The evaluation is based on the result of kana-kanji conversion of a katakana spelling and the numbers of the characters in the correct longest common subsequence (LCS). Letting the number of the characters contained in a spelling in the first corpus (word-segmented) be N C , the number of the characters contained in the result of kana-kanji conversion be N SYS , and the number of the characters in the longest common subsequence be N LCS , then the recall can be defied as N LCS /N C and the precision can be defined as N LCS /N SYS .   FIG. 11 shows the corpuses used in this experiment. FIG. 12 shows the models used in the experiment and the result. The test was conducted by using 509,261 characters in the EDR corpus. These experiments show that the proposed method (model C) has an advantage over models A and B both in precision and recall.  (Variation)  One may want to use the second corpus (word-unsegmented) more often in some technical fields than in others. A method according to the present invention can readily control this by adjusting the weight in linear interpolation with the first corpus (word-segmented). This is based on the concept that the n-gram probability estimated from the second corpus (word-unsegmented) is less accurate than a language model estimated from a corpus in which words are manually and precisely segmented.  Variations described for the present invention can be realized in any combination desirable for each particular application. Thus particular limitations, and/or embodiment enhancements described herein, which may have particular advantages to a particular application need not be used for all applications. Also, not all limitations need be implemented in methods, systems and/or apparatus including one or more concepts of the present invention. Methods may be implemented as signal methods employing signals to implement one or more steps. Signals include those emanating from the Internet, etc.  The present invention can be realized in hardware, software, or a combination of hardware and software. A visualization tool according to the present invention can be realized in a centralized fashion in one computer system, or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system—or other apparatus adapted for carrying out the methods and/or functions described herein—is suitable. A typical combination of hardware and software could be a general purpose computer system with a computer program that, when being loaded and executed, controls the computer system such that it carries out the methods described herein. The present invention can also be embedded in a computer program product, which comprises all the features enabling the implementation of the methods described herein, and which—when loaded in a computer system—is able to carry out these methods.  Computer program means or computer program in the present context include any expression, in any language, code or notation, of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after conversion to another language, code or notation, and/or reproduction in a different material form.  Thus the invention includes an article of manufacture which comprises a computer usable medium having computer readable program code means embodied therein for causing a function described above. The computer readable program code means in the article of manufacture comprises computer readable program code means for causing a computer to effect the steps of a method of this invention. Similarly, the present invention may be implemented as a computer program product comprising a computer usable medium having computer readable program code means embodied therein for causing a function described above. The computer readable program code means in the computer program product comprising computer readable program code means for causing a computer to affect one or more functions of this invention. Furthermore, the present invention may be implemented as a program storage device readable by machine, tangibly embodying a program of instructions executable by the machine to perform method steps for causing one or more functions of this invention.  It is noted that the foregoing has outlined some of the more pertinent objects and embodiments of the present invention. This invention may be used for many applications. Thus, although the description is made for particular arrangements and methods, the intent and concept of the invention is suitable and applicable to other arrangements and applications. It will be clear to those skilled in the art that modifications to the disclosed embodiments can be effected without departing from the spirit and scope of the invention. The described embodiments ought to be construed to be merely illustrative of some of the more prominent features and applications of the invention. Other beneficial results can be realized by applying the disclosed invention in a different manner or modifying the invention in ways known to those familiar with the art.     Claims ( 17 )     1. A word boundary probability estimating device comprising: a memory and a processor device;  a first corpus comprising a first character string comprising a plurality of characters, the characters word-segmented manually;  a second corpus, the second corpus larger than the first corpus and comprising a second character string, the second character string comprising a plurality of word-unsegmented characters;  means for calculating a probability that a word boundary exists between each of the plurality of characters in the first character string, the calculating comprising: determining if a sequence of characters matching the plurality of characters already exists in the first corpus;  determining if a word boundary already exists between the plurality of characters in the first character string in response to determining that a matching sequence exists;  calculating the probability that a word boundary exists in the plurality of characters based on relations in the sequence of character types of successive characters in the first character string in the first corpus by using only the probability of a word boundary already existing between characters of those character types in response to determining that a word boundary already exists between the plurality of characters in the first character string; and  using a probability of 0.5 in response to determining that the sequence matching the plurality of characters does not exist in the first corpus; and   means for estimating the probability that a word boundary exists in the plurality of non-word-segmented characters in the second corpus by referring to the calculated probability of the boundaries between each of the characters in the characters of the first character string.       2. The word boundary probability estimating device according to claim 1 , wherein the probability that a word boundary exists between the plurality of characters in the first character string is set to 1.0 for known sentence end points.       3. The word boundary probability estimating device according to claim 1 , wherein the calculated probability that a word boundary exists includes values between 0 and 1 capable of probabilistically indicating a degree of segmentation as well as values of 0 and 1 indicating whether or not segmentation is already determined, in order to discriminate whether the first character string including the plurality of characters is segmented or not.       4. The word boundary probability estimating device according to claim 1 ; further comprising: word n-gram probability calculating means for calculating the probability that a word boundary exists on the basis of a relation between a preceding character and a subsequent character at a position of each occurrence of a character string including one or more characters constituting a word n-gram, the probability being a word n-gram probability, and thereby forming a probabilistic language model building device.        5. The word boundary probability estimating device according to claim 1 ; further comprising: word n-gram probability calculating means for calculating the probability that a word boundary exists on the basis of a decision tree in which relations between a plurality of characters constituting a word n-gram are described or on the basis of Prediction by Partial Match (PPM), the probability being a word n-gram probability, and thereby forming a probabilistic language model building device.        6. The word boundary probability estimating device according to claim 4 ; further comprising: the first character string which is segmented into at least two words each word including one or more characters, and storing an occurrence probability indicating a likelihood of an occurrence of each of the segmented words in association with the segmented word;  the second character string further comprising a word n-gram probability calculated by the probabilistic language model building device;  a vocabulary dictionary storing, correspondingly to the first corpus, pronunciations associated with each of words segmented as known words;  a character dictionary storing, correspondingly to the first corpus, a pronunciation and spelling of each character in a plurality of conversion candidates that can be converted from an unknown word in association with each other so that that the plurality of conversion candidates can be listed for the unknown word, and storing the occurrence probability of a pronunciation of each character; and  a language decoding section converting a input spelling into a conversion candidate by referring to the occurrence probabilities associated with each word stored in the first corpus and the occurrence probabilities of pronunciations of each character stored in the character dictionary, and the word n-gram probability stored in the second corpus for an input pronunciation, and thereby forming a kana-kanji converting device.       7. A word boundary probability estimating method comprising the steps of: calculating, on a computer processor, a probability that a word boundary exists between each of a plurality of characters in a first character string stored in a first corpus, the calculating comprising:  determining if a sequence of characters matching the plurality of characters already exists in the first corpus;  determining if a word boundary already exists between the plurality of characters in the first character string in response to determining that a matching sequence exists;  calculating the probability that a word boundary exists in the plurality of characters based on relations in the sequence of character types of successive characters in the first character string in the first corpus by using only the probability of a word boundary already existing between characters of those character types in response to determining that a word boundary already exists between the plurality of characters in the first character string; and  using a probability of 0.5 in response to determining that the sequence matching the plurality of character does not exist in the first corpus; and  estimating, on a computer processor, the probability that a word boundary will exist in a plurality of non-word-segmented characters stored in a second corpus by referring to the calculated probability of the boundaries between each of the plurality of characters in the first character string.       8. The word boundary probability estimating method according to claim 7 , wherein the step of calculating the probability that a word boundary exists between each of a plurality of characters is set to 1.0 for known sentence end points.       9. The word boundary probability estimating method according to claim 7 , wherein the calculated probability that a word boundary exists includes values between 0 and 1 capable of probabilistically indicating a degree of segmentation as well as values of 0 and 1 indicating whether or not segmentation is already determined, in order to discriminate whether the first character string including the plurality of characters is segmented or not.       10. The word boundary probability estimating method according to claim 7 , further comprising the step of calculating the probability that a word boundary exists on the basis of a relation between a preceding character and a subsequent character at a position of each occurrence of a character string including at least one character constituting a word n-gram, the probability being a word n-gram probability, thereby constituting a probabilistic language model building method.      11. The word boundary probability estimating method according to claim 7 , further comprising the step of calculating the probability that a word boundary exists on the basis of a decision tree in which relations between a plurality of characters constituting a word n-gram are described or on the basis of Prediction by Partial Match (PPM), the probability being a word n-gram probability, thereby constituting a probabilistic language model building method.      12. The word boundary probability estimating method according to claim 10 , further comprising the steps of: referring to the first character string which is segmented into at least two words each word including at least one character, and storing an occurrence probability indicating a likelihood of an occurrence of each of the segmented words in association with the segmented word;  referring the second corpus further storing a word n-gram probability calculated by the probabilistic language model building device according to a probabilistic language model building method;  referring to a vocabulary dictionary storing, correspondingly to the first corpus, pronunciations associated with each of words segmented as known words;  referring to a character dictionary storing, correspondingly to the first corpus, pronunciations and spelling of each character in a plurality of conversion candidates that can be converted from an unknown word in association with each other so that that the plurality of conversion candidates can be listed for the unknown word and storing the occurrence probability of a pronunciation of each character; and  converting a input spelling into a conversion candidate by referring to the occurrence probabilities associated with each word stored in the first corpus and the occurrence probabilities of pronunciations of each character stored in the character dictionary, and the word n-gram probability stored in the second corpus for an input pronunciation, thereby forming a kana-kanji conversion method.       13. The word boundary probability estimating method according to claim 11 , further comprising the steps of: referring to the character string which is segmented into at least two words each word including at least one character, and storing an occurrence probability indicating the likelihood of occurrence of each of the segmented words in association with the segmented word;  referring to the second corpus further storing a word n-gram probability calculated by a probabilistic language model building device according to the probabilistic language model building method;  referring to a vocabulary dictionary storing, correspondingly to the first corpus, pronunciations associated with each of words segmented as known words;  referring to a character dictionary storing, correspondingly to the first corpus, pronunciations and spelling of each character in a plurality of conversion candidates that can be converted from an unknown word in association with each other so that that the plurality of conversion candidates can be listed for the unknown word and storing the occurrence probability of a pronunciation of each character; and  converting a input spelling into a conversion candidate by referring to the occurrence probabilities associated with each word stored in the first corpus and the occurrence probabilities of pronunciations of each character stored in the character dictionary, and the word n-gram probability stored in the second corpus for an input pronunciation, thereby forming a kana-kanji conversion method.       14. The word boundary probability estimating device according to claim 5 ; further comprising: the first character string which is segmented into at least two words each word including at least one character, and storing an occurrence probability indicating a likelihood of occurrence of each of the segmented words in association with the segmented word;  the second corpus storing a character string including a plurality of characters and a word n-gram probability calculated by the probabilistic language model building device;  a vocabulary dictionary storing, correspondingly to the first corpus, pronunciations associated with each of words segmented as known words;  a character dictionary storing, correspondingly to the first corpus, a pronunciation and spelling of each character in a plurality of conversion candidates that can be converted from an unknown word in association with each other so that that the plurality of conversion candidates can be listed for the unknown word, and storing the occurrence probability of a pronunciation of each character; and  a language decoding section converting a input spelling into a conversion candidate by referring to the occurrence probabilities associated with each word stored in the first corpus and the occurrence probabilities of pronunciations of each character stored in the character dictionary, and the word n-gram probability stored in the second corpus for an input pronunciation, and thereby forming a kana-kanji converting device.        15. An article of manufacture for word boundary probability estimating comprising: a memory device storing the article of manufacture;  a computer processor; and  an application executing on the computer processor, the application executing the method steps of claim 7 .       16. A non-transitory computer readable storage device comprising computer executable instructions tangibly embodied on a computer readable medium that when executed by said computer perform a method for word boundary probability estimating, the method comprising the steps of claim 7 .      17. A non-transitory computer readable storage device comprising computer executable instructions tangibly embodied on a computer readable medium that when executed by said computer perform a method for word boundary probability estimating, the method comprising the steps of claim 12 .        US12126980  2004-07-14  2008-05-26  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building  Active  2026-02-23   US7917350B2 ( en )   Priority Applications (4)     Application Number  Priority Date  Filing Date  Title       JP2004207864A   JP4652737B2 ( en )  2004-07-14  2004-07-14  Word boundary probability estimating apparatus and method, probabilistic language model building apparatus and method, kana-kanji conversion apparatus and method, and, how to build the unknown word model,    JP2004-207864   2004-07-14     US11180153   US20060015326A1 ( en )  2004-07-14  2005-07-13  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building    US12126980   US7917350B2 ( en )  2004-07-14  2008-05-26  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building     Applications Claiming Priority (1)     Application Number  Priority Date  Filing Date  Title       US12126980   US7917350B2 ( en )  2004-07-14  2008-05-26  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building     Related Parent Applications (1)     Application Number  Title  Priority Date  Filing Date       US11180153  Continuation   US20060015326A1 ( en )   2004-07-14  2005-07-13  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building     Publications (2)     Publication Number  Publication Date       US20080228463A1  true  US20080228463A1
                       ( en )   2008-09-18    US7917350B2  true  US7917350B2
                       ( en )   2011-03-29      Family  ID=35600561  Family Applications (2)     Application Number  Title  Priority Date  Filing Date       US11180153  Abandoned   US20060015326A1 ( en )   2004-07-14  2005-07-13  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building    US12126980  Active  2026-02-23   US7917350B2 ( en )   2004-07-14  2008-05-26  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building     Family Applications Before (1)     Application Number  Title  Priority Date  Filing Date       US11180153  Abandoned   US20060015326A1 ( en )   2004-07-14  2005-07-13  Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building     Country Status (2)     Country  Link       US ( 2 )     US20060015326A1 ( en )     JP ( 1 )    JP4652737B2 ( en )      Cited By (4)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US20100070520A1 ( en )  *   2008-09-18  2010-03-18  Fujitsu Limited  Information retrieval method and apparatus     US20110252010A1 ( en )  *   2008-12-31  2011-10-13  Alibaba Group Holding Limited  Method and System of Selecting Word Sequence for Text Written in Language Without Word Boundary Markers     US8364709B1 ( en )  *   2010-11-22  2013-01-29  Google Inc.  Determining word boundary likelihoods in potentially incomplete text     US8914277B1 ( en )  *   2011-09-20  2014-12-16  Nuance Communications, Inc.  Speech and language translation of an utterance     Families Citing this family (87)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US8645137B2 ( en )   2000-03-16  2014-02-04  Apple Inc.  Fast, language-independent method for user authentication by voice     US7937396B1 ( en )   2005-03-23  2011-05-03  Google Inc.  Methods and systems for identifying paraphrases from an index of information items and associated sentence fragments     JP4769031B2 ( en )  *   2005-06-24  2011-09-07  マイクロソフト  コーポレーション  How to create a language model, a kana-kanji conversion method, apparatus, computer program and computer-readable storage medium     US7937265B1 ( en )   2005-09-27  2011-05-03  Google Inc.  Paraphrase acquisition     US7908132B2 ( en )  *   2005-09-29  2011-03-15  Microsoft Corporation  Writing assistance using machine translation techniques     WO2008029881A1 ( en )  *   2006-09-07  2008-03-13  Nec Corporation  Natural language processing system and dictionary registration system     JP5207642B2 ( en )  *   2007-03-06  2013-06-12  ニュアンス  コミュニケーションズ，インコーポレイテッド  System for acquiring a character string to newly recognized as words, METHOD AND COMPUTER PROGRAM     US8977255B2 ( en )   2007-04-03  2015-03-10  Apple Inc.  Method and system for operating a multi-function portable electronic device using voice-activation     JP2010531492A ( en )  *   2007-06-25  2010-09-24  グーグル・インコーポレーテッド  Word probability determination     US8086441B1 ( en )  *   2007-07-27  2011-12-27  Sonicwall, Inc.  Efficient string search     US8010341B2 ( en )  *   2007-09-13  2011-08-30  Microsoft Corporation  Adding prototype information into probabilistic models     US9330720B2 ( en )   2008-01-03  2016-05-03  Apple Inc.  Methods and apparatus for altering audio output signals     US8996376B2 ( en )   2008-04-05  2015-03-31  Apple Inc.  Intelligent text-to-speech conversion     US8046222B2 ( en )  *   2008-04-16  2011-10-25  Google Inc.  Segmenting words using scaled probabilities     US9411800B2 ( en )  *   2008-06-27  2016-08-09  Microsoft Technology Licensing, Llc  Adaptive generation of out-of-dictionary personalized long words     US8595282B2 ( en )  *   2008-06-30  2013-11-26  Symantec Corporation  Simplified communication of a reputation score for an entity     US20100030549A1 ( en )   2008-07-31  2010-02-04  Lee Michael M  Mobile device having human language translation capability with positional feedback     US9959870B2 ( en )   2008-12-11  2018-05-01  Apple Inc.  Speech recognition involving a mobile device     US9858925B2 ( en )   2009-06-05  2018-01-02  Apple Inc.  Using context information to facilitate processing of commands in a virtual assistant     US9431006B2 ( en )   2009-07-02  2016-08-30  Apple Inc.  Methods and apparatuses for automatic speech recognition     US9318108B2 ( en )   2010-01-18  2016-04-19  Apple Inc.  Intelligent automated assistant     US8682667B2 ( en )   2010-02-25  2014-03-25  Apple Inc.  User profiling for selecting user specific voice input processing information     CN102237081B ( en )  *   2010-04-30  2013-04-24  国际商业机器公司  Method and system for estimating rhythm of voice     JP5466588B2 ( en )  *   2010-07-02  2014-04-09  株式会社Ｋｄｄｉ研究所  Word boundary determination device     DE102010040553A1 ( en )  *   2010-09-10  2012-03-15  Siemens Aktiengesellschaft  Speech recognition method     CN102681981A ( en )  *   2011-03-11  2012-09-19  富士通株式会社  Natural language lexical analysis method, device and analyzer training method     US9262612B2 ( en )   2011-03-21  2016-02-16  Apple Inc.  Device access using voice authentication     US10057736B2 ( en )   2011-06-03  2018-08-21  Apple Inc.  Active transport based notifications     US8706472B2 ( en )  *   2011-08-11  2014-04-22  Apple Inc.  Method for disambiguating multiple readings in language conversion     US8994660B2 ( en )   2011-08-29  2015-03-31  Apple Inc.  Text correction processing     CN103186522B ( en )  *   2011-12-29  2018-01-26  富泰华工业（深圳）有限公司  Electronic equipment and natural language analysis method     US9483461B2 ( en )   2012-03-06  2016-11-01  Apple Inc.  Handling speech synthesis of content for multiple languages     US9280610B2 ( en )   2012-05-14  2016-03-08  Apple Inc.  Crowd sourcing information to fulfill user requests     US9721563B2 ( en )   2012-06-08  2017-08-01  Apple Inc.  Name recognition system     US9495129B2 ( en )   2012-06-29  2016-11-15  Apple Inc.  Device, method, and user interface for voice-activated navigation and browsing of a document     US9576574B2 ( en )   2012-09-10  2017-02-21  Apple Inc.  Context-sensitive handling of interruptions by intelligent digital assistant     US9547647B2 ( en )   2012-09-19  2017-01-17  Apple Inc.  Voice-based media searching     JP6055267B2 ( en )  *   2012-10-19  2016-12-27  株式会社フュートレック  String split device, the model file learning device and the character string dividing system     JP5770753B2 ( en )  *   2013-01-15  2015-08-26  グーグル・インコーポレーテッド  Cjk name detection     US9368114B2 ( en )   2013-03-14  2016-06-14  Apple Inc.  Context-sensitive handling of interruptions     WO2014144579A1 ( en )   2013-03-15  2014-09-18  Apple Inc.  System and method for updating an adaptive speech recognition model     US9582608B2 ( en )   2013-06-07  2017-02-28  Apple Inc.  Unified ranking with entropy-weighted information for phrase-based semantic auto-completion     WO2014197336A1 ( en )   2013-06-07  2014-12-11  Apple Inc.  System and method for detecting errors in interactions with a voice-based digital assistant     WO2014197334A3 ( en )   2013-06-07  2015-01-29  Apple Inc.  System and method for user-specified pronunciation of words for speech synthesis and recognition     WO2014197335A1 ( en )   2013-06-08  2014-12-11  Apple Inc.  Interpreting and acting upon commands that involve sharing information with remote devices     JP2016521948A ( en )   2013-06-13  2016-07-25  アップル インコーポレイテッド  System and method for emergency call initiated by voice command     CN103885938B ( en )  *   2014-04-14  2015-04-22  东南大学  Industry spelling mistake checking method based on user feedback     US9620105B2 ( en )   2014-05-15  2017-04-11  Apple Inc.  Analyzing audio input for efficient speech and music recognition     US9502031B2 ( en )   2014-05-27  2016-11-22  Apple Inc.  Method for supporting dynamic grammars in WFST-based ASR     US9430463B2 ( en )   2014-05-30  2016-08-30  Apple Inc.  Exemplar-based natural language processing     US9734193B2 ( en )   2014-05-30  2017-08-15  Apple Inc.  Determining domain salience ranking from ambiguous words in natural speech     US9842101B2 ( en )   2014-05-30  2017-12-12  Apple Inc.  Predictive conversion of language input     US9785630B2 ( en )  *   2014-05-30  2017-10-10  Apple Inc.  Text prediction using combined word N-gram and unigram language models     US10078631B2 ( en )   2014-05-30  2018-09-18  Apple Inc.  Entropy-guided text prediction using combined word and character n-gram language models     US9633004B2 ( en )   2014-05-30  2017-04-25  Apple Inc.  Better resolution when referencing to concepts     US9715875B2 ( en )   2014-05-30  2017-07-25  Apple Inc.  Reducing the need for manual start/end-pointing and trigger phrases     US9760559B2 ( en )   2014-05-30  2017-09-12  Apple Inc.  Predictive text input     EP3149728A1 ( en )   2014-05-30  2017-04-05  Apple Inc.  Multi-command single utterance input method     US9338493B2 ( en )   2014-06-30  2016-05-10  Apple Inc.  Intelligent automated assistant for TV user interactions     JP6269953B2 ( en )  *   2014-07-10  2018-01-31  日本電信電話株式会社  Word segmentation apparatus, method, and program     US9818400B2 ( en )   2014-09-11  2017-11-14  Apple Inc.  Method and apparatus for discovering trending terms in speech requests     US10074360B2 ( en )   2014-09-30  2018-09-11  Apple Inc.  Providing an indication of the suitability of speech recognition     US9668121B2 ( en )   2014-09-30  2017-05-30  Apple Inc.  Social reminders     US10127911B2 ( en )   2014-09-30  2018-11-13  Apple Inc.  Speaker identification and unsupervised speaker adaptation techniques     US9646609B2 ( en )   2014-09-30  2017-05-09  Apple Inc.  Caching apparatus for serving phonetic pronunciations     US9886432B2 ( en )   2014-09-30  2018-02-06  Apple Inc.  Parsimonious handling of word inflection via categorical stem + suffix N-gram language models     US9711141B2 ( en )   2014-12-09  2017-07-18  Apple Inc.  Disambiguating heteronyms in speech synthesis     US9865280B2 ( en )   2015-03-06  2018-01-09  Apple Inc.  Structured dictation using intelligent automated assistants     US9886953B2 ( en )   2015-03-08  2018-02-06  Apple Inc.  Virtual assistant activation     US9721566B2 ( en )   2015-03-08  2017-08-01  Apple Inc.  Competing devices responding to voice triggers     US9899019B2 ( en )   2015-03-18  2018-02-20  Apple Inc.  Systems and methods for structured stem and suffix language models     US9703394B2 ( en )  *   2015-03-24  2017-07-11  Google Inc.  Unlearning techniques for adaptive language models in text entry     US9842105B2 ( en )   2015-04-16  2017-12-12  Apple Inc.  Parsimonious continuous-space phrase representations for natural language processing     US10083688B2 ( en )   2015-05-27  2018-09-25  Apple Inc.  Device voice control for selecting a displayed affordance     US10127220B2 ( en )   2015-06-04  2018-11-13  Apple Inc.  Language identification from short strings     US10101822B2 ( en )   2015-06-05  2018-10-16  Apple Inc.  Language input correction     US9697820B2 ( en )   2015-09-24  2017-07-04  Apple Inc.  Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks     US10049668B2 ( en )   2015-12-02  2018-08-14  Apple Inc.  Applying neural network language models to weighted finite state transducers for automatic speech recognition     CN105845133A ( en )  *   2016-03-30  2016-08-10  乐视控股（北京）有限公司  Voice signal processing method and apparatus     US9934775B2 ( en )   2016-05-26  2018-04-03  Apple Inc.  Unit-selection text-to-speech synthesis based on predicted concatenation parameters     US9972304B2 ( en )   2016-06-03  2018-05-15  Apple Inc.  Privacy preserving distributed evaluation framework for embedded personalized systems     US10049663B2 ( en )   2016-06-08  2018-08-14  Apple, Inc.  Intelligent automated assistant for media exploration     US10067938B2 ( en )   2016-06-10  2018-09-04  Apple Inc.  Multilingual word prediction     DK179415B1 ( en )   2016-06-11  2018-06-14  Apple Inc  Intelligent device arbitration and control     CN106339367B ( en )  *   2016-08-22  2018-09-18  内蒙古大学  One kind of Mongolian automatic calibration method     US10043516B2 ( en )   2016-09-23  2018-08-07  Apple Inc.  Intelligent automated assistant     US9864956B1 ( en )   2017-05-01  2018-01-09  SparkCognition, Inc.  Generation and use of trained file classifiers for malware detection     Citations (15)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US5806021A ( en )  *   1995-10-30  1998-09-08  International Business Machines Corporation  Automatic segmentation of continuous text using statistical approaches     US5987409A ( en )   1996-09-27  1999-11-16  U.S. Philips Corporation  Method of and apparatus for deriving a plurality of sequences of words from a speech signal     US6092038A ( en )  *   1998-02-05  2000-07-18  International Business Machines Corporation  System and method for providing lossless compression of n-gram language models in a real-time decoder     US6185524B1 ( en )   1998-12-31  2001-02-06  Lernout & Hauspie Speech Products N.V.  Method and apparatus for automatic identification of word boundaries in continuous text and computation of word boundary scores     US6363342B2 ( en )  *   1998-12-18  2002-03-26  Matsushita Electric Industrial Co., Ltd.  System for developing word-pronunciation pairs     US6411932B1 ( en )  *   1998-06-12  2002-06-25  Texas Instruments Incorporated  Rule-based learning of word pronunciations from training corpora     US20020111793A1 ( en )   2000-12-14  2002-08-15  Ibm Corporation  Adaptation of statistical parsers based on mathematical transform     US20030093263A1 ( en )   2001-11-13  2003-05-15  Zheng Chen  Method and apparatus for adapting a class entity dictionary used with language models     US20030097252A1 ( en )   2001-10-18  2003-05-22  Mackie Andrew William  Method and apparatus for efficient segmentation of compound words using probabilistic breakpoint traversal     US20030152261A1 ( en )  *   2001-05-02  2003-08-14  Atsuo Hiroe  Robot apparatus, method and device for recognition of letters or characters, control program and recording medium     US6738741B2 ( en )  *   1998-08-28  2004-05-18  International Business Machines Corporation  Segmentation technique increasing the active vocabulary of speech recognizers     US20050071148A1 ( en )  *   2003-09-15  2005-03-31  Microsoft Corporation  Chinese word segmentation     US20050091030A1 ( en )  *   2003-10-23  2005-04-28  Microsoft Corporation  Compound word breaker and spell checker     US6983248B1 ( en )  *   1999-09-10  2006-01-03  International Business Machines Corporation  Methods and apparatus for recognized word registration in accordance with speech recognition     US7349839B2 ( en )  *   2002-08-27  2008-03-25  Microsoft Corporation  Method and apparatus for aligning bilingual corpora      Patent Citations (15)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US5806021A ( en )  *   1995-10-30  1998-09-08  International Business Machines Corporation  Automatic segmentation of continuous text using statistical approaches     US5987409A ( en )   1996-09-27  1999-11-16  U.S. Philips Corporation  Method of and apparatus for deriving a plurality of sequences of words from a speech signal     US6092038A ( en )  *   1998-02-05  2000-07-18  International Business Machines Corporation  System and method for providing lossless compression of n-gram language models in a real-time decoder     US6411932B1 ( en )  *   1998-06-12  2002-06-25  Texas Instruments Incorporated  Rule-based learning of word pronunciations from training corpora     US6738741B2 ( en )  *   1998-08-28  2004-05-18  International Business Machines Corporation  Segmentation technique increasing the active vocabulary of speech recognizers     US6363342B2 ( en )  *   1998-12-18  2002-03-26  Matsushita Electric Industrial Co., Ltd.  System for developing word-pronunciation pairs     US6185524B1 ( en )   1998-12-31  2001-02-06  Lernout & Hauspie Speech Products N.V.  Method and apparatus for automatic identification of word boundaries in continuous text and computation of word boundary scores     US6983248B1 ( en )  *   1999-09-10  2006-01-03  International Business Machines Corporation  Methods and apparatus for recognized word registration in accordance with speech recognition     US20020111793A1 ( en )   2000-12-14  2002-08-15  Ibm Corporation  Adaptation of statistical parsers based on mathematical transform     US20030152261A1 ( en )  *   2001-05-02  2003-08-14  Atsuo Hiroe  Robot apparatus, method and device for recognition of letters or characters, control program and recording medium     US20030097252A1 ( en )   2001-10-18  2003-05-22  Mackie Andrew William  Method and apparatus for efficient segmentation of compound words using probabilistic breakpoint traversal     US20030093263A1 ( en )   2001-11-13  2003-05-15  Zheng Chen  Method and apparatus for adapting a class entity dictionary used with language models     US7349839B2 ( en )  *   2002-08-27  2008-03-25  Microsoft Corporation  Method and apparatus for aligning bilingual corpora     US20050071148A1 ( en )  *   2003-09-15  2005-03-31  Microsoft Corporation  Chinese word segmentation     US20050091030A1 ( en )  *   2003-10-23  2005-04-28  Microsoft Corporation  Compound word breaker and spell checker      Non-Patent Citations (9)   * Cited by examiner, † Cited by third party    Title       Brent et al, " Chinese Text segmentation with MBDP-!: Making the most of training Corpora ", 2001, In Proc. of the ACL 2001, pp. 90-97.  *     Lee et al, " Language Model Based Arabic Word Segmentation ", Jul. 2003, In Proc. of the 41st Annual meeting of the ACL, pp. 399-406.  *     Luo et al, " An Iterative Algorithm to Build Chinese Language Models ", InProc. of the 34th Annual meeting of the Association for Computational Linguistics, pp. 139-143.  *     Nagata, " A stochastic Morphological Analyzer Using a Forward-DP Backward- N-Best Search Algorithm ",1994, InProc. COLING'94, pp. 201-207.  *     Nagata, " Japanese OCR Error Correction using Character Shape Similarity and Statistical Language Model ", 1998, InProc. COLING'98, Montreal, Canada, pp. 922-928.  *     Nagata, Masaaki; " A Self-Organizing Japanese Word Segmenter using Heuristic Word Identification adn Re-Estimation "; 1997; pp. 203-215.     Teahan et al, " A Compression-Based Algorithm for Chinese Word Segmentation ", 2000, Association for Computational Linguistics, vol. 26, No. 3, pp. 375-393.  *     W.J. Teahan and John G. Cleary, 1996; " The Entropy of English usin PPM-Based Models " ; 1996; pp. 53-62.     Xue, " Chinese Word Segmentation as Character Tagging ", Feb. 2003, Computational Linguistics and Chinese Language Processing, vol. 8, No. 1, pp. 29-48.  *      Cited By (8)  * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US20100070520A1 ( en )  *   2008-09-18  2010-03-18  Fujitsu Limited  Information retrieval method and apparatus     US8549025B2 ( en )  *   2008-09-18  2013-10-01  Fujitsu Limited  Information retrieval method and apparatus     US20110252010A1 ( en )  *   2008-12-31  2011-10-13  Alibaba Group Holding Limited  Method and System of Selecting Word Sequence for Text Written in Language Without Word Boundary Markers     US8510099B2 ( en )  *   2008-12-31  2013-08-13  Alibaba Group Holding Limited  Method and system of selecting word sequence for text written in language without word boundary markers     US8364709B1 ( en )  *   2010-11-22  2013-01-29  Google Inc.  Determining word boundary likelihoods in potentially incomplete text     US8930399B1 ( en )   2010-11-22  2015-01-06  Google Inc.  Determining word boundary likelihoods in potentially incomplete text     US9239888B1 ( en )   2010-11-22  2016-01-19  Google Inc.  Determining word boundary likelihoods in potentially incomplete text     US8914277B1 ( en )  *   2011-09-20  2014-12-16  Nuance Communications, Inc.  Speech and language translation of an utterance     Also Published As     Publication number  Publication date  Type        US20060015326A1 ( en )   2006-01-19  application     JP2006031295A ( en )   2006-02-02  application     JP4652737B2 ( en )   2011-03-16  grant     US20080228463A1 ( en )   2008-09-18  application      Similar Documents     Publication  Publication Date  Title         Knight et al.    1998   Machine transliteration      US5890103A ( en )    1999-03-30   Method and apparatus for improved tokenization of natural language text      Dunning    1994   Statistical identification of language      US6178396B1 ( en )    2001-01-23   Word/phrase classification processing method and apparatus      Mao et al.    2003   Document structure analysis algorithms: a literature survey      Brants    2000   TnT: a statistical part-of-speech tagger      US7031908B1 ( en )    2006-04-18   Creating a language model for a language processing system      US5510981A ( en )    1996-04-23   Language translation apparatus and method using context-based translation models      US5883986A ( en )    1999-03-16   Method and system for automatic transcription correction      US7680646B2 ( en )    2010-03-16   Retrieval method for translation memories containing highly structured documents      Sproat et al.    2001   Normalization of non-standard words      US6393388B1 ( en )    2002-05-21   Example-based translation method and system employing multi-stage syntax dividing      Van den Bosch et al.    1999   Memory-based morphological analysis      US6581034B1 ( en )    2003-06-17   Phonetic distance calculation method for similarity comparison between phonetic transcriptions of foreign words      Creutz et al.    2007   Unsupervised models for morpheme segmentation and morphology learning      US6848080B1 ( en )    2005-01-25   Language input architecture for converting one text form to another text form with tolerance to spelling, typographical, and conversion errors      US20070100814A1 ( en )    2007-05-03   Apparatus and method for detecting named entity      Denis et al.    2009   Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less human effort      Zitouni et al.    2006   Maximum entropy based restoration of Arabic diacritics      Munoz et al.    2000   A learning approach to shallow parsing      US20010056352A1 ( en )    2001-12-27   Computer -aided reading system and method with cross-language reading wizard      Chen    1996   Building probabilistic models for natural language      US20100161313A1 ( en )    2010-06-24   Region-Matching Transducers for Natural Language Processing      US20060245641A1 ( en )    2006-11-02   Extracting data from semi-structured information utilizing a discriminative context free grammar      US20060206313A1 ( en )    2006-09-14   Dictionary learning method and device using the same, input method and user terminal device using the same      Legal Events     Date  Code  Title  Description      2008-06-30  AS  Assignment    Owner name : REXAM DISPENSING SMT S.A., FRANCE    Free format text : ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:ROSSIGNOL, ERIC;REEL/FRAME:021170/0275    Effective date : 20080526      2011-01-25  AS  Assignment    Owner name : INTERNATIONAL BUSINESS MACHINES CORPORATION, NEW Y    Free format text : ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MORI, SHINSUKE;TAKUMA, DAISUKE;SIGNING DATES FROM 20050824 TO 20050826;REEL/FRAME:025689/0117      2014-09-26  FPAY  Fee payment    Year of fee payment : 4      2018-07-16  MAFP     Free format text : PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552)    Year of fee payment : 8      
https://www.google.com/patents/US6393399
      US6393399B1 - Compound word recognition 
        - Google Patents  Compound word recognition   Download PDF  Info   Publication number  US6393399B1    US6393399B1  US09163422  US16342298A  US6393399B1  US 6393399 B1  US6393399 B1  US 6393399B1  US 09163422  US09163422  US 09163422  US 16342298 A  US16342298 A  US 16342298A  US 6393399 B1  US6393399 B1  US 6393399B1  Authority  US  Grant status  Grant   Patent type     Prior art keywords  compound word  words  speech  method  compound  Prior art date  1998-09-30  Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)   Expired - Lifetime    Application number  US09163422  Inventor  Stijn Van Even  Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)  Nuance Communications Inc  Original Assignee  Scansoft Inc  Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)  1998-09-30 Filing date 1998-09-30 Publication date 2002-05-21 Grant date 2002-05-21 Links    USPTO     USPTO Assignment     Espacenet     Global Dossier     Discuss    Images                                                         Classifications      G — PHYSICS    G10 — MUSICAL INSTRUMENTS; ACOUSTICS    G10L — SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING    G10L15/00 — Speech recognition    G10L15/08 — Speech classification or search    G10L15/18 — Speech classification or search using natural language modelling    G10L15/183 — Speech classification or search using natural language modelling using context dependencies, e.g. language models    G10L15/19 — Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules    G10L15/193 — Formal grammars, e.g. finite state automata, context free grammars or word networks         Abstract   Recognition of a text string is improved by analyzing the text string with respect to information about expected patterns of the parts of speech of words in the text string and by modifying the text string based on the analysis. Analyzing may include comparing the combinations of parts of speech to parts of speech associated with the words in the text string and, if at least one of the combinations of parts of speech matches parts of speech associated with the words, indicating that a compound word should be formed from the words associated with the matched parts of speech.     Description   TECHNICAL FIELD  The invention relates to computer-implemented speech recognition.  BACKGROUND  A typical speech recognition system includes a recognizer and a stored vocabulary of words which the recognizer is capable of recognizing. The recognizer receives information about utterances by a speaker and delivers a corresponding recognized word or string of recognized words drawn from the vocabulary. The stored vocabulary often includes additional information for each of the vocabulary words, such as the word's part of speech (e.g., noun, verb, adverb).  In German, consecutive words in a sentence are frequently concatenated to form compound words. For example, referring to FIG. 1 a, in the string of spoken words “er hört daB der President Wahl Kampf Geschichten geschrieben hat” 8 (which, translated into English, is “he hears that the president has written election campaign stories”), the words “Wahl,” “Kampf,” and “Geschichten” would be combined to form the compound word “Wahlkampfgeschicten.”  Some German speech recognition systems place frequently used compound words in the stored vocabulary to enable them to recognize those words using standard recognition techniques. Other German speech recognition systems are trained with text containing compound words. During training, such systems identify compounds words in the text and also identify the constituent words which make up the compound words. During recognition of German speech, such systems form compound words by concatenating words which were previously identified as making up compound words in the training text.  SUMMARY  In one aspect, a computer is used to improve recognition of a text string including words in a language (e.g., German) having associated parts of speech. The text string is analyzed with respect to information about expected patterns of the parts of speech in the language and modified based on the analysis. The information may include rules descriptive of combinations of parts of speech in the language corresponding to compound words in the language. The combinations of parts of speech may be sequences of parts of speech.  Analyzing may include comparing the combinations of parts of speech to parts of speech associated with the words in the text string and indicating that a compound word should be formed from the words associated with the matched parts of speech if at least one of the combinations of parts of speech matches parts of speech associated with the words. Modifying the text string may include forming a compound word from words in the text string. The compound word may be added to a vocabulary.  Modifying the text string may include replacing words in the text string with the compound word. The modified text string may be added to a list of candidate text strings. The text string may be analyzed with respect to rules descriptive of other, unpreferred combinations of parts of speech in the language corresponding to combinations of words which do not typically form compound words in the language and it may be indicated that a compound word should not be formed from the words associated with the matched parts of speech if at least one of the unpreferred combinations of parts of speech matches parts of speech associated with the words. The unpreferred combinations of parts of speech may correspond to combinations of groups (e.g., pairs) of parts of speech, with the groups corresponding to phrases.  The compound word may be added to a compound word cache. Adding the compound word may include increasing the frequency count of the compound word in the compound word cache. The compound word also may be added to a vocabulary.  The text string may be analyzed with respect to agreement rules descriptive of patterns of agreement of case, number, and gender of words corresponding to combinations of words which do not typically form compound words in the language, and it may be indicated that a compound word should not be formed from the matching words if at least one of the agreement rules matches words in the text string.  The agreement rules may include a rule indicating that if a noun in a subordinate clause matches the case, number, and gender of a preceding determiner, a compound word should not be formed from the noun and subsequent words in the subordinate clause. The agreement rules may include a rule indicating that if a noun in a non-subordinate clause matches the case, number, and gender of a preceding determiner, a compound word should not be formed from words in the noun phrase containing the noun and words subsequent to the noun phrase.  The compound word may be identified as an incorrect compound word, and the compound word may be added to a compound word error cache. Adding the compound word to the compound word error cache may include increasing a frequency of the compound word in the compound word error cache. If the compound word has been identified as an incorrect compound word, it may be indicated that the compound word should not be formed from the words associated with the matched parts of speech. The compound word may be identified as an incorrect compound word in response to action of a user by adding the compound word to a compound word error cache. It may be indicated that the compound word should not be formed from the words associated with the matched parts of speech if the compound word has been identified as an incorrect compound word more frequently than the compound word has not been identified to be an incorrect compound word.  Among the advantages of the invention are one or more of the following.  Use of language-specific compounding rules to recognize compound words allows recognition of compound words which are not in the stored vocabulary. A speech recognition system that is capable of recognizing compound words may, therefore, use a stored vocabulary which contains only ordinary (non-compound) words, or which contains only a small number of frequently-used compound words. Reducing the number of compound words that are stored in the stored vocabulary reduces the amount of time and effort needed to generate the vocabulary and reduces the total size of the vocabulary. The ability to recognize compound words not stored in the vocabulary also potentially increases the total number of recognizable compound words. Reduction in vocabulary size may also result in increased recognition speed. Furthermore, the space that is saved may be used for other purposes, such as storing domain-specific vocabularies.  Use of compounding rules to recognize compound words also facilitates modification of the speech recognition system's compound word recognition capabilities. The set of compound words recognized by the speech recognition system may be changed by adding, deleting, or modifying the compounding rules, rather than by modifying the stored vocabulary. This feature also facilitates addition of compound word recognition capabilities to existing speech recognition systems.  The techniques may be implemented in computer hardware or software, or a combination of the two. However, the techniques are not limited to any particular hardware or software configuration; they may find applicability in any computing or processing environment that may be used for improvement of speech recognition. Preferably, the techniques are implemented in computer programs executing on programmable computers that each include a processor, a storage medium readable by the processor (including volatile and non-volatile memory and/or storage elements), at least one input device, and one or more output devices. Program code is applied to data entered using the input device to perform the functions described and to generate output information. The output information is applied to the one or more output devices.  Each program is preferably implemented in a high level procedural or object-oriented programming language to communicate with a computer system. However, the programs can be implemented in assembly or machine language, if desired. In any case, the language may be a compiled or interpreted language.  Each such computer program is preferably stored on a storage medium or device (e.g., CD-ROM, hard disk or magnetic diskette) that is readable by a general or special purpose programmable computer for configuring and operating the computer when the storage medium or device is read by the computer to perform the procedures described in this document. The system may also be considered to be implemented as a computer-readable storage medium, configured with a computer program, where the storage medium so configured causes a computer to operate in a specific and predefined manner.  Other features and advantages of the invention will become apparent from the following description, including the drawings, and from the claims.  DESCRIPTION OF DRAWINGS  FIG. 1 a is a diagram of a sequence of German words spoken by a user and a sequence of corresponding recognized words.  FIG. 1 b is a diagram of a category sequence corresponding to the sequence of recognized words shown in FIG. 1 a.   FIG. 2 is a block diagram of a computer.  FIG. 3 is a diagram of a choice list of possible sentence choices.  FIG. 4 is a diagram of a sequence of word identifiers and a vocabulary stored in a computer-readable memory.  FIG. 5 is a flow chart of a computer-implemented method for concatenating words in a sequence of words into compound words.  FIG. 6 is a flow chart of a computer-implemented method for matching syntactic templates against a category sequence.  FIG. 7 a is a diagram of a sequence of recognized words, a corresponding category sequence, and a syntactic template.  FIG. 7 b is a diagram of a sequence of recognized words, a corresponding category sequence, and a syntactic template which matches part of the category sequence.  FIG. 7 c is a diagram of a category sequence which includes a boundary flag.  FIG. 8 is a flow chart of a computer-implemented method for applying agreement rules to a category sequence.  FIG. 9 is a flow chart of a method for concatenating words into compound words.  FIGS. 10 a - 10  c are diagrams of a sentence choice in various stages of the compounding process.  FIG. 11 is a diagram of a choice list with a sentence choice including a compound word.  FIG. 12 is a flow chart of a method for adding compound words to a compound word cache and to a vocabulary.  FIG. 13 is a flow chart of a method for correcting an incorrect compound word.  FIG. 14 is a flow chart of a method for improving recognition of compound words.  DETAILED DESCRIPTION  Referring to FIG. 2, to correctly recognize compound words spoken in German or other languages, a computer 202 includes a compounder process 200 stored in a memory 204 . When presented with a sentence choice 10 (FIG. 1 a ) corresponding to a string of German words 8 spoken by a user, the compounder process 200 identifies the words “Wahl,” “Kampf,” and “Geschichten” as words to be concatenated into a compound word, and then concatenates them into the compound word “WahlfKampfGeschichten.”  When a user speaks the string of words 8 into a microphone 206 , analog signals representing the user's speech are sent to the computer 202 , converted from analog into digital form by an analog-to-digital (A/D) converter 208 , and processed by a digital signal processor (DSP) 210 . The processed speech signals are stored as processed speech 211 in memory 204 . A continuous speech recognizer process 212 uses the processed speech 211 to identify the start and end of each spoken sentence, to recognize words in the sentence, and to produce a choice list 220 of possible sentence choices 10 , 14 , and 16 (FIG. 3 ). A suitable continuous speech recognizer process is part of NaturallySpeaking™, available from Dragon Systems, Inc. of West Newton, Mass. Each of the sentence choices 10 , 14 , and 16 represents a possible match for the string of words 8 spoken by the user. The choice list 220 is stored in memory 204 and is ordered such that the most likely correct sentence choice 10 , as determined by the recognizer process 212 , is at the top of the choice list 220 .  The sentence choices 10 , 14 , and 16 are stored in memory 204 as sequences of word identifiers. For example, referring to FIG. 4, sentence choice 10 is represented in memory 204 as a sequence of word identifiers 400 uniquely identifying vocabulary entries in the stored vocabulary 214 . For example, the word “er” 10  a in sentence choice 10 is represented in memory 204 by a word identifier 400  a that matches the “WORD ID” field of a vocabulary entry 408 in the stored vocabulary 214 . The “NAME” field in the vocabulary entry 408 is the string “er,” the “PRONUNCIATION” field contains a pointer to a speech model of the word “er,” and the “CATEGORY TAG” field contains information such as the part of speech of the vocabulary entry 408 , e.g., that it is a noun.  Referring to FIG. 5, the compounder process 200 forms compound words from the words 10  a-j in the most likely correct sentence choice 10 of the choice list 220 as follows. The compounder process 200 creates a category sequence 12 (FIG. 1 b ) containing a sequence of categories 12  a-j corresponding to the words 10  a-j in the most likely correct sentence choice 10 (step 500 ). For example, category 12  e (noun) corresponds to word 10  e (“President”). Each of the categories 12  a-j is derived from the category tag in the corresponding word's vocabulary entry in the stored vocabulary 214 .  The compounder process 200 matches the category sequence 12 against syntactic templates 224 which are also stored in memory 204 (step 502 ). As described in more detail below with respect to FIG. 6, the syntactic templates 224 are used to identify words within the sentence choice 10 which should not be concatenated with other words to form compound words, by defining sequences of word categories which typically do not result in creation of compound words in German.  Each syntactic template 224 includes a pair of phrasal templates drawn from phrasal templates 222 , stored in memory 204 . A phrasal template defines a sequence of word categories. Six phrasal templates used by the compounder process 200 are shown in Table 1, below.                TABLE 1            Phrasal Template        Label  Phrase              PH1  P GAP N      PH2  N/      PH3  N V      PH4  N VV      PH5  oos GAP N      PH6  N+             Within a phrasal template, “P” represents a preposition, “N” represents a noun, “GAP” represents any string of one or more words that does not include a noun or a personal pronoun, “/” represents a past participle, “V” represents a verb infinitive, “VV” represents an inflected verb, “oos” represents a subordinate conjunctor, and “N+” represents one or more nouns. Phrasal template PH 4 , for example, represents a phrase consisting of a noun followed by an inflected verb.  The set of syntactic templates 224 used by the compounder 200 is shown in Table 2, below. Syntactic template R 1 , for example, consists of the phrasal template PH 1 followed by the phrasal template PH 2 . The compounder process 200 uses the syntactic templates 224 shown in Table 2 because, in German, if the categories of a sequence of words match a sequence of categories defined by a syntactic template, then words in the sequence whose categories cross a phrasal template boundary are typically not concatenated to form a compound word.                TABLE 2            Syntactic        Template  Phrasal Templates              R1  PH1 PH2      R2  PH1 PH3      R3  PH1 PH4      R4  PH5 PH2      R5  PH5 PH3      R6  PH5 PH4      R7  PH5 PH6             Referring now to FIG. 6, the compounder process 200 matches the syntactic templates 224 against the category sequence 12 as follows. The compounder process 200 selects a syntactic template (step 600 ), e.g., syntactic template R 7 in Table 2. A pointer p is set to point to the beginning of category sequence 12 (step 601 ). The compounder process 200 compares the selected syntactic template to the category sequence 12 beginning at point p (step 602 ). For example, the compounder process 200 compares syntactic template R 7 (containing the phrasal templates [oos GAP n] and [N+]) to the beginning of category sequence 12 . As shown in FIG. 7 a, since the first category in the selected syntactic template is a subordinate conjunctor and the first category in category sequence 12 is a noun, the comparison fails.  If the comparison fails (decision step 602 ), then the compounder process 200 advances the pointer p to the next category in category sequence 12 (step 607 ) and compares the selected syntactic template against the category sequence 12 beginning at the new point p (step 602 ).  If the comparison at step 602 succeeds, then a boundary flag is placed after the category in the category sequence 12 corresponding to the last word in the first phrasal template of the selected syntactic template (step 604 ). For example, as shown in FIG. 7 b, syntactic template R 7 matches the categories of the words “daB der President Wahl Kampf Geschichten.” As a result, a boundary flag 18 is inserted into category sequence 12 after category 12  e (corresponding to “President”) and before category 12  f (corresponding to “Wahl”), corresponding to the boundary between the two phrasal templates in syntactic template R 7 . The resulting category sequence 12 is shown in FIG. 7 c.   The compounder process 200 continues to match syntactic templates against the category sequence 12 until all syntactic templates have been compared with all subsequences of the category sequence 12 .  Referring again to FIG. 5, after matching the syntactic templates against the category sequence 12 , the compounder process 200 applies agreement rules to the category sequence 12 (step 504 ). The agreement rules make use of agreement of case, gender, and number within the sentence choice 10 to further identify which words within the sentence choice 10 should not be concatenated to form compound words.  A “determiner” is defined as any word that is a definite or indefinite article, a personal pronoun, a demonstrative pronoun, or a possessive pronoun. As shown in FIG. 8, if there are no determiners within the category sequence 12 (decision step 800 ), then the agreement rules are not applicable. Otherwise, the compounder process 200 identifies the first determiner in the category sequence 12 (step 802 ) and identifies the first noun, if any, in the clause begun by the determiner in case, number, and gender (step 804 ). If such a noun is found (decision step 806 ), then: (1) if the noun is in a subordinate clause (decision step 808 ), a boundary flag is placed in the category sequence 12 after the noun (step 810 ) and after each word in the noun phrase following the noun (step 812 ), (2) if the noun is not in a subordinate clause (decision step 808 ), then a boundary flag is placed in the category sequence 12 after the end of the noun phrase (step 814 ). This process is repeated for each determiner in the category sequence 12 . Placement of boundary flags guards against overgeneration of compound words. A greater or fewer number of boundary flags may be placed within the category sequence 12 depending on the extent to which generation of compound words is favored.  Referring again to FIG. 5, after the compounder process 200 applies agreement rules to the category sequence 12 , the compounder process applies compounding rules to the category sequence to determine which words in the sentence choice 10 , if any, should be concatenated into compound words (step 506 ). A compounding rule defines a category sequence. The compounder process 200 concatenates sequences of words whose categories match a sequence of categories defined by a compounding rule, unless there is a boundary flag within the sequence of words. The compounding rules used by the compounding process 200 are shown in Table 3.                TABLE 3            Compounding Rule  Category Sequence              C1  N N      C2  N_N N      C3  P cdz V      C4  a cdz V      C5  P //      C6  P /      C7  P V      C8  a N      C9  a ag      C10  cff N      C11  cff CTR      C12  cff cff      C13  caf N      C14  cdd N      C15  cai N      C16  cai V      C17  cai /      C18  cai //      C19  cai a      C20  cai ag      C21  V L      C22  E cdz V      C23  E //      C24  E /      C25  E V      C26  ZA ZA      C27  ZA cfr ZA      C28  cgl ag      C29  cgl //      C30  cgl /             As used in Table 3, N_N represents a “new noun.” If the compounder process 200 encounters a capitalized word that is not in the recognition vocabulary 214 , the compounder process 200 assumes that the word is a noun and assigns the category N_N to it. As used in Table 3, cdz represents the German preposition “zu,” V represents a verb infinitive, a represents a predicative adjective, ag represents a conjugated adjective, cff represents directions (e.g., North and East), CTR represents a country, state, region, or area, caf represents any month of the yar, cai represents a hyphenated noun (e.g., a noun beginning with Euro- or Geo-), L represents a verb infinitive of the German word “lernen” (to learn), E represents the German word “ein,” ZA represents a number, cfr represents the German word “und,” and cgl represents words that are prepositions and adverbs at the same time. The categories used in Table 3 are derived from a larger set of categories that are assigned to words in the recognition vocabulary 214 .  Referring to FIG. 9, the compounder process 200 concatenates words in the sentence choice 10 into compound words as follows. The compounder process 200 makes a copy 20 (FIG. 10 a ) of the sentence choice 10 and stores the copy in memory 204 (step 900 ). The compounder process selects the first compounding rule (step 902 ) and compares the sequence of categories defined by the compounding rule to the category sequence 12 associated with the sentence choice 10 (step 904 ). If the compounding rule matches any subsequence in the category sequence 12 (decision step 906 ), then a loop 908  a is entered in which for each matching subsequence (step 910 ), the compounder process 200 creates a compound word by concatenating the words in the sentence choice copy 20 corresponding to the subsequence (step 914 ) if the subsequence does not contain a boundary flag (decision step 912 ). The resulting compound word is queued for submission to a compound word cache 216 (step 915 ), described in more detail with respect to FIG. 12, below. The compounder applies the remaining compounding rules to the category sequence 12 (steps 902 - 919 ).  For example, compounding rule C 1 (N N) matches the words “Wahl” 20  f and “Kampf” 20  g in sentence choice copy 20 , so the words 20  f and 20  g are compounded, resulting in the sentence choice copy 20 shown in FIG. 10 b. Compounding rule C 2 (N_N N) matches the compound word “Wahlkampf” 20  k and the word “Geschichten” 20  h, so the words 20  k and 20  h are compounded, resulting in the sentence choice copy 20 shown in FIG. 10 c.   If no compound words were created during application of the compounding rules (decision step 918 ), then application of the compounding rules is complete. Otherwise, the sentence choice copy is added to the top of the choice list 220 (step 920 ). The choice list 220 resulting from application of the compounding rules to the sentence choice 10 is shown in FIG. 11 . The compound words are then added to a compound word cache 216 and to the recognition vocabulary 214 (step 922 ). Adding the compound words to the recognition vocabulary 214 allows the continuous speech recognizer 212 to directly recognize future occurrences of such words without the aid of the compounder process 200 .  The compound word cache 216 contains compound words which have previously been created by the compounder process 200 . Associated with each compound word in the compound word cache 216 is a frequency corresponding to the number of times that the compound word has been recognized. Referring to FIG. 12, compound words that have been queued for submission to the compound word cache 216 are added to the compound word cache 216 and to the recognition vocabulary 214 as follows. The compounder process 200 selects a compound word from the set of compound words (step 1000 ). If the selected compound word is already in the compound word cache (decision step 1002 ), then the frequency of the selected compound word is incremented (step 1004 ).  If the selected compound word is not in the compound word cache (decision step 1002 ), then the selected compound word is added to the compound word cache 216 (step 1008 ) if the compound word cache 216 is not full (decision step 1006 ). If the compound word cache 216 is full (decision step 1006 ), then the oldest compound word in the compound word cache 216 is deleted from the compound word cache 216 and from the recognition vocabulary 214 (step 1012 ). If the deleted compound word is frequently used (e.g., if its frequency is greater than a predetermined threshold frequency) (decision step 1014 ), then the deleted compound word is added to the compound word cache and the recognition vocabulary 214 with a new timestamp corresponding to the current time (step 1016 ). Steps 1012 - 1016 are repeated as necessary until the compound word that is deleted is not a frequently used compound word. The selected compound word is added to the compound word cache 216 and to the recognition vocabulary 214 (step 1008 ).  If there are more compound words in the queue (decision step 1018 ), then the next compound word is selected from the queue (step 1020 ), and steps 1002 - 1016 are repeated. Otherwise, addition of compound words is complete (step 1022 ).  The compounder process 200 may create incorrect compound words. In such cases the user may replace the incorrect compound word with a replacement word. Referring to FIG. 13, when a user replaces an incorrect compound word with a replacement word, the compounder process 200 removes the incorrect compound word from the compound word cache 216 and from the recognition vocabulary 214 (step 1050 ). Compound words which have been identified by the user as incorrect are stored in a compound word error cache 218 . Associated with each compound word in the compound word error cache is a frequency indicating the number of times that the user has identified the compound word as being incorrect. If the incorrect compound word is not in the compound word error cache 218 (decision step 1052 ), then the incorrect compound word is added to the compound word error cache (step 1054 ). Otherwise, the frequency of the incorrect compound word in the compound word error cache 218 is incremented (step 1056 ).  The compounder process 200 can use the compound word error cache 218 to improve recognition of compound words by not generating compound words that were previously identified as incorrect. For example, referring to FIG. 14, a loop 908  b may be used in place of the loop 910  a (FIG. 9) during compound word recognition. For each subsequence of words matching a compound rule (step 910 ), if the subsequence does not contain a boundary flag (decision step 912 ), a candidate compound word is generated by concatenating the sequence of matching words (step 924 ). If the candidate compound word is in the compound error cache (decision step ( 926 ), and the candidate compound word is not in the compound word cache (decision step 928 ), then a compound word is created by concatenating the matched words (step 914 ). If the candidate compound word is in both the compound word error cache (decision step 926 ) and the compound word cache (decision step 928 ), then a compound word is created from the matched words (step 914 ) only if the frequency of the candidate word in the compound word cache is greater than the frequency of the compound word in the compound word error cache (decision step 930 ).  Although elements of the invention are described in terms of a software implementation, the invention may be implemented in software or hardware or firmware, or a combination of the three. Other embodiments are within the scope of the claims.     Claims ( 24 )   What is claimed is:    1. In a system for recognizing the speech in a language, a computer-implemented method for improving recognition of a text string, the text string comprising words associated with parts of speech, the method comprising:  analyzing the text string with respect to information about expected patterns of the parts of speech in the language, the information comprising:  rules descriptive of combinations of parts of speech in the language corresponding to compound words in the language; and  rules descriptive of unpreferred combinations of parts of speech in the language; and  modifying the text string based on the analysis.      2. The method of claim 1 , wherein the combinations comprise sequences.      3. The method of claim 1 , wherein the analyzing step comprises:  comparing the combinations of parts of speech to parts of speech associated with the words in the text string; and  if at least one of the combinations of parts of speech matches parts of speech associated with the words, indicating that a compound word should be formed from the words associated with the matched parts of speech.      4. The method of claim 3 , further comprising:  analyzing the text string with respect to rules descriptive of unpreferred combinations of parts of speech in the language corresponding to combinations of words which do not typically form compound words in the language; and  if at least one of the unpreferred combinations of parts of speech matches parts of speech associated with the words, indicating that a compound word should not be formed from the words associated with the matched parts of speech.      5. The method of claim 4 , further comprising:  analyzing the text string with respect to agreement rules descriptive of patterns of agreement of case, number, and gender of words corresponding to combinations of words which do not typically form compound words in the language; and  if at least one of the agreement rules matches words in the text string, indicating that a compound word should not be formed from the matching words.      6. The method of claim 5 , wherein the agreement rules include a rule indicating that if a noun in a subordinate clause matches the case, number, and gender of a preceding determiner, a compound word should not be formed from the noun and subsequent words in the subordinate clause.      7. The method of claim 5 , wherein the agreement rules include a rule indicating that if a noun in a non-subordinate clause matches the case, number, and gender of a preceding determiner, a compound word should not be formed from words in the noun phrase containing the noun and words subsequent to the noun phrase.      8. The method of claim 3 , wherein the unpreferred combinations of parts of speech correspond to combinations of groups of parts of speech, the groups corresponding to phrases.      9. The method of claim 8 , wherein groups comprise pairs.      10. The method of claim 3 , further comprising:  adding the compound word to a compound word cache.      11. The method of claim 10 , wherein adding the compound word to the compound word cache comprises increasing a frequency of the compound word in the compound word cache.      12. The method of claim 3 , further comprising:  identifying the compound word as an incorrect compound word; and  adding the compound word to a compound word error cache.      13. The method of claim 12 , wherein adding the compound word to the compound word error cache comprises increasing a frequency of the compound word in the compound word error cache.      14. The method of claim 3 , further comprising:  if the compound word has been identified as an incorrect compound word, indicating that the compound word should not be formed from the words associated with the matched parts of speech.      15. The method of claim 14 , wherein the compound word has been identified as an incorrect compound word in response to action of a user by adding the compound word to a compound word error cache.      16. The method of claim 3 , further comprising:  indicating that the compound word should not be formed from the words associated with the matched parts of speech if the compound word has been identified as an incorrect compound word more frequently than the compound word has not been identified to be an incorrect compound word.      17. The method of claim 1 , wherein modifying the text string comprises forming a compound word from words in the text string.      18. The method of claim 17 , further comprising adding the compound word to a vocabulary.      19. The method of claim 17 , wherein modifying the text string comprises replacing words in the text string with the compound word.      20. The method of claim 19 , further comprising:  adding the modified text string to a list of candidate text strings.      21. The method of claim 17 , further comprising:  adding the compound word to a compound word cache.      22. The method of claim 21 , wherein adding the compound word comprises increasing the frequency count of the compound word in the compound word cache.      23. The method of claim 17 , further comprising:  adding the compound word to a vocabulary.      24. The method of claim 1 , wherein the language comprises German.        US09163422  1998-09-30  1998-09-30  Compound word recognition  Expired - Lifetime   US6393399B1 ( en )   Priority Applications (1)     Application Number  Priority Date  Filing Date  Title       US09163422   US6393399B1 ( en )  1998-09-30  1998-09-30  Compound word recognition     Applications Claiming Priority (2)     Application Number  Priority Date  Filing Date  Title       US09163422   US6393399B1 ( en )  1998-09-30  1998-09-30  Compound word recognition    EP19990307567   EP0992979A3 ( en )  1998-09-30  1999-09-24  Compound word recognition     Publications (1)     Publication Number  Publication Date       US6393399B1  true  US6393399B1
                       ( en )   2002-05-21      Family  ID=22589952  Family Applications (1)     Application Number  Title  Priority Date  Filing Date       US09163422  Expired - Lifetime   US6393399B1 ( en )   1998-09-30  1998-09-30  Compound word recognition     Country Status (2)     Country  Link       US ( 1 )     US6393399B1 ( en )     EP ( 1 )    EP0992979A3 ( en )      Cited By (50)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US20030120479A1 ( en )  *   2001-12-20  2003-06-26  Parkinson David J.  Method and apparatus for determining unbounded dependencies during syntactic parsing     US20030233235A1 ( en )  *   2002-06-17  2003-12-18  International Business Machines Corporation  System, method, program product, and networking use for recognizing words and their parts of speech in one or more natural languages     US6708150B1 ( en )  *   1999-09-09  2004-03-16  Zanavi Informatics Corporation  Speech recognition apparatus and speech recognition navigation apparatus     US20040254784A1 ( en )  *   2003-02-12  2004-12-16  International Business Machines Corporation  Morphological analyzer, natural language processor, morphological analysis method and program     US20050043947A1 ( en )  *   2001-09-05  2005-02-24  Voice Signal Technologies, Inc.  Speech recognition using ambiguous or phone key spelling and/or filtering     US20050040952A1 ( en )  *   2000-10-20  2005-02-24  Promega Corporation  RF point of sale and delivery method and system using communication with remote computer and having features to read a large number of RF tags     US20050102278A1 ( en )  *   2003-11-12  2005-05-12  Microsoft Corporation  Expanded search keywords     US20050159948A1 ( en )  *   2001-09-05  2005-07-21  Voice Signal Technologies, Inc.  Combined speech and handwriting recognition     US20050159957A1 ( en )  *   2001-09-05  2005-07-21  Voice Signal Technologies, Inc.  Combined speech recognition and sound recording     US6965857B1 ( en )  *   2000-06-02  2005-11-15  Cogilex Recherches & Developpement Inc.  Method and apparatus for deriving information from written text     US7027987B1 ( en )  *   2001-02-07  2006-04-11  Google Inc.  Voice interface for a search engine     US20060100856A1 ( en )  *   2004-11-09  2006-05-11  Samsung Electronics Co., Ltd.  Method and apparatus for updating dictionary     US20070162281A1 ( en )  *   2006-01-10  2007-07-12  Nissan Motor Co., Ltd.  Recognition dictionary system and recognition dictionary system updating method     US20070168176A1 ( en )  *   2006-01-13  2007-07-19  Vadim Fux  Handheld electronic device and method for disambiguation of compound text input and that employs N-gram data to limit generation of low-probability compound language solutions     US20070260451A1 ( en )  *   2006-03-27  2007-11-08  Casio Computer Co., Ltd.  Information display control apparatus and recording medium recording information display control program     US20080082524A1 ( en )  *   2006-09-28  2008-04-03  Kabushiki Kaisha Toshiba  Apparatus, method and computer program product for selecting instances     US20080228705A1 ( en )  *   2007-03-16  2008-09-18  Expanse Networks, Inc.  Predisposition Modification Using Co-associating Bioattributes     US7444286B2 ( en )   2001-09-05  2008-10-28  Roth Daniel L  Speech recognition using re-utterance recognition     US7464020B1 ( en )  *   2003-09-24  2008-12-09  Yahoo! Inc.  Visibly distinguishing portions of compound words     US20090006079A1 ( en )  *   2007-06-29  2009-01-01  Microsoft Corporation  Regular expression word verification     US20090043795A1 ( en )  *   2007-08-08  2009-02-12  Expanse Networks, Inc.  Side Effects Prediction Using Co-associating Bioattributes     US20090063462A1 ( en )  *   2007-09-04  2009-03-05  Google Inc.  Word decompounder     US7610189B2 ( en )   2001-10-18  2009-10-27  Nuance Communications, Inc.  Method and apparatus for efficient segmentation of compound words using probabilistic breakpoint traversal     US20090309698A1 ( en )  *   2008-06-11  2009-12-17  Paul Headley  Single-Channel Multi-Factor Authentication     US20100005296A1 ( en )  *   2008-07-02  2010-01-07  Paul Headley  Systems and Methods for Controlling Access to Encrypted Data Stored on a Mobile Device     US20100063843A1 ( en )  *   2008-09-10  2010-03-11  Expanse Networks, Inc.  Masked Data Record Access     US20100063830A1 ( en )  *   2008-09-10  2010-03-11  Expanse Networks, Inc.  Masked Data Provider Selection     US20100076950A1 ( en )  *   2008-09-10  2010-03-25  Expanse Networks, Inc.  Masked Data Service Selection     US20100115114A1 ( en )  *   2008-11-03  2010-05-06  Paul Headley  User Authentication for Social Networks     US7747428B1 ( en )   2003-09-24  2010-06-29  Yahoo! Inc.  Visibly distinguishing portions of compound words     US20100169340A1 ( en )  *   2008-12-30  2010-07-01  Expanse Networks, Inc.  Pangenetic Web Item Recommendation System     US20100169342A1 ( en )  *   2008-12-30  2010-07-01  Expanse Networks, Inc.  Pangenetic Web Satisfaction Prediction System     US20100169262A1 ( en )  *   2008-12-30  2010-07-01  Expanse Networks, Inc.  Mobile Device for Pangenetic Web     US20100169313A1 ( en )  *   2008-12-30  2010-07-01  Expanse Networks, Inc.  Pangenetic Web Item Feedback System     US7809574B2 ( en )   2001-09-05  2010-10-05  Voice Signal Technologies Inc.  Word recognition using choice lists     US20110153356A1 ( en )  *   2008-09-10  2011-06-23  Expanse Networks, Inc.  System, Method and Software for Healthcare Selection Based on Pangenetic Data     US8818793B1 ( en )  *   2002-12-24  2014-08-26  At&T Intellectual Property Ii, L.P.  System and method of extracting clauses for spoken language understanding     US8849648B1 ( en )  *   2002-12-24  2014-09-30  At&T Intellectual Property Ii, L.P.  System and method of extracting clauses for spoken language understanding     US9046932B2 ( en )   2009-10-09  2015-06-02  Touchtype Ltd  System and method for inputting text into electronic devices based on text and text category predictions     US9052748B2 ( en )   2010-03-04  2015-06-09  Touchtype Limited  System and method for inputting text into electronic devices     US20150161898A1 ( en )  *   2012-06-04  2015-06-11  Hallmark Cards, Incorporated  Fill-in-the-blank audio-story engine     US9189472B2 ( en )   2009-03-30  2015-11-17  Touchtype Limited  System and method for inputting text into small screen devices     US9230037B2 ( en )   2013-01-16  2016-01-05  Sap Se  Identifying and resolving cache poisoning     US9384185B2 ( en )   2010-09-29  2016-07-05  Touchtype Ltd.  System and method for inputting text into electronic devices     US9424246B2 ( en )   2009-03-30  2016-08-23  Touchtype Ltd.  System and method for inputting text into electronic devices     US20160253990A1 ( en )  *   2015-02-26  2016-09-01  Fluential, Llc  Kernel-based verbal phrase splitting devices and methods     US9460088B1 ( en )  *   2013-05-31  2016-10-04  Google Inc.  Written-domain language modeling with decomposition     US20160336004A1 ( en )  *   2015-05-14  2016-11-17  Nuance Communications, Inc.  System and method for processing out of vocabulary compound words     US20170025117A1 ( en )  *   2015-07-23  2017-01-26  Samsung Electronics Co., Ltd.  Speech recognition apparatus and method     US9659002B2 ( en )   2009-03-30  2017-05-23  Touchtype Ltd  System and method for inputting text into electronic devices     Families Citing this family (1)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US7120582B1 ( en )   1999-09-07  2006-10-10  Dragon Systems, Inc.  Expanding an effective vocabulary of a speech recognition system     Citations (22)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US4181821A ( en )   1978-10-31  1980-01-01  Bell Telephone Laboratories, Incorporated  Multiple template speech recognition system     US4227176A ( en )   1978-04-27  1980-10-07  Dialog Systems, Inc.  Continuous speech recognition method     US4481593A ( en )   1981-10-05  1984-11-06  Exxon Corporation  Continuous speech recognition     US4489435A ( en )   1981-10-05  1984-12-18  Exxon Corporation  Method and apparatus for continuous word string recognition     US4783803A ( en )   1985-11-12  1988-11-08  Dragon Systems, Inc.  Speech recognition apparatus and method     US4805219A ( en )   1987-04-03  1989-02-14  Dragon Systems, Inc.  Method for speech recognition     US4805218A ( en )   1987-04-03  1989-02-14  Dragon Systems, Inc.  Method for speech analysis and speech recognition     US4829576A ( en )   1986-10-21  1989-05-09  Dragon Systems, Inc.  Voice recognition system     US4864501A ( en )  *   1987-10-07  1989-09-05  Houghton Mifflin Company  Word annotation system     US5027406A ( en )   1988-12-06  1991-06-25  Dragon Systems, Inc.  Method for interactive speech recognition and training     US5233681A ( en )   1992-04-24  1993-08-03  International Business Machines Corporation  Context-dependent speech recognizer using estimated next word context     US5267345A ( en )   1992-02-10  1993-11-30  International Business Machines Corporation  Speech recognition apparatus which predicts word classes from context and words from word classes     US5369577A ( en )  *   1991-02-01  1994-11-29  Wang Laboratories, Inc.  Text searching system     US5428707A ( en )   1992-11-13  1995-06-27  Dragon Systems, Inc.  Apparatus and methods for training speech recognition systems and their users and otherwise improving speech recognition performance     DE19510083A1 ( en )   1995-03-20  1996-09-26  Ibm  Method and arrangement for speech recognition in compound words containing languages     US5666465A ( en )  *   1993-12-10  1997-09-09  Nec Corporation  Speech parameter encoder     US5754972A ( en )  *   1992-03-06  1998-05-19  Dragon Systems, Inc.  Speech recognition system for languages with compound words     US5765132A ( en )   1995-10-26  1998-06-09  Dragon Systems, Inc.  Building speech models for new words in a multi-word utterance     US5799274A ( en )  *   1995-10-09  1998-08-25  Ricoh Company, Ltd.  Speech recognition system and method for properly recognizing a compound word composed of a plurality of words     US5893133A ( en )  *   1995-08-16  1999-04-06  International Business Machines Corporation  Keyboard for a system and method for processing Chinese language text     US5907821A ( en )  *   1995-11-06  1999-05-25  Hitachi, Ltd.  Method of computer-based automatic extraction of translation pairs of words from a bilingual text     US5946648A ( en )  *   1996-06-28  1999-08-31  Microsoft Corporation  Identification of words in Japanese text by a computer system      Patent Citations (23)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US4227176A ( en )   1978-04-27  1980-10-07  Dialog Systems, Inc.  Continuous speech recognition method     US4181821A ( en )   1978-10-31  1980-01-01  Bell Telephone Laboratories, Incorporated  Multiple template speech recognition system     US4481593A ( en )   1981-10-05  1984-11-06  Exxon Corporation  Continuous speech recognition     US4489435A ( en )   1981-10-05  1984-12-18  Exxon Corporation  Method and apparatus for continuous word string recognition     US4783803A ( en )   1985-11-12  1988-11-08  Dragon Systems, Inc.  Speech recognition apparatus and method     US4829576A ( en )   1986-10-21  1989-05-09  Dragon Systems, Inc.  Voice recognition system     US4805219A ( en )   1987-04-03  1989-02-14  Dragon Systems, Inc.  Method for speech recognition     US4805218A ( en )   1987-04-03  1989-02-14  Dragon Systems, Inc.  Method for speech analysis and speech recognition     US4864501A ( en )  *   1987-10-07  1989-09-05  Houghton Mifflin Company  Word annotation system     US5027406A ( en )   1988-12-06  1991-06-25  Dragon Systems, Inc.  Method for interactive speech recognition and training     US5369577A ( en )  *   1991-02-01  1994-11-29  Wang Laboratories, Inc.  Text searching system     US5267345A ( en )   1992-02-10  1993-11-30  International Business Machines Corporation  Speech recognition apparatus which predicts word classes from context and words from word classes     US5754972A ( en )  *   1992-03-06  1998-05-19  Dragon Systems, Inc.  Speech recognition system for languages with compound words     US5233681A ( en )   1992-04-24  1993-08-03  International Business Machines Corporation  Context-dependent speech recognizer using estimated next word context     US5428707A ( en )   1992-11-13  1995-06-27  Dragon Systems, Inc.  Apparatus and methods for training speech recognition systems and their users and otherwise improving speech recognition performance     US5666465A ( en )  *   1993-12-10  1997-09-09  Nec Corporation  Speech parameter encoder     DE19510083A1 ( en )   1995-03-20  1996-09-26  Ibm  Method and arrangement for speech recognition in compound words containing languages     US5797122A ( en )  *   1995-03-20  1998-08-18  International Business Machines Corporation  Method and system using separate context and constituent probabilities for speech recognition in languages with compound words     US5893133A ( en )  *   1995-08-16  1999-04-06  International Business Machines Corporation  Keyboard for a system and method for processing Chinese language text     US5799274A ( en )  *   1995-10-09  1998-08-25  Ricoh Company, Ltd.  Speech recognition system and method for properly recognizing a compound word composed of a plurality of words     US5765132A ( en )   1995-10-26  1998-06-09  Dragon Systems, Inc.  Building speech models for new words in a multi-word utterance     US5907821A ( en )  *   1995-11-06  1999-05-25  Hitachi, Ltd.  Method of computer-based automatic extraction of translation pairs of words from a bilingual text     US5946648A ( en )  *   1996-06-28  1999-08-31  Microsoft Corporation  Identification of words in Japanese text by a computer system      Non-Patent Citations (3)   * Cited by examiner, † Cited by third party    Title       Bandara, Upali et al., " Handling German Compound Words in an Isolated-Word Speech Recognizer, " 1991 IEEE Workshop on Speech Recognition, Harriman, NY (Dec. 15-18, 1991).     European Search Report (EP 99 30 7567); Apr. 12, 2000.     Frisch et al.; " Spelling assistance for compound words "; IBM Journal of Research and Development; No. 32(2); pp. 195-200; Mar. 1998.      Cited By (101)  * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US6708150B1 ( en )  *   1999-09-09  2004-03-16  Zanavi Informatics Corporation  Speech recognition apparatus and speech recognition navigation apparatus     US6965857B1 ( en )  *   2000-06-02  2005-11-15  Cogilex Recherches & Developpement Inc.  Method and apparatus for deriving information from written text     US20050040952A1 ( en )  *   2000-10-20  2005-02-24  Promega Corporation  RF point of sale and delivery method and system using communication with remote computer and having features to read a large number of RF tags     US8380502B1 ( en )   2001-02-07  2013-02-19  Google Inc.  Voice interface for a search engine     US8515752B1 ( en )   2001-02-07  2013-08-20  Google Inc.  Voice interface for a search engine     US7027987B1 ( en )  *   2001-02-07  2006-04-11  Google Inc.  Voice interface for a search engine     US8768700B1 ( en )   2001-02-07  2014-07-01  Google Inc.  Voice search engine interface for scoring search hypotheses     US7366668B1 ( en )   2001-02-07  2008-04-29  Google Inc.  Voice interface for a search engine     US20050159948A1 ( en )  *   2001-09-05  2005-07-21  Voice Signal Technologies, Inc.  Combined speech and handwriting recognition     US20050159957A1 ( en )  *   2001-09-05  2005-07-21  Voice Signal Technologies, Inc.  Combined speech recognition and sound recording     US20050043947A1 ( en )  *   2001-09-05  2005-02-24  Voice Signal Technologies, Inc.  Speech recognition using ambiguous or phone key spelling and/or filtering     US7444286B2 ( en )   2001-09-05  2008-10-28  Roth Daniel L  Speech recognition using re-utterance recognition     US7467089B2 ( en )   2001-09-05  2008-12-16  Roth Daniel L  Combined speech and handwriting recognition     US7809574B2 ( en )   2001-09-05  2010-10-05  Voice Signal Technologies Inc.  Word recognition using choice lists     US7526431B2 ( en )   2001-09-05  2009-04-28  Voice Signal Technologies, Inc.  Speech recognition using ambiguous or phone key spelling and/or filtering     US7505911B2 ( en )   2001-09-05  2009-03-17  Roth Daniel L  Combined speech recognition and sound recording     US7610189B2 ( en )   2001-10-18  2009-10-27  Nuance Communications, Inc.  Method and apparatus for efficient segmentation of compound words using probabilistic breakpoint traversal     US7113905B2 ( en )  *   2001-12-20  2006-09-26  Microsoft Corporation  Method and apparatus for determining unbounded dependencies during syntactic parsing     US20060253275A1 ( en )  *   2001-12-20  2006-11-09  Microsoft Corporation  Method and apparatus for determining unbounded dependencies during syntactic parsing     US20030120479A1 ( en )  *   2001-12-20  2003-06-26  Parkinson David J.  Method and apparatus for determining unbounded dependencies during syntactic parsing     US20030233235A1 ( en )  *   2002-06-17  2003-12-18  International Business Machines Corporation  System, method, program product, and networking use for recognizing words and their parts of speech in one or more natural languages     US7680649B2 ( en )  *   2002-06-17  2010-03-16  International Business Machines Corporation  System, method, program product, and networking use for recognizing words and their parts of speech in one or more natural languages     US8849648B1 ( en )  *   2002-12-24  2014-09-30  At&T Intellectual Property Ii, L.P.  System and method of extracting clauses for spoken language understanding     US9484020B2 ( en )   2002-12-24  2016-11-01  At&T Intellectual Property Ii, L.P.  System and method of extracting clauses for spoken language understanding     US8818793B1 ( en )  *   2002-12-24  2014-08-26  At&T Intellectual Property Ii, L.P.  System and method of extracting clauses for spoken language understanding     US9176946B2 ( en )   2002-12-24  2015-11-03  At&T Intellectual Property Ii, L.P.  System and method of extracting clauses for spoken language understanding     US9703769B2 ( en )   2002-12-24  2017-07-11  Nuance Communications, Inc.  System and method of extracting clauses for spoken language understanding     US7684975B2 ( en )  *   2003-02-12  2010-03-23  International Business Machines Corporation  Morphological analyzer, natural language processor, morphological analysis method and program     US20040254784A1 ( en )  *   2003-02-12  2004-12-16  International Business Machines Corporation  Morphological analyzer, natural language processor, morphological analysis method and program     US7464020B1 ( en )  *   2003-09-24  2008-12-09  Yahoo! Inc.  Visibly distinguishing portions of compound words     US7747428B1 ( en )   2003-09-24  2010-06-29  Yahoo! Inc.  Visibly distinguishing portions of compound words     US20050102278A1 ( en )  *   2003-11-12  2005-05-12  Microsoft Corporation  Expanded search keywords     US8311807B2 ( en )  *   2004-11-09  2012-11-13  Samsung Electronics Co., Ltd.  Periodically extracting and evaluating frequency of occurrence data of unregistered terms in a document for updating a dictionary     US20060100856A1 ( en )  *   2004-11-09  2006-05-11  Samsung Electronics Co., Ltd.  Method and apparatus for updating dictionary     US9020819B2 ( en )  *   2006-01-10  2015-04-28  Nissan Motor Co., Ltd.  Recognition dictionary system and recognition dictionary system updating method     US20070162281A1 ( en )  *   2006-01-10  2007-07-12  Nissan Motor Co., Ltd.  Recognition dictionary system and recognition dictionary system updating method     US8090572B2 ( en )   2006-01-13  2012-01-03  Research In Motion Limited  Handheld electronic device and method for disambiguation of compound text input and that employs N-gram data to limit generation of low-probability compound language solutions     US20100153096A1 ( en )  *   2006-01-13  2010-06-17  Vadim Fux  Handheld Electronic Device and Method for Disambiguation of Compound Text Input and That Employs N-Gram Data to Limit Generation of Low-Probability Compound Language Solutions     US8265926B2 ( en )   2006-01-13  2012-09-11  Research In Motion Limited  Handheld electronic device and method for disambiguation of compound text input and that employs N-gram data to limit generation of low-probability compound language solutions     US20070168176A1 ( en )  *   2006-01-13  2007-07-19  Vadim Fux  Handheld electronic device and method for disambiguation of compound text input and that employs N-gram data to limit generation of low-probability compound language solutions     US7698128B2 ( en )  *   2006-01-13  2010-04-13  Research In Motion Limited  Handheld electronic device and method for disambiguation of compound text input and that employs N-gram data to limit generation of low-probability compound language solutions     US8515740B2 ( en )   2006-01-13  2013-08-20  Research In Motion Limited  Handheld electronic device and method for disambiguation of compound text input and that employs N-gram data to limit generation of low-probability compound language solutions     US20070260451A1 ( en )  *   2006-03-27  2007-11-08  Casio Computer Co., Ltd.  Information display control apparatus and recording medium recording information display control program     US8027831B2 ( en )  *   2006-03-27  2011-09-27  Casio Computer Co., Ltd.  Information display control apparatus and recording medium recording information display control program     US20080082524A1 ( en )  *   2006-09-28  2008-04-03  Kabushiki Kaisha Toshiba  Apparatus, method and computer program product for selecting instances     US20080228705A1 ( en )  *   2007-03-16  2008-09-18  Expanse Networks, Inc.  Predisposition Modification Using Co-associating Bioattributes     US8458121B2 ( en )   2007-03-16  2013-06-04  Expanse Networks, Inc.  Predisposition prediction using attribute combinations     US20080228723A1 ( en )  *   2007-03-16  2008-09-18  Expanse Networks, Inc.  Predisposition Prediction Using Attribute Combinations     US20110040791A1 ( en )  *   2007-03-16  2011-02-17  Expanse Networks, Inc.  Weight and Diet Attribute Combination Discovery     US8655899B2 ( en )   2007-03-16  2014-02-18  Expanse Bioinformatics, Inc.  Attribute method and system     US20080228797A1 ( en )  *   2007-03-16  2008-09-18  Expanse Networks, Inc.  Creation of Attribute Combination Databases Using Expanded Attribute Profiles     US8185461B2 ( en )   2007-03-16  2012-05-22  Expanse Networks, Inc.  Longevity analysis and modifiable attribute identification     US20080228735A1 ( en )  *   2007-03-16  2008-09-18  Expanse Networks, Inc.  Lifestyle Optimization and Behavior Modification     US20110184656A1 ( en )  *   2007-03-16  2011-07-28  Expanse Networks, Inc.  Efficiently Determining Condition Relevant Modifiable Lifestyle Attributes     US20110184944A1 ( en )  *   2007-03-16  2011-07-28  Expanse Networks, Inc.  Longevity analysis and modifiable attribute identification     US20080228767A1 ( en )  *   2007-03-16  2008-09-18  Expanse Networks, Inc.  Attribute Method and System     US8224835B2 ( en )   2007-03-16  2012-07-17  Expanse Networks, Inc.  Expanding attribute profiles     US8051033B2 ( en )   2007-03-16  2011-11-01  Expanse Networks, Inc.  Predisposition prediction using attribute combinations     US20080228768A1 ( en )  *   2007-03-16  2008-09-18  Expanse Networks, Inc.  Individual Identification by Attribute     US20110016105A1 ( en )  *   2007-03-16  2011-01-20  Expanse Networks, Inc.  Predisposition Modification     US8630841B2 ( en )  *   2007-06-29  2014-01-14  Microsoft Corporation  Regular expression word verification     US9336201B2 ( en )  *   2007-06-29  2016-05-10  Microsoft Technology Licensing, Llc  Regular expression word verification     US20090006079A1 ( en )  *   2007-06-29  2009-01-01  Microsoft Corporation  Regular expression word verification     US20090043795A1 ( en )  *   2007-08-08  2009-02-12  Expanse Networks, Inc.  Side Effects Prediction Using Co-associating Bioattributes     US20090063462A1 ( en )  *   2007-09-04  2009-03-05  Google Inc.  Word decompounder     US8380734B2 ( en )   2007-09-04  2013-02-19  Google Inc.  Word decompounder     US8046355B2 ( en )   2007-09-04  2011-10-25  Google Inc.  Word decompounder     US8536976B2 ( en )   2008-06-11  2013-09-17  Veritrix, Inc.  Single-channel multi-factor authentication     US20090309698A1 ( en )  *   2008-06-11  2009-12-17  Paul Headley  Single-Channel Multi-Factor Authentication     US8166297B2 ( en )   2008-07-02  2012-04-24  Veritrix, Inc.  Systems and methods for controlling access to encrypted data stored on a mobile device     US8555066B2 ( en )   2008-07-02  2013-10-08  Veritrix, Inc.  Systems and methods for controlling access to encrypted data stored on a mobile device     US20100005296A1 ( en )  *   2008-07-02  2010-01-07  Paul Headley  Systems and Methods for Controlling Access to Encrypted Data Stored on a Mobile Device     US20100063843A1 ( en )  *   2008-09-10  2010-03-11  Expanse Networks, Inc.  Masked Data Record Access     US8452619B2 ( en )   2008-09-10  2013-05-28  Expanse Networks, Inc.  Masked data record access     US20100076950A1 ( en )  *   2008-09-10  2010-03-25  Expanse Networks, Inc.  Masked Data Service Selection     US8326648B2 ( en )   2008-09-10  2012-12-04  Expanse Networks, Inc.  System for secure mobile healthcare selection     US20110153356A1 ( en )  *   2008-09-10  2011-06-23  Expanse Networks, Inc.  System, Method and Software for Healthcare Selection Based on Pangenetic Data     US8200509B2 ( en )   2008-09-10  2012-06-12  Expanse Networks, Inc.  Masked data record access     US20100063830A1 ( en )  *   2008-09-10  2010-03-11  Expanse Networks, Inc.  Masked Data Provider Selection     US20100115114A1 ( en )  *   2008-11-03  2010-05-06  Paul Headley  User Authentication for Social Networks     US8185646B2 ( en )   2008-11-03  2012-05-22  Veritrix, Inc.  User authentication for social networks     US20100169313A1 ( en )  *   2008-12-30  2010-07-01  Expanse Networks, Inc.  Pangenetic Web Item Feedback System     US20100169340A1 ( en )  *   2008-12-30  2010-07-01  Expanse Networks, Inc.  Pangenetic Web Item Recommendation System     US20100169262A1 ( en )  *   2008-12-30  2010-07-01  Expanse Networks, Inc.  Mobile Device for Pangenetic Web     US8255403B2 ( en )   2008-12-30  2012-08-28  Expanse Networks, Inc.  Pangenetic web satisfaction prediction system     US20100169342A1 ( en )  *   2008-12-30  2010-07-01  Expanse Networks, Inc.  Pangenetic Web Satisfaction Prediction System     US8386519B2 ( en )   2008-12-30  2013-02-26  Expanse Networks, Inc.  Pangenetic web item recommendation system     US9659002B2 ( en )   2009-03-30  2017-05-23  Touchtype Ltd  System and method for inputting text into electronic devices     US9189472B2 ( en )   2009-03-30  2015-11-17  Touchtype Limited  System and method for inputting text into small screen devices     US9424246B2 ( en )   2009-03-30  2016-08-23  Touchtype Ltd.  System and method for inputting text into electronic devices     US10073829B2 ( en )   2009-03-30  2018-09-11  Touchtype Limited  System and method for inputting text into electronic devices     US9046932B2 ( en )   2009-10-09  2015-06-02  Touchtype Ltd  System and method for inputting text into electronic devices based on text and text category predictions     US9052748B2 ( en )   2010-03-04  2015-06-09  Touchtype Limited  System and method for inputting text into electronic devices     US9384185B2 ( en )   2010-09-29  2016-07-05  Touchtype Ltd.  System and method for inputting text into electronic devices     US20150161898A1 ( en )  *   2012-06-04  2015-06-11  Hallmark Cards, Incorporated  Fill-in-the-blank audio-story engine     US9230037B2 ( en )   2013-01-16  2016-01-05  Sap Se  Identifying and resolving cache poisoning     US9460088B1 ( en )  *   2013-05-31  2016-10-04  Google Inc.  Written-domain language modeling with decomposition     US20160253990A1 ( en )  *   2015-02-26  2016-09-01  Fluential, Llc  Kernel-based verbal phrase splitting devices and methods     US20160336004A1 ( en )  *   2015-05-14  2016-11-17  Nuance Communications, Inc.  System and method for processing out of vocabulary compound words     US20170025117A1 ( en )  *   2015-07-23  2017-01-26  Samsung Electronics Co., Ltd.  Speech recognition apparatus and method     US9911409B2 ( en )  *   2015-07-23  2018-03-06  Samsung Electronics Co., Ltd.  Speech recognition apparatus and method     Also Published As     Publication number  Publication date  Type        EP0992979A2 ( en )   2000-04-12  application     EP0992979A3 ( en )   2000-05-31  application      Similar Documents     Publication  Publication Date  Title         Collins    2002   Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms      US5689617A ( en )    1997-11-18   Speech recognition system which returns recognition results as a reconstructed language model with attached data values      US6212498B1 ( en )    2001-04-03   Enrollment in speech recognition      US6424983B1 ( en )    2002-07-23   Spelling and grammar checking system      US5787230A ( en )    1998-07-28   System and method of intelligent Mandarin speech input for Chinese computers      US6374224B1 ( en )    2002-04-16   Method and apparatus for style control in natural language generation      US7315818B2 ( en )    2008-01-01   Error correction in speech recognition      US6535849B1 ( en )    2003-03-18   Method and system for generating semi-literal transcripts for speech recognition systems      US7149688B2 ( en )    2006-12-12   Multi-lingual speech recognition with cross-language context modeling      US7640158B2 ( en )    2009-12-29   Automatic detection and application of editing patterns in draft documents      US6581033B1 ( en )    2003-06-17   System and method for correction of speech recognition mode errors      US6188976B1 ( en )    2001-02-13   Apparatus and method for building domain-specific language models      US6910004B2 ( en )    2005-06-21   Method and computer system for part-of-speech tagging of incomplete sentences      US7085716B1 ( en )    2006-08-01   Speech recognition using word-in-phrase command      US20050203738A1 ( en )    2005-09-15   New-word pronunciation learning using a pronunciation graph      US6952665B1 ( en )    2005-10-04   Translating apparatus and method, and recording medium used therewith      US5970449A ( en )    1999-10-19   Text normalization using a context-free grammar      Lowerre et al.    1990   The HARPY speech understanding system      Ward et al.    1994   Recent improvements in the CMU spoken language understanding system      US6266642B1 ( en )    2001-07-24   Method and portable apparatus for performing spoken language translation      US6278968B1 ( en )    2001-08-21   Method and apparatus for adaptive speech recognition hypothesis construction and selection in a spoken language translation system      US20020046025A1 ( en )    2002-04-18   Grapheme-phoneme conversion      US6330530B1 ( en )    2001-12-11   Method and system for transforming a source language linguistic structure into a target language linguistic structure based on example linguistic feature structures      US6163768A ( en )    2000-12-19   Non-interactive enrollment in speech recognition      US6092044A ( en )    2000-07-18   Pronunciation generation in speech recognition      Legal Events     Date  Code  Title  Description      1998-12-21  AS  Assignment    Owner name : DRAGON SYSTEMS, INC., MASSACHUSETTS    Free format text : ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:VAN EVEN, STIJN;REEL/FRAME:009665/0728    Effective date : 19981027      2002-03-28  AS  Assignment    Owner name : L & H HOLDINGS USA, INC., MASSACHUSETTS    Free format text : MERGER;ASSIGNOR:DRAGON SYSTEMS, INC.;REEL/FRAME:012741/0384    Effective date : 20000607    Owner name : SCANSOFT, INC., MASSACHUSETTS    Free format text : ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:L&H HOLDINGS USA, INC.;REEL/FRAME:012741/0392    Effective date : 20011212      2005-11-21  FPAY  Fee payment    Year of fee payment : 4      2005-12-06  AS  Assignment    Owner name : NUANCE COMMUNICATIONS, INC., MASSACHUSETTS    Free format text : CHANGE OF NAME;ASSIGNOR:SCANSOFT, INC.;REEL/FRAME:016851/0772    Effective date : 20051017      2006-04-07  AS  Assignment    Owner name : USB AG, STAMFORD BRANCH, CONNECTICUT    Free format text : SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:017435/0199    Effective date : 20060331    Owner name : USB AG, STAMFORD BRANCH,CONNECTICUT    Free format text : SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:017435/0199    Effective date : 20060331      2006-08-24  AS  Assignment    Owner name : USB AG. STAMFORD BRANCH, CONNECTICUT    Free format text : SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909    Effective date : 20060331    Owner name : USB AG. STAMFORD BRANCH,CONNECTICUT    Free format text : SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909    Effective date : 20060331      2009-11-11  FPAY  Fee payment    Year of fee payment : 8      2013-10-23  FPAY  Fee payment    Year of fee payment : 12      2016-05-20  AS  Assignment    Owner name : DICTAPHONE CORPORATION, A DELAWARE CORPORATION, AS    Free format text : PATENT RELEASE (REEL:017435/FRAME:0199);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0824    Effective date : 20160520    Owner name : INSTITIT KATALIZA IMENI G.K. BORESKOVA SIBIRSKOGO    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : SPEECHWORKS INTERNATIONAL, INC., A DELAWARE CORPOR    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : NUANCE COMMUNICATIONS, INC., AS GRANTOR, MASSACHUS    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : NOKIA CORPORATION, AS GRANTOR, FINLAND    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : DSP, INC., D/B/A DIAMOND EQUIPMENT, A MAINE CORPOR    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : SCANSOFT, INC., A DELAWARE CORPORATION, AS GRANTOR    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : ART ADVANCED RECOGNITION TECHNOLOGIES, INC., A DEL    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : ART ADVANCED RECOGNITION TECHNOLOGIES, INC., A DEL    Free format text : PATENT RELEASE (REEL:017435/FRAME:0199);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0824    Effective date : 20160520    Owner name : DSP, INC., D/B/A DIAMOND EQUIPMENT, A MAINE CORPOR    Free format text : PATENT RELEASE (REEL:017435/FRAME:0199);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0824    Effective date : 20160520    Owner name : TELELOGUE, INC., A DELAWARE CORPORATION, AS GRANTO    Free format text : PATENT RELEASE (REEL:017435/FRAME:0199);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0824    Effective date : 20160520    Owner name : DICTAPHONE CORPORATION, A DELAWARE CORPORATION, AS    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : STRYKER LEIBINGER GMBH & CO., KG, AS GRANTOR, GERM    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : TELELOGUE, INC., A DELAWARE CORPORATION, AS GRANTO    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : SCANSOFT, INC., A DELAWARE CORPORATION, AS GRANTOR    Free format text : PATENT RELEASE (REEL:017435/FRAME:0199);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0824    Effective date : 20160520    Owner name : HUMAN CAPITAL RESOURCES, INC., A DELAWARE CORPORAT    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : MITSUBISH DENKI KABUSHIKI KAISHA, AS GRANTOR, JAPA    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : NUANCE COMMUNICATIONS, INC., AS GRANTOR, MASSACHUS    Free format text : PATENT RELEASE (REEL:017435/FRAME:0199);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0824    Effective date : 20160520    Owner name : NORTHROP GRUMMAN CORPORATION, A DELAWARE CORPORATI    Free format text : PATENT RELEASE (REEL:018160/FRAME:0909);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0869    Effective date : 20160520    Owner name : SPEECHWORKS INTERNATIONAL, INC., A DELAWARE CORPOR    Free format text : PATENT RELEASE (REEL:017435/FRAME:0199);ASSIGNOR:MORGAN STANLEY SENIOR FUNDING, INC., AS ADMINISTRATIVE AGENT;REEL/FRAME:038770/0824    Effective date : 20160520      
https://www.google.com/appserve/mkt/p/APDk4sOJaQoDMm8Sz8lCyRwSb_mP-ZpW31bKrbQ-vgwjuIRSEXz0yAGMhJRgyRKJ2gz6CFuj-5IEFzWFAX2ioQ-9UZ9SWpYgJTpJzFD2KgI3fb6ZqsrZQPwmvocJIaaoHtMa0NjFTLlI6PhG6TIhnbEY5knhAKgI3KZmkVY
                   Updates    This official feed from the G Suite team provides essential information about new features and improvements for G Suite customers.             New ways to comment on Microsoft files (and more) in Google Drive     February 7, 2018         (Cross-posted from The Keyword )   Eric Zhang, Software Engineer, Google Drive  Birkan Icacan , Product Manager, Google Drive     Google Drive makes it easy to store and share files in the cloud so you can collaborate securely with your teams and clients while on the go. And we understand that teams work with all kinds of files and tools to get things done. Starting today, we're making it possible to comment directly on more file types including Microsoft Office files, PDFs and images—without having to convert them into Docs, Sheets or Slides.  When you're collaborating with an external agency, negotiating a contract with a client or coordinating a sales agreement with a supplier, chances are you're dealing with multiple file formats. With this update, you can now comment on those files in Drive the way you're used to in Google Docs. In the Drive preview pane , comment, assign tasks, or mention coworkers and the people you work can reply back, even if they’re not using G Suite. Let’s say your coworker opens a file on her Windows laptop using MS Word, she will see your comment in the file and can reply right from there.        Provide quick feedback on the most popular file formats, including Word, PowerPoint, Excel and PDF files, in the Drive preview pane .  With commenting in Drive preview, you won’t need to open up other tools to complete tasks. Instead, open Microsoft Office files, like Word, Excel and PowerPoint, in Drive preview and give feedback in comments then get back to work.   AODocs , a company that helps enterprises replace their legacy document management applications and automate business processes integrated with Drive, has been testing the new Drive commenting feature for the past few months. As AODocs helps companies transition from legacy systems, they often collaborate across various licensed software.  “We frequently use the comments feature in Google Docs and Slides when preparing specification documents, reviewing project proposals and creating marketing materials,” says Stéphane Donzé, chief executive officer and founder of AODocs. “With commenting in Drive Preview feature, we can now extend the same collaboration, review and validation processes to Microsoft Office Applications, images and PDFs.”       Collaborate on Excel files—as well as other Microsoft files—all within Google Drive.  The ability to comment in Drive preview mode is just one example of interoperability between G Suite and Microsoft Office products. You can also use a Drive plug-in for Outlook to make it easier to insert files stored in Drive to an Outlook email and save incoming attachments to your Drive from Outlook. Our help center has more specifics on how to enable the plug-in for admins.  Of course, you can also convert Microsoft Office files into Docs, Sheets or Slides. In Google Drive, right click on an Office file and click Open with and then select Docs (or Sheets or Slides). Lastly, you can choose to edit Microsoft files without converting them fully using Office Compatibility Mode (OCM).  With G Suite, you can work the way you’re used to working. Get started .   Additional information for G Suite admins  As a part of this launch, we will also change the “Can view” permission setting for MS Office files. This creates further interoperability between Google Drive, Microsoft Office and other popular applications. What is changing? :    Users with “Can view” access to MS Office files will start seeing any comments added after the launch of this change, but they will not be able to reply to those comments or create new comments.  MS Office files with comments added before this change (i.e. using the Google Drive API, or from an older version of this feature) will not be affected.  Comments, actions items and suggested edits in Google Docs, Sheets, Slides and Drawings files will continue to remain invisible to users with “Can view” access.  In addition to making and replying to file-level comments while previewing a file in Google Drive Preview, users can now make new inline (a.k.a. anchored) comments, for example:    Draw a rectangle on a PDF, or PowerPoint (.pptx), or image file and make an anchored comment.  Select a sentence in a MS Office Word (.docx), or PowerPoint (.pptx), or PDF file and make an anchored comment (similar to comments in Google Docs).  Select a cell in MS Office Excel (.xlsx) file and make a cell-based comment.  This new feature also supports importing and exporting comments to the underlying file for certain OOXML file formats such as MS Office file formats(.docx, .xlsx, .pptx) and PDF files. This means that existing comments available in those file formats will be imported and displayed in Drive Preview. Additionally, comments made in Drive Preview will be exported to the underlying file so that when you open up the file using the local client application, you will be able to see the comments made in Drive Preview.   Launch Details  Release track: - Web: Launching to Rapid Release, with Scheduled Release coming in 5 weeks - Mobile: Launching to both Rapid Release and Scheduled Release   Editions: Available to all G Suite editions   Rollout pace:    Web: Extended rollout (potentially longer than 15 days for feature visibility)*  Mobile: Extended rollout (potentially longer than 15 days for feature visibility)     *On web, we’ll roll out these features to Rapid Release domains over the course of three weeks. Two weeks after we complete the rollout to Rapid Release, we’ll begin the rollout to Scheduled Release. We anticipate it will take up to three weeks after that point to roll out to 100% of Scheduled Release domains.   Impact: All end users   Action: Change management suggested/FYI   More Information  Help Center: Comment on Microsoft Office files, PDFs, images, and other files in Google Drive   Launch release calendar   Launch detail categories   Get these product update alerts by email   Subscribe to the RSS feed of these updates                       Google   Labels:   Google Drive , Rapid Release                                                        Labels        Accessibility    Accounts    Admin console    Admin SDK    Android    API    App Maker    Chrome    Chromebox for meetings    Cloud Connect    Cloud Search    Contacts    Data Studio    Developer    DKIM    DLP    Docs    Domain Directory    Drive File Stream    Drive for Mac/PC    Editors    Education Edition    End-user    G Suite    G Suite for Education    G Suite for Government    G Suite Marketplace    G Suite Migration    G Suite Password Sync    G Suite Sync    Gmail    Gmail Add-Ons    Google Apps Script    Google Calendar    Google Classroom    Google Cloud Directory Sync    google contacts    Google Docs    Google Drawings    Google Drive    Google Forms    Google Hangouts    Google Keep    Google Labs    Google Plus    Google Sheets    Google Sites    Google Slides    Google Sync    Google Talk    Google Vault    Google Video    Google Wave    Groups    Hangouts Chat    Hangouts Meet    Hire    Identity    iGoogle    Inbox by Gmail    iOS    Jamboard    Marketplace    MDM    Microsoft Exchange    Microsoft Outlook    Migration    Mobile    New look    Other    Partner Edition    Premier Edition    Project Fi    Rapid Release    Reports API    SAML    Scheduled Release    Security and Compliance    SSO    Standard Edition    Takeout    Team Edition    Training for Google Apps    What's New    YouTube                  Archive                     2018       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2017       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2016       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2015       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2014       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2013       Dec          Nov          Oct          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2012       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2011       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2010       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2009       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2008       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb          Jan                       2007       Dec          Nov          Oct          Sep          Aug          Jul          Jun          May          Apr          Mar          Feb                          Feed               Subscribe by email                Localized G Suite Updates    Español   Français 日本語 Português              Useful Links     Join the official community for G Suite administrators   In the Cloud Connect Community, discuss the latest features with Googlers and other G Suite admins like you. Learn tips and tricks that will make your work and life easier. Be the first to know what's happening with G Suite.  ______________   Learn about more G Suite launches   On the “What’s new in G Suite?” Help Center page, learn about new products and features launching in G Suite, including smaller changes that haven’t been announced on the G Suite Updates blog.  ______________   View active G Suite Beta Programs   G Suite Beta Programs give participating customers an opportunity to help us improve and develop new products and features as well as provide feedback on them, before they’re made generally available.                        Company-wide     Official Google Blog    Public Policy Blog    Student Blog                  Products     Android Blog    Chrome Blog    Lat Long Blog                  Developers     Developers Blog    Ads Developer Blog    Android Developers Blog                           Google    Privacy    Terms           
https://www.google.com/patents/US5213333
      US5213333A - Word association game 
        - Google Patents  Word association game   Download PDF  Info   Publication number  US5213333A    US5213333A  US07706980  US70698091A  US5213333A  US 5213333 A  US5213333 A  US 5213333A  US 07706980  US07706980  US 07706980  US 70698091 A  US70698091 A  US 70698091A  US 5213333 A  US5213333 A  US 5213333A  Authority  US  Grant status  Grant   Patent type     Prior art keywords  word  game  set  die  sets  Prior art date  1991-05-29  Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)   Expired - Fee Related    Application number  US07706980  Inventor  Joseph J. Petrovich  James D. Gazdick  Original Assignee  Petrovich Joseph J Gazdick James D  Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)  1991-05-29 Filing date 1991-05-29 Publication date 1993-05-25 Grant date 1993-05-25 Links    USPTO     USPTO Assignment     Espacenet     Global Dossier     Discuss    Images             Classifications      A — HUMAN NECESSITIES    A63 — SPORTS; GAMES; AMUSEMENTS    A63F — CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR    A63F3/00 — Board games; Raffle games    A63F3/04 — Geographical or like games ; Educational games    A63F3/0423 — Word games, e.g. scrabble          A — HUMAN NECESSITIES    A63 — SPORTS; GAMES; AMUSEMENTS    A63F — CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR    A63F3/00 — Board games; Raffle games    A63F3/00003 — Types of board games    A63F3/00006 — Board games played along a linear track, e.g. game of goose, snakes and ladders, along an endless track    A63F2003/00009 — Board games played along a linear track, e.g. game of goose, snakes and ladders, along an endless track with an intersection in the track          A — HUMAN NECESSITIES    A63 — SPORTS; GAMES; AMUSEMENTS    A63F — CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR    A63F11/00 — Game accessories of general use, e.g. score counters, boxes    A63F11/0051 — Indicators of values, e.g. score counters    A63F2011/0067 — Score or tally sheets          A — HUMAN NECESSITIES    A63 — SPORTS; GAMES; AMUSEMENTS    A63F — CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR    A63F2250/00 — Miscellaneous game characteristics    A63F2250/10 — Miscellaneous game characteristics with measuring devices    A63F2250/1063 — Timers    A63F2250/1068 — Sandglasses         Abstract   A word association game wherein the players derive different but commonly known word sets from elements of a previous commonly known set including a die, a board, a plurality of preprinted cards, a "minute glass" timer, a plurality of pads and writing instruments, and game tokens for movement on the board. The board has a path of adjacent spaces, which path is arranged in a FIG. 8 with the start space at the crossing. The spaces are color coded in one of six different colors. The preprinted cards each have a blind side and a face side. The latter has six different color coded areas with a different word set in each. A word set is a common phrase (e.g. TIE CLASP) or single (e.g. BLACKJACK) which includes a plurality of different word elements (e.g. TIE, CLASP, BLACK, JACK). Each card has different word sets from the other cards with those of one color coding being of a different word sets from the other categories: PEOPLE, PLACES, TITLES, THINGS, GENERAL and SPORTS/GAMES. A player's turn includes throwing a die and drawing a card at random. The player's piece is moved the number of spaces on the die to a space whose color code determines the category and thus the starting word set. The player must in a time period enter successive word sets on the score pad, each entry related to the previous word set by using one of its elements for points and/or movement of tokens. Players alternate turns until one reaches a set goal (e.g. 100 points).     Description   FIELD OF THE INVENTION  This invention relates to a novel word association game for two or more players.  BACKGROUND OF THE INVENTION  Word games provide amusement and education. Many of such games, for example, that disclosed in U.S. Pat. No. 4,854,594 to Eaton, employ a board wherein players move tokens about a board and answer trivia questions related to the space upon which their tokens land. Another game employing trivia questions is disclosed in U.S. Pat. No. 4,884,815 to Glenn, which game also employs decoding a number to letters which are rearranged by the players into words. Others, such as U.S. Pat. No. 4,666,161, Elesie, et al., employ a track or pathway on a board and have players draw cards with words and definitions, and wherein players advance by knowing the correct definition of drawn words. In other word games, such as that shown in U.S. Pat. No. 4,089,527, to Roth, a player advances by memorizing items on cards. Word puzzles are involved in the game of U.S. Pat. No. 4,941,668, to Mobrem, and U.S. Pat. No. 4,911,452 to Marchese requires players to name a series of words which fit within a particular category and start with particular letters. U.S. Pat. No. 4,955,614 to Buda involves guessing particular words by a process of selecting and eliminating letters contained in the word. And U.S. Pat. No. 3,606,336 to Krause is directed to a word association game wherein the player builds words from lettered tiles obeying the rule that successive words start from the last letter of the prior word and have a length derived from chance.  Clever and as educational as these games and others like them may be, there is still room for additional educational and entertaining word games, and especially for one which does not rely on knowledge of definitions, trivia facts, or short time memorization of patterns or items, but instead develops and rewards free association.  SUMMARY OF THE INVENTION  A word association game in accordance with the present invention includes a recorded data base of a plurality of different commonly known word sets (such as TIE CLASP or BLACKJACK), each of which includes a plurality of word elements (such as TIE, CLASP, BLACK, and JACK). Means for randomly selecting one of these word sets (e.g. BLACKJACK) is provided, as well as timing means and means for recording a sequence of word sets, starting from the one which, to count toward winning, must be commonly known, and sets employing one word element (e.g. JACK FROST, FROST BITE, BITE THE BULLET, BULLET TRAIN) from the preceding word set of the sequence. A player (or team) advances relative to his competitors by recording word sets in a sequence in a set period, and thus the game rewards and encourages quick wit and free association of commonly known phrases or word sets.  The invention, together with further advantages and features thereof, may best be understood by reference to the following description taken in connection with the accompanying drawings, in the several figures of which like reference numerals identify like elements.  BRIEF DESCRIPTION OF THE DRAWINGS  FIG. 1 is a plan view of a preferred embodiment of a game board constructed in accordance with the present invention.  FIG. 2 is a perspective view of the equipment (namely, score pads, writing instruments, die, timer, pre-recorded word set cards [two of which are shown on a slightly enlarged scale for clarity], and player tokens), which together with the board of FIG. 1 is the presently preferred best mode of practicing the present invention.  DETAILED DESCRIPTION OF ONE PREFERRED EMBODIMENT  Referring to FIG. 1, there is depicted a board 10 having a plurality of spaces 11-53 arranged in a path or track 8 which is laid out in the general shape of a FIG. 8. Each of the 43 individual spaces has its center color coded in one of six colors: red, green, blue, yellow, orange or purple. The cross-over space 11 serves as the "start" and is so labeled. A player or team of players chooses a token (such as one of the six tokens 61-66 of FIG. 2) which for ease of identification are differentiated from each other by color (different tokens 61-66 being red, blue, orange, green, yellow or purple).  The direction of travel on the path 8 is one-way in the directions of the arrows shown on spaces 12, 32, 33 and 53. The only choice of direction is upon leaving "start" space 11, and that is preferably the choice of the player or team. Once committed, the token travels the FIG. 8 track 8 going by the cross-over space 11 in a straight line form space 53 to space 12 or form space 32 to space 33.  Six of the spaces, i.e. spaces 15, 22, 29, 36, 43 and 50, have indicia on them ("SI") which sets them off from the majority of the spaces 11-53. These spaces have special instructions associated with them, and tokens landing on them requires the player to follow those instructions, which will be explained below under "Rules."  As shown in FIG. 2, there is provided a timer 70 which preferably may be a simple "minute glass." In use, this timer is turned over at the start of a player's (or team's) turn and when the sand has run out so has the turn.  Each player or team is provided with means for recording a sequence of word sets and, as also shown in FIG. 2, this is preferably a score pad 72 and pencil 74.  The game includes a data base of word sets, each commonly-known phase or word set having a plurality of elements. This collection is preferably included in a set of cards 80 which is also shown in FIG. 2. These cards 80 have a back side (the blind side) which, like the back or ordinary playing cards, has the same design on each. (The two cards shown in FIG. 2 with their face side showing are depicted on a slightly enlarged scale for clarity. All such cards are preferably the same size.) The face side of the cards 80 are all laid out in the same arrangement, that is, with six colored border rectangles 81-86. These are colored red (81), orange (82), yellow (83), green (84), blue (85) and purple (86). Inside each rectangle is a different word set 91-96 (or 91'-96'). These word sets are different on each card in the set.  The word sets of the data base of cards 80 are in different groups or classifications or categories which are color-coded. These categories and their colors are  PEOPLE--Red  PLACES--Orange  TITLES--Blue  THINGS--Yellow  GENERAL--Green  SPORTS/GAMES--Purple  For convenience of the players, the names of these categories with their associated colors as a background are placed on the board 10 (FIG. 1) at 99. These colors are, of course, the same as those used on the central areas of the spaces 11-53. That is, the centers of the spaces 11, 17, 25, 29, 34, 42, 44 and 52 are purple--the SPORTS/GAMES color. And the centers of the spaces 12, 20, 22, 28, 30, 38 and 46 are orange--or the PLACES color, while the center areas of spaces 13, 21, 23, 31, 39, 47 and 50 are yellow--the THINGS color code, and the centers of spaces 14, 16, 24, 31, 36, 40 and 48 are green--the GENERAL category color coded. Completing the list, the center areas of spaces 15, 19, 27, 35, 37, 45 and 53 are red, for PEOPLE, and the remaining spaces 17, 25, 33, 41, 49 and 51, are, for TITLES, color-coded blue in their centers. Note that of the 43 spaces eight are purple and seven are of each of the other colors. This provides a more or less equal change for a token to land on each color and thus a more or less equal chance to select each category during the course of a game.  As mentioned above, the cards 80 have six color coded word sets each, one of each of the six categories. Examples of these are set out below in Table 1 for a number of different cards listed from categories.    TABLE 1__________________________________________________________________________                                  SPORTS/PEOPLE PLACES  THINGS GENERAL TITLES   GAMES__________________________________________________________________________David  Shepherd of          Cover  Sunny Side                         Fantasy  SpeedLetterman  the Hills Farms          Letter Up      Island   RatingEric The  Ripley, Drill Press                 Shipshape                         Great    SecondRed    Tennessee              Pretender                                  BaseJulius Longwood          Juice Glass                 Leave No                         Newsweek MontrealCeasar Gardens        Stone Unturned   CanadiansRobin Hood  Dead Sea          Scaletail                 Coyote  The House of the                                  Down                 Ugly    Seven Gables                                  RiggerTony   Battle Creek,          Universal                 Against the                         Watership                                  Bank ShotBennett  Michigan          Joint  Grain   Down__________________________________________________________________________  Tables 2 through 6 are of more examples:    TABLE 2__________________________________________________________________________                               SPORTS/PEOPLE PLACES THINGS GENERAL                       TITLES  GAMES__________________________________________________________________________Bozo the  Tom Mix         Business                Throw in                       Bridget TheClown  Museum Card   the Towel                       Loves Bernie                               MeadowlandsLou Christie  Joplin,         Magazine                I Owe You                       Please Don't                               Lawn  Missouri         Rack   One    Eat The Daisies                               BowlingBill Cosby  Ulysses,         Bookend                Two of a                       Behind  Time Trial  Kansas        Kind   Closed DoorsHopalong  Ohio River         Fruit Punch                Out of Juke Box                               PotCassidy              Order  Hero    BunkerCharles  Strait of         Checkout                The Honey-                       Come on Switch-Schultz  Gibraltar         Counter                moon's Over                       Danger  Hitter__________________________________________________________________________    TABLE 3__________________________________________________________________________                                   SPORTS/PEOPLE PLACES      THINGS GENERAL                            TITLES GAMES__________________________________________________________________________Yogi Bear &  Pilgrim Glass              False Bot-                     Quiet as a                            I am   RodeoBoo Boo            tom    Mouse  Woman  ClownForest Prison Yard Elephant's                     Rubbing                            High   Sacrifice FlyTucker             Ear    Elbows HopesBlake  Truth or Consequences,              Boxcar Torn Down                            Black  Three RiversEdwards  New Mexico                Beauty StadiumBruce  The Tomb of the              Camera Thin-  Butterflies                                   The FloridaWayne  Unknown Soldier              Tripod Skinned                            Are Free                                   Citrus BowlMa Barker  Jamestown,  Masking                     Hope Springs                            Prom Night                                   Kidney  Rhode Island              Tape   Eternal       Punch__________________________________________________________________________    TABLE 4__________________________________________________________________________                                   SPORTS/PEOPLE  PLACES   THINGS GENERAL                          TITLES   GAMES__________________________________________________________________________Sitting Bull   Chesapeake Bay            Space  Bottom The Sea  Clean and   Maritime Museum            Heater Out    Wolves   JerkChicago Steamboat            Chimney                   The Golden                          Daddy's  Super StockSymphony   Springs, Colorado            Piece  Rule   Little GirlOrchestraHulk Hogan   North Cascades            Fuzzy Dice                   One More                          King Kong                                   Pop-A-Shot   Highway         TimeGirl Friday   Petrified Forest            Fence Post                   Purr Like a                          I Heard It                                   Ringtoss   National Park   Kitten Through                          the GrapevineCharlie Stouffers            Cheesepuff                   The Home                          Creature from                                   SweatbandBrown   Hotels          of the Brave                          the Black Lagoon__________________________________________________________________________    TABLE 5__________________________________________________________________________                                     SPORTS/PEOPLE  PLACES    THINGS  GENERAL TITLES  GAMES__________________________________________________________________________Lauren  Lake Mead National             Lemonade                     Just The                             Over My Los AngelesBacall  Recreation Center             Stand   Facts, Ma'am                             Head    KingsHarlem  Outback   Foam Rubber                     Penny for                             Iron Eagle                                     Put andGlobetrotters             Your Thoughts   TakeBlood,  Trunk Bay Boy Scout                     Top Dog Black Sheep                                     Three of aSweat & Tears     Jamboree        Squadron                                     KindTruman  Baptism River             Vanity  Act Your                             Welcome Bait CasterCapote  State Park             Mirror  Age     Back, KotterDwight D.   Saints Audi-             Diamondback                     Sock It To                             Gentlemen                                     Shut theEisenhower   torium    Rattlesnake                     Me      Prefer Blondes                                     Box__________________________________________________________________________    TABLE 6__________________________________________________________________________                                    SPORTS/PEOPLE  PLACES   THINGS  GENERAL  TITLES GAMES__________________________________________________________________________King Authur   Lincoln Park            Lace    Kissin'  Old Yeller                                    Michigan            Panties Cousins         WolverinesBarry   Virgin Mountains,            Bike Rental                    Wait and Bachelor                                    Tree StandGoldwater   Arizona          See      PartyGroucho Page, North            Flight  Pins and In Harm's                                    Race Walk-Marx    Dakota   Attendant                    Needles  Way    ingThe Pink   Blue Licks            Tie Clasp                    As Easy as a Hot                             Trilogy of                                    BlackjackPanther Battlefield      Knife Thru Butter                             TerrorPerry Como   Fort Lauder-            Port of Call                    Education                             The African                                    Free Throw   dale, Florida    Makes the Man                             Queen__________________________________________________________________________  Also shown in FIG. 2 is a single die 100 which with the cards 80 and board 10 serves as means for randomly selecting one of the included word sets such as those listed above in Tables 1, 2, 3, 4, 5 and 6, as will be explained below.  RULES  A set of rules is also provided with the game, which rules may read as follows:  Rules for Basic Play  Equipment:  COMBONOTIONS contains one game board, one die, one minute timer, six player tokens, six score pads, six pencils and 798 word combo cards.  Object of the Game:  To be the first team (consisting of one or more players (to reach 100 points by creating various word combinations based on the previous word combination. (These combinations will be described later in detail.)  Preparation:  Place the timer die, and combo cards, so that all players have access to them. Divide the players into equal teams. Each team should be issued a token, a scorecard, and a pencil. Place a token for each team on the starting space located in the center of the board. Select a member from each team to roll the die to determine which team rolls first. The highest roller begins. Each team should select a person to write the combinations down. (Note: the scribe may change between team members for each round.)  Play:  Roll the die. Following either arrow, move the token the appropriate number of spaces. When the token lands on a colored square, a card is drawn from the card box. The color of the square's center determines the category of the initial word combo. The six categories are PEOPLE, PLACES, TITLES, THINGS, GENERAL, and SPORTS/GAMES. The scribe announces the initial word combo. He should be given sufficient time to write down the initial word combo in the space provided on the score pad before the opposing team starts the timer. Once the timer is started, the scribe and his teammates being creating new word combinations by using one or more words from the previous combo until the timer has run out. The word combos created must be well known people, places, things, titles, sports/games or phrases. The last word combo created becomes the starting point for the next word combo to be created. Please see example 1a, 1b and 1c. NOTE: The first word combo created does not necessarily have to be in the same category as the initial word combo.  Scoring  At the end of each round, the scribe totals up the points with one point awarded for each valid word combination (including the initial word combo written by the scribe. If time expires while a word combo is being written, the team receives full credit for that combo. Play continues until one team reaches at least 100 points.  Special Instruction (S.I.) Spaces:  When a team's token lands on a S.I. space, roll the die an extra time to determine which of the following special instructions apply.    ______________________________________DIE ROLL SPECIAL INSTRUCTION______________________________________1 or 2   SI 1. The word combo created must end with the    first word from the previous combo.    Please see example 2a.3 or 4   SI 2. The word combo created must begin with the    last word from the previous combo.    Please see example 2b.5        SI 3. No special instructions apply.6        SI 4. Team loses turn.______________________________________  Restrictions:  During a round, NO more than 3 combinations can use the same word. This is necessary to eliminate people from using lists. Please see example 3a.  The created word combination must be "well known." By "well known" we mean the word combination has to be an actual or real person, place, title, phrase or thing known to a majority of the players. For example, the word combo "CHICKEN ICE CREAM" would be invalid, because to the best of our knowledge, no such thing exists. From time to time, due to the nature of this game, questionable word combos will occur. The final determination of whether or not a word combo or phrase is valid must ultimately be made by the players.  When creating new combos, variations, portions, or homonyms of a previous word can be used as the starting point of the next word combo. If the starting combo was DOWN COAT, the word "DOWN" could be used to create a multitude of combinations. Examples of the would be "Down By the Old Mill Stream," "Downy Fabric Softener," "Down's Syndrome," "Churchill Downs," etc.  If the created word combo consists of only one word, that word must be able to be phonetically broken apart into at least two smaller words. For example, "LEADERSHIP" can be broken into LEADER and SHIP. Whereas "MISSION" cannot be phonetically broken into two meaningful words; thus, MISSION is an invalid word combo.  Articles of the English language (a, an, the) cannot be used as the basis for creating new word combos. For example, "The Godfather"]could not be created from "The Man from Uncle".  Challenging:  At the end of each round, opposing team may challenge the other team's word combos on the basis that the combo falls to meet the above restrictions. If the challenged combo is found invalid, the team only receives points for those combos created before the invalid combo. However, if the challenged word combo is valid, the points awarded are doubled for it and each succeeding combo.  Helpful Hints:  If during play a team generates a "dead end combo" (that is, a word combo from which no other combos can be derived), the dead end combo may be erased, making the previous combo the point from which to continue. Also, the scribe should not be overly concerned with spelling or punctuation when writing down combos. He should try to get as many combos written down as possible. Ditto marks (") may be used if desired to mark similar parts of combos.  Rules for Advanced Play  General:  Rules for the advanced game are similar to those of the basic game, with the following changes:  Play:  After a token lands on any space other than a Special Instruction space, draw a card form each card box. The first card drawn contains the starting word combo; its category is determined by the color of the square's center. The color of the square's border determines the category of the ending word combo from the second card. If the beginning and ending combos have a common word, the ending combo must be redrawn. Once the timer is starting, the scribe and his teammates must attempt to derive the ending word combo from the starting word combo. Please see examples 4a, 4b and 4c. The round continues until the ending combo has been derived or time has expired.  Scoring:  Five points are awarded for reaching the concluding combination. Additional Bonus Points are awarded as follows:    ______________________________________# of combos used (not includ-             Bonus Pointsing starting or ending combos)             Awarded     Examples______________________________________1                 92                 83                 7           4a4                 65                 56                 4           4b7                 38                 29                 110 or more        No bonus points                         4c______________________________________  Special Instructions (S.I.) Spaces:  When a team's token lands on a S.I. space, before drawing the combo cards, roll the die again. A die roll of 1-3; the rolling team determines categories of the stating and ending word combos. A die roll of 4-6; the opposing team to the left of the rolling team chooses the categories for the starting and ending combinations.  Challenging:  If an opposing team successfully challenges a word combo, no points are awarded to the rolling team. If an opposing team unsuccessfully challenges a word combo, the points awarded to the rolling team are doubled.  Helpful Hints:  If the players determine that the one minute time limit is too severe a restriction for the advanced game, the time limit may be extended in one minute intervals by flipping the timer over. All players must agree on the length of time permitted during the rounds.    __________________________________________________________________________Examples:__________________________________________________________________________1a  CLOWN FACE       1b         JOHN WAYNE                   1c                     CHICAGO CUBS  Face the Music         Wayne Newton                     Cub Scout  Music Man   Fig Newton  Eagle Scout  Man from Uncle         Fig Tree    Jeep Eagle  Uncle Toms Cabin         Peach Tree  Jeepers Creepers  Cabin Fever Peaches & Cream                     where'd you get  Scarlet Fever         Creamed Corn                     those peepers  Scarlet O'Hara         Corn on the Cob                     Mr. Peepers  etc.        Ty Cobb     Mister Rogers         etc.        Roger Rabbit                     Braer Rabbit                     etc.2a  TABLESPOON       2c         PET ROCK  3a                     KANSAS CITY,  End Table   Rock Hudson Missouri  Rear End    Hudson Bay  Kansas State  Step to the Rear         Bay Leaf    University  Texas Two Step         etc.        Indiana University  etc.                    University of                     Kansas                     Kansas City                     Chiefs-INVALID                     Kansas City                     Royals-INVALID4a  FIELD GOAL       4b         COFFEE MUG                   4c                     BOOK SHELF  Soccer Goal Cup of Coffee                     School Books  Soccer Ball Tea Cup     School Bell  Gum Ball    Tea Bag     Bell Tower  Bubble Gum  Paper Bag   Bell Pepper         Paper Boy   Salt & Pepper         Delivery Boy                     Salt Block         Delivery Room                     Wood Blocks                     Wood Shop__________________________________________________________________________  It should be clear from the above rules as to the manner of play. However, the purposes of illustrations, let us go through a series of moves in a typical match. For simplicity, let's have only two players (or teams). Although the game would go substantially in the same way, it is easier to describe a two-player match and just as illustrative. Let us assume Red is the first player and the second player is Yellow and that they respectively chose red token 61 and yellow token 63. The die 100 is cast by each and Red wins the right to start. Red places token 61 on space 11 and again throws the die 100 and gets a 6. At this point, he can choose to move his token sic spaces in either of two directions (as indicated by the arrows in spaces 12 and 33) and place it on space 17 (blue) or space 38 (orange). Since Red likes the category "Titles" (blue) better than "Places" he chooses to place his token on space 17. Red then (blindly without looking at the faces) draws the card 80 which, let's say, reads:    ______________________________________SONNY LISTONCEDAR POINT, KANSASSPEEDSTICKBLOW HOT AND COLDTHE SHAKIEST GUN IN THE WESTCHIP SHOT______________________________________  Since he has landed on a blue space, his starting word set is "The Shakiest Gun in the West" and Red writes this down on the top sheet of a pad 72 using a pencil 74. At the same time, Yellow turns over the timer 70. Red enters the following sequence before his turn runs out.  1. The Shakiest Gun in the West  2. West Virginia  3. Virginia Ham  4. Ham and Eggs  5. The Chicken or the Egg  6. Chicken Little  7. Little Richard  8. Richard Kinney  9. Kinney Shoes  10. Shoe Fly Shoe  11. Fly the Friendly Skies  12. Officer Friendly  and thus Red gets 12 points.  (Yellow could "challenge" Red on his 8th entry on the grounds that Richard Kinney is not a commonly known word set, but believing patent lawyers are perhaps famous and Yellow being somewhat cowardly, decides not to risk a challenge.) Yellow places his yellow token at start (space 11) and throws a "1" with die 100. His choices are spaces 12 or 33 and he selects the orange space 12. Picking a card blindly, Yellow gets the following card    ______________________________________FRANKLIN D. ROOSEVELTPOWDER RIVER, WYOMINGENGLISH MUFFINROCK-N-ROLLHITCHHIKER'S GUIDE TO THE GALAXYRUNNING BACK______________________________________  And Yellow enters the following on his pad 72 while the sands run out of the timer 70 which was reset by Red:  1. Powder River, Wyoming  2. Yellow River, China  3. John River  4. Peter River  5. Mary River  However, Red being naturally aggressive challenges John River, Peter River and Mary River, citing the restrictions against non-well-known people and repeating the same elements. Yellow gets two points.  Next, Red throws the die 100 again and gets a "5" and thus lands on space 22 which is both orange color-coded and an special rule instruction (SI) space. Following the rules, Red throws the die again and gets a "2". This means that his entered word sets must all end with the first word element of the preceding set. He draws a new card from the pile of cards 80 and its orange-bordered word set reads  SEVEN SISTERS FALLS, MANITOBA  Red enters that and the following on his pad  MAGNIFICENT SEVEN  TRULY MAGNIFICENT  YOURS TRULY  SINCERELY YOURS  MOST SINCERELY  to gain six more pints or a total of 18.  Yellow and Red continue taking turns in this manner until Red wins with 101 points to Yellow's 36.  It should now be apparent that a new and improved word association game has been described and depicted which can be used to develop verbal skills and provided entertainment. While th game has been described in is presently preferred form, many other forms can be employed, for example, it can be adapted to be a computer game or other timer and random selection means can be employed.  While one particular embodiment of the invention has been shown and described, it will be obvious to those skilled in the art that changes and modifications may be made without departing from the invention and, therefore, the aim in the appended claims is to cover all such changes and modifications as fall within the true spirit and scope of the invention.    APPENDIX - CURRENTLY PREFERRED DATA BASE WORD SETS © J. D. Gazdick & J. J. Petrovich 1990, 1991   BATMAN & BEAVER BEETLE BEN BENJAMIN BERT & ROBIN CLEAVER BAILEY CRENSHAW FRANKLIN ERNIE VIDEO VEGA, VAUGHN, VATICAN VALLEY VALENTINE, ARCADE TEXAS NEW MEXICO CITY FORGE NEBRASKA BIRD FEEDER BIRTHSTONE BISCUITS AND GRAVY BLACK RUSSIAN BLOODY MARY BLOW-FISH UP THE UP THE CREEK UNTIL THE COWS UNLUCKY IN LOVE, UNDER THE UNDER MY RIVER WITHOUT A PADDLE COME HOME LUCKY IN CARDS WIRE THUMB BANANA BOAT BAND ON BANG THE DRUM BARNEY BATTLE OF BEAST OF SONG THE RUN SLOWLY MILLER THE BULGE BURDEN TOUCHDOWN TOUCH TOTE TOSSUP TORONTO TORONTO  FOOTBALL BOARD MAPLE LEAFS BLUE JAYS BETSY BETTE BIG AUDIO ROSS DAVIS DYNAMITE UNIVERSIT Y OF UNITED NATIONS UNION CITY, MONTANA BUILDING TENNESSEE BLUE BLUEBERRY  BOAT JEANS  LAUNCH UNDER LOCK UGLY TWO WRONGS DON'T AND KEY DUCKLING MAKE A RIGHT BEAUTY AND BEDTIME FOR BEER BARREL THE BEAST BONZO POLKA TOP WATER TOP TOE PLUG TEN HOLD KUKLA, FRAN AND LADY BIRD LADY LADY LADY LARRY OLLIE JOHNSON DIANA GODIVA MACBETH BIRD LEAVENWORTH LAWRENCE, LAVA BED LAUREL, LASALLE LAKE SHORE STATE PRISON KANSAS NATIONAL MONUMENT DELAWARE STREET DRIVE LAWBOOK LAWN LAZYBOY LEAD LEAF LEAP  JOCKEY RECLINER PIPE LETTUCE YEAR KIDS SAY THE KICK KICK LIKE KEEP YOUR SHOULDER KEEP YOUR KEEP YOUR DARNDEST THINGS THE BUCKET A MULE TO THE WHEEL DISTANCE CHIN UP ONCE IN LOVE ONCE UPON A TIME ONE DAY ONE LESS BELL ONE LIFE ONLY WITH AMY IN THE WEST AT A TIME TO ANSWER TO LIVE THE LONELY MATCH MARIO MAN-IN MAN MAKE-UP MAJOR PLAY BROTHERS MOTION OF WAR GAME INDEPENDENT LARRY LARRY LAS VEGAS FINE HAGMAN SHOWGIRL LAKE POINT LAKE PLACID, LAKE TOWER NEW YORK MICHIGAN LEATHER LEG LEGAL GUARDIAN JACKET GARTER KEEP IT UNDER KEEP IT TO JUST WIN, YOUR HAT YOURSELF BABY OTHER OUR OUT OF WOMAN MISS BROOKS AFRICA MADISON SQUARE LOSING LOS ANGELES GARDEN STREAK LAKERS WILLIAM ZEPPO WILLIAM WILLIE WILSON WILT POWELL MARX TELL SHOEMAKER PICKETT CHAMBERLAIN AMERIA'S AMAZON ALADDIN HOTEL & ALABAMA SPACE & AGRICULTURE ADLER DAIRYLAND RIVER CASINO ROCKET CENTER HALL OF FAMEPLANETARIUM WOOL WORD WORLD'S WRAPPING WRISTBAND YARD MITTEN PROCESSOR FAIR PAPER  STICK A FLYING A FATE WORSE A DIAMOND A CHIP OFF A BONE A BIRD IN THE HAND IS START THAN DEATH IN THE ROUGH THE OLD BLOCK TO PICK WORTH TWO IN  THE BUSH YOU ARE MY YOU BET YOU OUGHTA YOU'RE YOU'VE GOTYOUNG LUCKY STAR YOUR LIFE BE IN PICTURES SO VAIN A FRIEND FRANKENSTEIN ALL TERRAIN ALABAMA AIR AIR AEROBIC ADDRESS VEHICLE CRIMSON TIDE JORDON HOCKEY WORKOUT THE BALL WINDOW KATE KATHARINE WASHER SMITH HEPBURN ADAMS, LOGGING LOGAN, MASSACHUSETTS CAMP NEW MEXICO ZOOM KICKSTAND KING LENS  PENGUIN A BABE IN LADY LABOR OF THE WOODS LUCK LOVE YOUR CHEATING NORTH DALLAS NORTH HEART FORTY TO ALASKA ACROSS MISSILE MINOR THE BOARD COMMAND LEAGUES ARISTOTLE ARMED ARNOLD ART ARTHUR ARTIS ONASSIS GUARD PALMER CARNEY MURRAY GILMORE WEATHER WAX WATSONVILLE, WATERPROOF, WATCHTOWER WASHINGTON, STATION MUSEUM CALIFORNIA LOUISIANA DISTRICT OF COLUMBIA BAKED BAMBOO BAND BANKROLL BAR BARBED BEANS SHOOT SAW  STOOL WIRE WHATEVER WILL WHAT GOES AROUND WET YOUR WE'RE ON A ROAD WELL-DONE WELL, BE WILL BE COMES AROUND WHISTLE TO NOWHERE  FANCY THAT AMERICAN AN AMERICAN WERE- AN OFFICER AND ANATOMY OF A AND GOD AND I WOMAN WOLF IN LONDON A GENTLEMAN MURDER CREATED WOMAN LOVE HER UP THE UNSPORTSMANLIKE UNPLAYABLE UNITED STATES UNDERWATER UNDERHAND MIDDLE CONDUCT LIE GOLF ASSOCIATION PLUG AUDRY AVON B. B. HEPBURN LADY KING WARWICK, WAREHOUSE WAR, RHODE ISLAND  WEST VIRGINIA BARBER BARBIE BASS POLE DOLL ALE WELL I'LL WELCOME WITH WEAR AND BE OPEN ARMS TEAR ANGEL ON ANIMAL ANNIE GET MY SHOULDER FARM YOUR GUN TWO-MINUTE TWO STROKE TWENTY-ONE WARNING PENALTY THE INCREDIBLE THE JACKSON THE KEYSTONE THE LENNON THE LONE THE MARX HULK FIVE COPS SISTERS RANGER BROTHERS BOURBON BOSTON, BOOTH BAY BOOT HILL, BOONE, BONESTEEL, STREET MASSACHUSETTS HARBOR KANSAS IOWA SOUTH DAKOTA TEST TUBE THE MONA THERMAL THOMPSON THREE-RINGTHROW  LISA UNDERWEAR SEEDLESS GRAPES CIRCUS RUG AT DEATH'S AT BREAKNECK AT A FEVER ASHES TO ASHES, AS SLY AS AS RED DOOR SPEED PITCH DUST TO DUST A FOX AS BLOOD TIGHTROPE TO BUILD A TO CATCH TO KILL TOM TOO HOT  FIRE A THIEF A MOCKINGBIRD AND JERRY TO HANDLE BOSTON BOLT BODY BODY BOBSLED BOBBING FOR CELTICS ACTION CHECK BUILDING  APPLES THE NEW YORK THE OSMOND THE PET SHOP PHILHARMONIC BROTHERS BOYS BOMB BOGUS BASIN BOARDWALK SHELTER SKI AREA THUMBTACK TICKER TAPE TICKET  PARADE STUB AS MAS AS AS GOOD AS EASY A HORNET AS GOLD AS PIE TOO LATE TO TOUCH OF TRAIN KEPT TURN BACK NOW EVIL A ROLLIN' BLUE BLIND MAN'S BLEACHER CATFISH BLUFF BUM FAY FAYE FELIX THE FIBBER FIDEL FIREMAN WRAY DUNAWAY CAT McGEE CASTRO RED WING, RED LION, RED LIGHT RECOVERY RAPID RIVER RANGER MINNESOTA PENNSYLVANIA DISTRICT ROOM LOGGING CAMP PEAK DUNEBUGGY DUST DUTCH EARTHENWARE EASTER EELSKIN  BUSTER OVEN  CARD LEATHER SEA SCREW SCARED TO SCARED SCARED OF SAY YOUR LEGS LOOSE DEATH STIFF YOUR OWN SHADOW PRAYERS HARPER VALLEY HAVE GUN HAWAII HEART OF HEARTBREAK HEAVEN CAN P.T.A. WILL TRAVEL FIVE-O GOLD RIDGE WAIT SCAVENGER SAND SAN FRANCISCO SAN FRANCISCO SAN DIEGO SAN DIEGO HUNT TRAP GIANTS 49'RS PADRES CHARGERS FIRST FLIP FLORENCE MATE WILSON NIGHTINGALE RANDOLPH, RAIN RAIL VERMONT FOREST YARD EGG FOO ELBOWROOM ELECTRIC YOUNG  RAZOR SAY IT WITH SAVED BY SAVE YOUR FLOWERS THE BELL BACON HELLO, HERE COME THE HERE COMES DOLLY! BRIDES THE SUN SAN ANTONIO SAM SAILBOAT SPURS SNEAD CHRISTIE CHRISTOPHER CHRISTOPHER CHUCK CLARK CLARK BRINKLEY COLUMBUS ROBIN BERRY GABLE KENT ST. PETERSBURG, ST. PATRICK'S ST. LOUIS, ST. AUGUSTINE, ST. LAWRENCE SQUAW FLORIDA CATHEDRAL MISSOURI FLORIDA SEAWAY MOUNTAIN CIGARETTE CITIZEN'S CITRUS CITY CLAMBAKE CLASS FILTER BAND RATIO FRUIT CHICKEN  RING THE FAT OF THE ENEMY IS THE END OF THE END THE EARLY BIRD THE CREAM RISES THE LAND AT HAND THE RAINBOW JUSTIFIES  THE MEANS GETS THE WORM TO THE TOP DEAD DEATH ON THE DEEP IN THE HEART DENNIS THE DESIGNING DESTRY RINGERS NILE OF TEXAS MENACE WOMEN RIDES AGAIN STUD STROKE-AND- STROKE STRIKEOUT STRIKE STRAIGHT-ARM POKER DISTANCEPLAY  THREE CLAUDE CLEVON CLINT PEPPER LITTLE EASTWOOD SPRINGFIEL D, SPORTMART SPENCER, ILLINOIS  NEW YORK CLAW CLEAN AIR CLING HAMMER ACT PEACHES THE COAST THE CHECK IS THE CAUTIOUS IS CLEAR IN THE MAIL SELDOM ERR DIAL M DIAMONDS ARE DIFF'RENT FOR MURDER FOREVER STROKES STRAIGHT STOLEN STOCK FLUSH BASE CAR THOMAS THOMAS THREE DOG TIM TIMOTHY TIP EDISON JEFFERSON NIGHT CONWAY LEARY O'NEIL BERKELEY OPEN BERING BELT, BEAVER BEAR TOOTH BEACON FALLS, PIT MINE STRAIT MONTANA ISLAND SCENIC HIGHWAY CONNECTICUT TRACK TRAFFIC TRAILER TREASURE TURN TV TRAY LIGHTING COURT HITCH CHEST SIGNAL AN ANGEL ON ALL'S WELL ALL'S FAIR ALL MEN ARE ALL IN A ALL FIRED MY SHOULDER THAT ENDS WELL IN LOVE AND WAR CREATED EQUAL DAYS WORK UP VALLEY OF VISION VIVA VOYAGE TO THE WAGONMASTER WAIT UNTIL THE DOLLS QUEST LAS VEGAS BOTTOM OF THE SEA DARK BEAR BATTLESHIP BATTING BATTER BAT BASKETBALL ARCHERY  AVERAGE UP AROUND COURT TOM TOM TOM CRUISE JONES KITE BEACHES OF BEACH BAY NORMANDY FRONT BRIDGE U.S. ULTRAVIOLET UNDERTOW STEEL RAY ALIVE AND AIRSICK AIR-BORNE KICKING WALK LIKE WALK ON WAR AND AN EGYPTIAN THE WILD SIDE PEACE BASES BASE ON BARREL OF LOADED BALLS MONKEYS ORSON OSCAR THE P. T. PARKER PAT PAT WELLES GROUCH BARNUM BROTHERS BOONE GARRETT GANGES GAMBLE, GAIL, FUN FROSTBITE FRIENDSHIP, RIVER OHIO TEXAS HOUSE FALLS OHIO PIXIE PIZZA HUT PLACE PLANE PLASTIC PLAYBOY STICKS RESTAURANT MAT CRASH SHOP FINDERS KEEPERS, FIND FIGHT TO FIGHT FIRE FEET OF FEEL IT IN LOSERS WEEPERS FAULT THE FINISH WITH FIRE CLAY MY BONES SWEET BIRD SWEET LITTLE SWINGING ON SWISS FAMILYTAKE ME OUT TALK OF YOUTH SIXTEEN A STAR ROBINSON TO THE BALLGAME RADIO GO BIG GLASS GIVE AND GIN GEORGIA TECH GENERAL RED JAW GO RUMMY YELLOW JACKETS MANAGER PATTI PATTY PATTY PAGE DUKE HEARST FRIDAY HARBOR, FRESNO, FRENCH WASHINGTON CALIFORNIA RIVIERA PLUMBER'S POCKETWATCH POGO HELPER  STICK FEED A COLD FATHER'S FAT AND STARVE A FEVER DAY CAT TAXI TEA FOR TEENAGER DRIVER TWO IN LOVE GARDEN GAME GAME PATH BIRD BALL TED TELEPHONE TENNESSEE TERRY TEX THE ARTFUL TURNER REPAIR MAN WILLIAMS BRADSHAW RITTER DODGER BUSCH GARDENS, BUS BURNS, BUNKER BULLHEAD CITY, BUFFALO BILL FLORIDA STATION OREGON HILL ARIZONA STATE HISTORIC PARK SUNROOF SURFBOARD SWAMPLAND SWEAT SWEDISH SWING    PANTS MEATBALL SET BEAT BEAR BE ALL THAT BE A BATTLE BATTON DOWN IT DOWN THAT YOU CAN BE MAN ROYAL THE HATCHES THE WILD THE WIND IN THE WITCHES THE WOMAN THE WORLD THE WRONG BUNCH THE WILLOWS OF EASTWICK IN RED ACCORDING TO GARP MAN CALLING THE CALIFORNIA CALGARY BUFFERFLY BUSCH MEMORIAL BULLS-EYE SHOT ANGELS FLAMES STROKE STADIUM THE BEACH THE BIG THE BIRDMAN BOYS BAD WOLF OF ALCATRAZ BUCKLAND, BUCKINGHAM BRYCE CANYON ALASKA FOUNTAIN NATIONAL PARK SWISS SWITCH SWORDFISH CHALET HOOK BATS IN THE BARKING UP THE BAPTISM OF BELFRY WRONG TREE FIRE THERE SHE THERE! I'VE THERE'S NO BUSINESS GOES SAID IT AGAIN LIKE SHOW BUSINESS BULL BUFFALO BUFFALO FIGHTER SABRES BILLS ALAN ALBERT ALEX ALEXANDER ALEXANDER ALFRED E. KING EINSTEIN HALEY GRAHAM BELL THE GREAT NEWMAN WINTER HAVEN, WINDSOR LOCKS, WINDOW ROCK, WIND CAVE WINCHESTER, WILSON, FLORIDA CONNECTICUT ARIZONA NATIONAL PARK ILLINOIS NORTH CAROLINA ALARM ALLOVER ALUMINUM AMPLIFIER AND ANKLE ANNIVERSARY CLOCK TAN CAN TUNER HOLSTER CARD YOU CAN'T TEACH YOU CAN'T TAKE YOU CAN'T KEEP YOU BET YOUR YOU ARE X MARKS AN OLD DOG NEW IT WITH YOU A GOOD MAN DOWN BOTTOM DOLLAR WHAT YOU EAT THE SPOT TRICKS A MAN A PLACE IN A POOR LITTLE A SHOT IN A STREETCAR A STRING CALLED HORSE THE SUN RICH GIRL THE DARK NAMED DESIRE OF PEARLS WIDE WHITEWASH WHITE WATER WHEELBARROW WET WESTERN RECEIVER RAFTING RACE SUIT SADDLE ALFRED ALI ALLIE HITCHCOCK McGRAW SHERMAN WILL WILDLIFE WILDCAT MOUNTAIN COUNTY PRESERVE TRAMWAY ANSWERING APPLEJACKS ARABIC MACHINE  NUMERALS WRAPPED AROUND WORTH HIS WEIGHT WOLF IN YOUR LITTLE IN GOLD SHEEP'S CLOTHING FINGER A TALE OF A TREE GROWS A VIEW TWO CITIES IN BROOKLYN TO A KILL WELTERWEIGHT WEIGHT WEAK  LIFTING SIDE VALERIE VANESSA VANNA VICTORIA VINCE VINCENT HARPER WILLIAMS WHITE PRINCIPAL LOMBARDI PRICE BADLANDS BADGER BAD AXE, BACKSTAGE B & O RAILROAD AUTO BODY NATIONAL PARK MINE MICHIGAN  MUSEUM SHOP WAFFLE WALKIE- WALLFLOWER WATER WAX WEATHER IRON TALKIE  FOUNTAIN LIPS VANE ABOUT-FACE A WINNER NEVER A WELL OILED A WATCHED POT A TOUGH NUT A ROLLING STONE  QUITS AND A QUIT- MACHINE NEVER BOILS TO CRACK GATHERS NO MOSS TER NEVER WINS WHERE WHERE'D YOU GET WHISTLE WHILE WHITE WHITE HUNTER, WHO'S AFRAID OF EAGELS DARE THOSE EYES YOU WORK CHRISTMAS BLACK HARVEST VIRGINIA WOOLF ATLANTIC COAST ATLANTA ATHLETE'S AT THE ASSAULT ON ARTIFICIAL CONFERENCE FALCONS FOOT WIRE EVEREST TURF VIRGIL W. C. WALT WARD FIELDS DISNEY ATLANTIC CITY, ATLANTA, ASH FORK, NEW JERSEY GEORGIA ARIZONA WEDDING WELCOME WHEELBARROW BAND MAT A PIG A PIECE OF A PIECE OF CAKE IN A POKE THE PIE WHO'S WHOLE LOT OF WHY DO FOOLS NEXT SHAKIN' GOING ON FALL IN LOVE ARROWHEAD ARROW SHAFT AROUND STADIUM THE HORN SHIP SHIRLEY SHIRLEY SHIRLEY SHOE SID CAPTAIN BOOTH MACLAINE TEMPLE SALESPERSONCAESAR CLEMSON, CLEARWATER, CLAYTON, CINCINNATI, TOKYO, CHOCOLATE SOUTH CAROLINA FLORIDA NEW YORK OHIO JAPAN MOUNTAINS SOCIAL SECURITY SOFA SOFT SOLAR SONIC SONY NUMBER SLEEPER SOAP SYSTEM BOOM WALKMAN BROKEN BRING DOWN BREAK THE BREADBASKET BREAD AND BRAINWASH MAN THE HOUSE ICE  WATER THE POSTMAN THE PRIDE OF THE PUBLIC THE RAT THE REAL THE RETURN ALWAYS RINGS TWICE THE YANKEES ENEMY PATROL McCOYS OF THE KING COMMUNITY COMBONOTIONS COIN ON CO-ED CLUBHOUSE CLIFF CHEST  THE PLATE GYM  DIVER SIGMUND SIMON & SIR FRANCIS FREUD GARFUNKEL DRAKE CHINA LAKE, CHIMNEY ROCK CHIEF BLACK CALIFORNIA NATIONAL HISTORIC OTTER TRAIL SITE SOUND SOUP OF SOUR EFFECTS THE DAY CHERRY BRAINSTORM BOWED BUT BOUND AND  NOT BROKEN GAGGED THE THE ROAD TO THE ROAD RIFLEMAN SINGAPORE WARRIOR CLEVELAND CLEVELAND CLEMSON INDIANS BROWNS TIGERS GALE GARFIELD GARRETT GARRY GARY GEENA SAYERS GOOSE MORRIS SHANDLING COOPER DAVIS POMPEYS POLE POISON, POINT STREET PLYMOUTH PLUMMER, PILLAR BARN MONTANA PARK ROCK IDAHO EVERGREEN EXERCISE EXHAUST EXPENSE EXPRESSWAY EYEBROW BIKE FAN ACCOUNT REMEMBER THE REEL HIM READING, WRITING READ HIM RAN LIKE RAKED OVER ALAMO IN AND ARITHMETIC THE RIOT ACT A THIEF THE COALS HORTON HEARS HOTEL HOUND HOUR OF HOUSE OF THE HOW THE GRINCH A WHO CALIFORNIA DOG THE WOLF RISING SUN STOLE CHRISTMAS ROUNDHOUSE ROUND THE ROUND ROULETTE ROUGHING ROSIN  CLOCK ROBIN WHEEL THE KICKER BAG GENE GENERAL GENERAL DOUGLAS HACKMAN CUSTER McARTHUR PLAZA PIPESTONE PIPERS HOTEL NATIONAL MONUMENT OPERA HOUSE FACE FAIRY FAITH CARD TALE HEALER RAISE THE RAINING CATS RACE THE ROOF AND DOGS WIND HOW THE WEST HUSH . . . HUSH, I AIN'T GOT WAS WON SWEET CHARLOTTE NOBODY ROPE ROOKIE OF ROLLER JUMPING THE YEAR DERBY MR. MRS. MUDDY MUHAMMAD MURPHY NATALIE SPOCK O'LEARY WATERS ALI BROWN WOOD GRAHAM, GRACELAND GOOSE ROCKS GOODWATER,GOO DRICH, GOLDEN NUGGET TEXAS  BEACH ALABAMA NORTH DAKOTA HOTEL & CASINO PEAR PEER PEG PENCIL PEPPERMINT PERCUSSION TREE PRESSURE LEG LEAD ICE-CREAM SECTION FOUGHT TOOTH FORGIVE AND FOREVER FOREVER FOOTLOOSE AND FOOD FOR AND NAIL FORGET YOUNG AND A DAY FANCY FREE THOUGHT ST. ELSEWHERE  ST. VALENTINE'S STAGECOACH STAIRWAY STAKEOUT STAND AND  DAY MASSACRE TO HEAVEN  DELIVER HALL OF HALF HALF MILE HALF GUTTERBALL GUN FAME TIME TRACK HITCH  DOG NATHAN NEIL NELSON HALE ARMSTRONG EDDY GOLDEN GATE GOLD BEACH, GLEN ELLIS BRIDGE OREGON FALLS PERFUME PET PHOTO BOTTLE ROCK ALBUM FLY-BY-NIGHT FLY OFF THE FLEW THE  HANDLE COOP STAND BY STAR 80 STAR ME  TREK GUESS THE GROUND UNDER GROUND NUMBER REPAIR RULES SPIRO SPORTS ST. FRANCIS ST. STAN LAUREL & STEPHEN AGNEW WRITER OF ASSISI NICHOLAS OLIVER HARDY KING CARTER CAVE CARSON PIRIE CARSON CITY, CARROT RIVER, CARROLL, CARPENTER'S STATE PARK SCOTT & CO. NEVADA SASKATCHEWAN IOWA HALL SPOTLIGHT SPRING SQUIRT ST. CHRISTOPHERST. PATRICK'S STAMP BREAK GUN MEDAL DAY COLLECTION BLACK BLACK AS BITE THE HAND BITE THE BIRDS OF A FEATHER BIRD'S-EYE SHEEP COAL THAT FEEDS BULLET FLOCK TOGETHER VIEW THE SPY THE STAR-SPANGLED THE STREETS OF THE SUN THE SUNSHINE THE TELLTALE WHO LOVED ME BANNER SAN FRANCISCO ALSO RISES BOYS HEART CHICAGO CHEST CHEST CHESSBOARD CHEERLEADERS CHECKMATE BEARS WADERS PROTECTOR STEVE STEVE STEVIE ALLEN LAWRENCE WONDER CARNAGIE CARLSBAD CAVERNS CARLIN, HALL NATIONAL PARK NEVADA STAR OF STATE STEAK DAVID CAPITOL KNIFE BIGGER THAN BIGGER FISH BIG AS A BREAD BOX TO FRY ALL OUTDOORS THE TEN THE TEXAS THE THIEF COMMANDMENTS CHAINSAW MASSACRE OF BAGDAD CHECKERED CHECKERBOARD CHARLOTTE FLAG  HORNETS RUDOLPH RUTH RYAN RYNE SALLY SAM VALENTINO BUZZI O'NEAL SANDBERG RIDE SHEPARD COULY DAM NATIONAL COTTONDALE, COSTER CORNING CORN COPLY RECREATION AREA ALABAMA STATE PARK GLASS CENTER PALACE SQUARE SHOESTRING SHOOTING SHOPPING SHORE SHORT SHOULDER POTATOES STAR CENTER PATROL STORY BLADE CATNIP CATCHING TWENTY CATCH A CAST-IRON CARSICK CAN'T HAVE YOUR CAKE  WINKS RISING STAR STOMACH  AND EAT IT TOO THE MAN IN THE MANCHURIAN THE MICKEY THE MIRACLE THE MISSILES OF THE MONEY THE IRON MASK CANDIDATE MOUSE CLUB WORKER OCTOBER PIT DARTBOARD DALLAS DAILY D. P. FIT CY YOUNG CURLING  COWBOYS DOUBLE FOR LIFE AWARD STONE SAMMY SAMUEL SAMUEL DAVIS JR. ADAMS CLEMENTS COOPER LOG CABIN COOK CONWAY, MUSEUM COUNTY ARKANSAS SHOWDOWN SHOWER SHUT-EYE  MASSAGE CAN'T BEAT CALL OVER CABIN THE FEELING THE COALS FEVER THE MUMMY'S THE MUSIC THE NAKED HAND MAN GUN CUE CROWN CROSSWORD BALL AND ANCHOR PUZZLE EDDIE EDGAR ALLEN EDGER EDWARD G. EDWARD R. ELIZABETH VAN HALEN POE BERGEN ROBINSON MURROW TAYLOR ROSEBOWL ROOSEVELT, ROOFTOP RODEO ROCKY ROCKY FORD, RACEWAY UTAH  DRIVE MOUNTAINS COLORADO DISPOSABLE  DITCH-WITCH DOG DOLLAR DOORJAMB DOUBLE DIAPERS  COLLAR BILL  DATE SKATING ON SIX FEET SING LIKE SILVER-TOUNGED SILLY SILENCE IS THIN ICE UNDER A BIRD DEVIL GOOSE GOLDEN GIVE A GO AWAYGOATS HEAD GOD BLESS GOING IN GOLDFINGER LITTLE BIT LITTLE GIRL SOUP AMERICA STYLE SHOOTING SHOOT THE SHOCK SHINGUARDS SHADOW SHAD GLOVE TUBE WAVE  BOWLING RAP ELKE ELLIOT ELMER SOMMER GOULD FUDD ROCKEFELLER ROCK OF ROCK BRIDGE CENTER GIBRALTAR CANYON, ALABAMA DOWNPOUR DRAGONFLY DRAINPIPE SICK AS A SICK AND SHUT YOUR DOG TIRED MOUTH GOLDILOCKS AND THE GOOD MORNING, GOODBYE THREE BEARS VIETNAM MR. CHIPS SEVENTH INNING SEVEN TEN SETTING THE STRETCH SPLIT HOOK ROBIN ROCK ROCKET J. ROCKY ROD ROD WILLIAMS HUDSON SQUIRREL BALBOA SERLING STEWART DAYS DAVIS, DAVID CITY, DANIEL BOON DANCE DALTON GANG INN CALIFORNIA NEBRASKA NATIONAL FOREST, HALL MUSEUM KENTUCKY SCARLET SCHOOL SCOTCH SCOUTMASTER SCREAMIN' SCREENED IN RUNNER BUS POST-IT  EAGLE PORCH CORNBALL COOL AS A COME TO COME OUT COME HELL COLD  CUCUMBER ORDER FIGHTING AND HIGH WATER FEET THE INVISIBLE THE IRON THE JAGGED THE JAZZ THE JOY THE JUNGLE MAN HORSE EDGE SINGER OF COOKING BOOK DOUBLE-TEAM DOUBLE-HEADER DOUBLE DOUBLE DON'T BREAK DOGLEG   PLAY FAULT THE ICE RODDY ROGER ROGER McDOWELL MOORE WHITTAKER DADE CYPRESS GARDENS, CUT KNIFE, COUNTY FLORIDA SASKATCHEWAN SCREW SEA SEALING JACK GULL WAX COLD AS CLEAR CLEAR AS ICE SAILING A BELL THE KARATE THE KILLING THE KING KID FIELDS OF KINGS DOG SLED DODGEBALL DIRT RACING TRACK JANET JANIS JAYNE JAZZ JEAN PAUL JEAN JACKSON JOPLIN KENNEDY MUSICIAN GETTY STAPLETON MURPHY, MUNICIPAL PIER MOVIE MOUNT RUSHMORE MOUNT OLIVE, MOTT, NORTH CAROLINA NATIONAL PARK THEATER NATIONAL MEMORIAL MISSISSIPPI NORTH DAKOTA GRAVY GREASEPAINT GREEN GREETING GROUND GUARDIAN BOAT  ONIONS CARD CREW ANGEL NO GUTS, NO NIP IT IN NINE NEW YEAR'S NEVER NO GLORY DICE THE BUD LIVES DAY SURRENDER ISTEN LIKE LISTEN TO LITTLE HOUSE LITTLE LITTLE LIVE AND THIEVES THE MUSIC ON THE PRAIRIE RASCALS WOMEN LET DIE PENNY-PITCHING PENNANT PENN STATE PENALTY PENALTY PEEWEE  RACE NITTANY LIONS FLAG BOX GOLF JEANE JEFF JEFFERSON DIXON SMITH AIRPLANE MOSES LAKE, MORTON, MORGAN HILL, WASHINGTON MISSISSIPPI CALIFORNIA GUARDRAIL GUIDED GULF  MISSILE STREAM NEVER SAY NECESSITY IS THE MY FINE-FEATHERED DIE MOTHER OF FRIENDS INVENTION LIVING IN LOGAN'S LONELY ARE THE PAST RUN THE BRAVE PEANUT PASSING PASSED VENDOR ZONE BALL REGGIE REX RICH RICHARD RICHARD J. RICHARD JACKSON REED LITTLE BURTON DALEY PETTY DOWNEY, DOUGLAS, DOUBLEDAY  DOOR DOG DODGE CITY, IDAHO ALASKA FIELD COUNTY KENNEL KANSAS RING RIP ROAST ROCK ROCKING ROD-N-REEL FINGER CURRENT BEEF SLIDE CHAIR DON'T BE A DOG-TIRED DO YOU FEEL DO OR DO AS I SAY, DISCRETION IS THE LITTER BUG LUCKY, PUNK? DIE NOT AS I DO BETTER PART OF VALOR THE FALL THE FAR THE FELLOWSHIP OF THE FLIGHT OF THE FLYING THE FRONT GUY SIDE OF THE RING THE PHOENIX NUN PAGE ENGLISH END ELECTRIC FISHING EIGHTH EIGHT EDMONTON SADDLE ZONE MOTOR POLE BALL OILERS RICHARD RICHIE RICKIE LEE THOMAS RICH JONES DOCTOR'S DIXON, DISNEYWORLD OFFICE ILLINOIS ROLL ROLL OF ROLLER CALL FILM COASTER DIRTY DIFFERENT DEAD ON DANCING DRUMMERS ARRIVAL THE GHOST AND THE THE GODS MUST MR. CHICKEN GODFATHER BE CRAZY EASTER EGG EARNED RUN EAGLE HUNT AVERAGE CLAW ##STR1##     Claims ( 5 )   We claim:    1. A method of playing a word association game, consisting of a game board defining a FIG. 8 shaped track having a plurality of color coded spaces with a labeled starting space at the enter of the board, game pieces, a die, a resetable timer, score pads, writing instruments, a deck of cards with color coded word sets having words, comprising the steps of: selecting a game piece; rolling a die;  moving said game piece from the center starting space along the track a number of spaces corresponding to the die roll;  selecting a card from said deck of cards;  selecting a color coded word set corresponding to the color coded space onto which the game piece was moved;  recording said word set onto a score pad;  starting said timer;  creating and recording a sequence of word sets using one or more word elements from a preceding word set until the timer runs out; and  scoring the word sets created and recorded during a turn.       2. The method of playing a word association game of claim 1, further comprising the step of recording the credit toward winning for the number of created and recorded word sets in the sequence.      3. The method of playing a word association game of claim 2, wherein said color coded spaces define different categories by having all of the word sets of each category associated with a different color coding.      4. The method of playing a word association game of claim 3, wherein a created and recorded word set is scored only if the word set is commonly known to a majority of players.      5. The method of playing a word association game of claim 4, wherein a created and recorded word set is not scored if the word set is repetitive of any preceding word set.        US07706980  1991-05-29  1991-05-29  Word association game  Expired - Fee Related   US5213333A ( en )   Priority Applications (1)     Application Number  Priority Date  Filing Date  Title       US07706980   US5213333A ( en )  1991-05-29  1991-05-29  Word association game     Applications Claiming Priority (1)     Application Number  Priority Date  Filing Date  Title       US07706980   US5213333A ( en )  1991-05-29  1991-05-29  Word association game     Publications (1)     Publication Number  Publication Date       US5213333A  true  US5213333A
                       ( en )   1993-05-25      Family  ID=24839873  Family Applications (1)     Application Number  Title  Priority Date  Filing Date       US07706980  Expired - Fee Related   US5213333A ( en )   1991-05-29  1991-05-29  Word association game     Country Status (1)     Country  Link       US ( 1 )     US5213333A ( en )      Cited By (14)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US5312112A ( en )  *   1992-11-24  1994-05-17  Cohen Gene D  Word forming board game including elements of conflict     US6431545B1 ( en )   2000-10-24  2002-08-13  Scott A. Kuhne  Board game with novel format     US20040002043A1 ( en )  *   2002-06-10  2004-01-01  Peter Dowrick  Diagnostically and audibly responsive computer learning memory game and system provided therefor     US20040124583A1 ( en )  *   2002-12-26  2004-07-01  Landis Mark T.  Board game method and device     US20070032352A1 ( en )  *   2002-08-20  2007-02-08  Sunbeck Deborah I  "Figure-eight" track, apparatus, method, and game for sensory-motor exercise     US20070250313A1 ( en )  *   2006-04-25  2007-10-25  Jiun-Fu Chen  Systems and methods for analyzing video content     US20070284819A1 ( en )  *   2006-06-09  2007-12-13  Ellen Louise Ledger  Card game     US7425946B1 ( en )  *   2003-08-15  2008-09-16  Britton Rick A  Remote camouflage keypad for alarm control panel     US20100092928A1 ( en )  *   2008-09-19  2010-04-15  Mira Stulberg-Halpert  System and method for teaching     US20110062666A1 ( en )  *   2009-09-14  2011-03-17  Castineiras Companies, Llc  Communications game and method of playing the same     US8313102B1 ( en )  *   2009-06-16  2012-11-20  Yolanda Gail Thornton  Current affair, political game apparatus and method engaging role play     US20150258422A1 ( en )  *   2014-03-11  2015-09-17  Rob Volpe  Educational and information-based board game     USD778368S1 ( en )  *   2016-02-23  2017-02-07  Craig Franklin Edevold  Cribbage board     US20170136343A1 ( en )  *   2015-11-16  2017-05-18  Arin Betsworth  Matching Card Game and Method of Play     Citations (14)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US2602667A ( en )  *   1948-06-15  1952-07-08  Curtis A Poarch  Game board with chance means and cards     US3389480A ( en )  *   1965-10-22  1968-06-25  L. Virginia Holland  Game and teaching method     US3606336A ( en )  *   1968-05-29  1971-09-20  Jack R Krause  Word association game     US4053154A ( en )  *   1976-05-24  1977-10-11  Niemann Henry P  Homicide board game     US4089527A ( en )  *   1976-03-11  1978-05-16  Roth Barry B  Board game apparatus     US4340231A ( en )  *   1980-02-19  1982-07-20  Cammarata Joseph G  Random selection word game     US4666161A ( en )  *   1985-01-10  1987-05-19  Elesie Louis D  Word definition game including a race track board     US4671516A ( en )  *   1985-10-31  1987-06-09  501 Maxigames Corporation  Sentence game     US4854594A ( en )  *   1988-08-09  1989-08-08  Eaton Ronald E  Method of playing a board game     US4884815A ( en )  *   1988-04-27  1989-12-05  Glenn Willie L  Educational automotive game method of play     US4911452A ( en )  *   1988-04-13  1990-03-27  Marchese Jr Alfred J  Method of playing a category game     US4941668A ( en )  *   1989-08-04  1990-07-17  Mobrem Matthew M  Word puzzle card game     US4955614A ( en )  *   1989-01-26  1990-09-11  Pualette Buda  Word forming by elimination game     US5004244A ( en )  *   1990-08-20  1991-04-02  Johnson Gary K  Memory game      Patent Citations (14)   * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US2602667A ( en )  *   1948-06-15  1952-07-08  Curtis A Poarch  Game board with chance means and cards     US3389480A ( en )  *   1965-10-22  1968-06-25  L. Virginia Holland  Game and teaching method     US3606336A ( en )  *   1968-05-29  1971-09-20  Jack R Krause  Word association game     US4089527A ( en )  *   1976-03-11  1978-05-16  Roth Barry B  Board game apparatus     US4053154A ( en )  *   1976-05-24  1977-10-11  Niemann Henry P  Homicide board game     US4340231A ( en )  *   1980-02-19  1982-07-20  Cammarata Joseph G  Random selection word game     US4666161A ( en )  *   1985-01-10  1987-05-19  Elesie Louis D  Word definition game including a race track board     US4671516A ( en )  *   1985-10-31  1987-06-09  501 Maxigames Corporation  Sentence game     US4911452A ( en )  *   1988-04-13  1990-03-27  Marchese Jr Alfred J  Method of playing a category game     US4884815A ( en )  *   1988-04-27  1989-12-05  Glenn Willie L  Educational automotive game method of play     US4854594A ( en )  *   1988-08-09  1989-08-08  Eaton Ronald E  Method of playing a board game     US4955614A ( en )  *   1989-01-26  1990-09-11  Pualette Buda  Word forming by elimination game     US4941668A ( en )  *   1989-08-04  1990-07-17  Mobrem Matthew M  Word puzzle card game     US5004244A ( en )  *   1990-08-20  1991-04-02  Johnson Gary K  Memory game      Non-Patent Citations (2)   * Cited by examiner, † Cited by third party    Title       Trival Pursuit, " Master Game Rules of Play ", Selchow & Righter Co., 1981.     Trival Pursuit, Master Game Rules of Play , Selchow & Righter Co., 1981.  *      Cited By (16)  * Cited by examiner, † Cited by third party    Publication number  Priority date  Publication date  Assignee  Title        US5312112A ( en )  *   1992-11-24  1994-05-17  Cohen Gene D  Word forming board game including elements of conflict     US6431545B1 ( en )   2000-10-24  2002-08-13  Scott A. Kuhne  Board game with novel format     US20040002043A1 ( en )  *   2002-06-10  2004-01-01  Peter Dowrick  Diagnostically and audibly responsive computer learning memory game and system provided therefor     US7001183B2 ( en )   2002-06-10  2006-02-21  Peter Dowrick  Diagnostically and audibly responsive computer learning memory game and system provided therefor     US20070032352A1 ( en )  *   2002-08-20  2007-02-08  Sunbeck Deborah I  "Figure-eight" track, apparatus, method, and game for sensory-motor exercise     US7708676B2 ( en )  *   2002-08-20  2010-05-04  Sunbeck Deborah T  “Figure-eight” track, apparatus, method, and game for sensory-motor exercise     US20040124583A1 ( en )  *   2002-12-26  2004-07-01  Landis Mark T.  Board game method and device     US7425946B1 ( en )  *   2003-08-15  2008-09-16  Britton Rick A  Remote camouflage keypad for alarm control panel     US20070250313A1 ( en )  *   2006-04-25  2007-10-25  Jiun-Fu Chen  Systems and methods for analyzing video content     US20070284819A1 ( en )  *   2006-06-09  2007-12-13  Ellen Louise Ledger  Card game     US20100092928A1 ( en )  *   2008-09-19  2010-04-15  Mira Stulberg-Halpert  System and method for teaching     US8313102B1 ( en )  *   2009-06-16  2012-11-20  Yolanda Gail Thornton  Current affair, political game apparatus and method engaging role play     US20110062666A1 ( en )  *   2009-09-14  2011-03-17  Castineiras Companies, Llc  Communications game and method of playing the same     US20150258422A1 ( en )  *   2014-03-11  2015-09-17  Rob Volpe  Educational and information-based board game     US20170136343A1 ( en )  *   2015-11-16  2017-05-18  Arin Betsworth  Matching Card Game and Method of Play     USD778368S1 ( en )  *   2016-02-23  2017-02-07  Craig Franklin Edevold  Cribbage board     Similar Documents     Publication  Publication Date  Title         Seymour et al.    1960   Baseball: the early years      Gorn et al.    2004   A brief history of American sports      Beezley    2018   Judas at the Jockey Club and other episodes of Porfirian Mexico      Baker    1988   Sports in the western world      Grundy et al.    2016   American sports      James    2013   Beyond a boundary      Spalding    1911   America's National Game: Historic Facts Concerning the Beginning, Evolution, Development and Popularity of Base Ball, with Personal Reminiscences of Its Vicissitudes, Its Victories and Its Votaries      Malcolmson    1979   Popular recreations in English society 1700-1850      Zirin    2008   A people's history of sports in the United States: 250 years of politics, protest, people, and play      Smith    1990   Sports and freedom: The rise of big-time college athletics      Holt    1990   Sport and the British: a modern history      Somers    1972   The Rise of Sports in New Orleans: 1850-1900      Gallico    2015   Farewell to Sport      Oxendine    1995   American Indian sports heritage      Montague    1922   Disenchantment      Peterson    2002   Cages to Jump Shots: Pro Basketball's Early Years      Ruck    1999   The tropic of baseball: Baseball in the Dominican Republic      Creamer    1996   Stengel: His Life and Times      Durocher et al.    2009   Nice guys finish last      Campanella    1995   It's Good to be Alive      Davies    2016   Sports in American life: A history      Liebling    2014   The sweet science      Stokes    1956   Psycho-analytic reflections on the development of ball games, particularly cricket      Dickson    2009   The Dickson baseball dictionary      Okrent et al.    1989   Baseball anecdotes      Legal Events     Date  Code  Title  Description      1996-12-31  REMI  Maintenance fee reminder mailed      1997-05-25  LAPS  Lapse for failure to pay maintenance fees      1997-08-05  FP  Expired due to failure to pay maintenance fee    Effective date : 19970528      
